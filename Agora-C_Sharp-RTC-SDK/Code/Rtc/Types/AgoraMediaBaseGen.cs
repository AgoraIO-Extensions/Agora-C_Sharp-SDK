#region Generated by `terra/node/src/rtc/struct_enumz/renderers.ts`. DO NOT MODIFY BY HAND.
#endregion

using System;
using Agora.Rtc.LitJson;
using view_t = System.UInt64;
using uint8_t = System.Byte;
using int16_t = System.Int16;

namespace Agora.Rtc
{
    ///
    /// <summary>
    /// Plugin context information.
    /// </summary>
    ///
    public class ExtensionContext
    {
        ///
        /// <summary>
        /// Whether the uid reported in ExtensionContext is valid: true : uid is valid. false : uid is invalid.
        /// </summary>
        ///
        public bool isValid;

        ///
        /// <summary>
        /// User ID. 0 represents the local user; values greater than 0 represent remote users.
        /// </summary>
        ///
        public uint uid;

        ///
        /// <summary>
        /// Name of the provider offering the plugin.
        /// </summary>
        ///
        public string providerName;

        ///
        /// <summary>
        /// Name of the plugin.
        /// </summary>
        ///
        public string extensionName;

        public ExtensionContext()
        {
            this.isValid = false;
            this.uid = 0;
            this.providerName = "";
            this.extensionName = "";
        }

        public ExtensionContext(bool isValid, uint uid, string providerName, string extensionName)
        {
            this.isValid = isValid;
            this.uid = uid;
            this.providerName = providerName;
            this.extensionName = extensionName;
        }
    }

    ///
    /// <summary>
    /// Types of video sources.
    /// </summary>
    ///
    public enum VIDEO_SOURCE_TYPE
    {
        ///
        /// <summary>
        /// 0: (Default) Video source is the primary camera.
        /// </summary>
        ///
        VIDEO_SOURCE_CAMERA_PRIMARY = 0,

        ///
        /// <summary>
        /// 0: (Default) Video source is the primary camera.
        /// </summary>
        ///
        VIDEO_SOURCE_CAMERA = VIDEO_SOURCE_CAMERA_PRIMARY,

        ///
        /// <summary>
        /// 1: Video source is the secondary camera.
        /// </summary>
        ///
        VIDEO_SOURCE_CAMERA_SECONDARY = 1,

        ///
        /// <summary>
        /// 2: Video source is the primary screen.
        /// </summary>
        ///
        VIDEO_SOURCE_SCREEN_PRIMARY = 2,

        ///
        /// <summary>
        /// 2: Video source is the primary screen.
        /// </summary>
        ///
        VIDEO_SOURCE_SCREEN = VIDEO_SOURCE_SCREEN_PRIMARY,

        ///
        /// <summary>
        /// 3: Video source is the secondary screen.
        /// </summary>
        ///
        VIDEO_SOURCE_SCREEN_SECONDARY = 3,

        ///
        /// <summary>
        /// 4: Custom video source.
        /// </summary>
        ///
        VIDEO_SOURCE_CUSTOM = 4,

        ///
        /// <summary>
        /// 5: Video source is a media player.
        /// </summary>
        ///
        VIDEO_SOURCE_MEDIA_PLAYER = 5,

        ///
        /// <summary>
        /// 6: Video source is a PNG image.
        /// </summary>
        ///
        VIDEO_SOURCE_RTC_IMAGE_PNG = 6,

        ///
        /// <summary>
        /// 7: Video source is a JPEG image.
        /// </summary>
        ///
        VIDEO_SOURCE_RTC_IMAGE_JPEG = 7,

        ///
        /// <summary>
        /// 8: Video source is a GIF image.
        /// </summary>
        ///
        VIDEO_SOURCE_RTC_IMAGE_GIF = 8,

        ///
        /// <summary>
        /// 9: Video source is a remote video retrieved over the network.
        /// </summary>
        ///
        VIDEO_SOURCE_REMOTE = 9,

        ///
        /// <summary>
        /// 10: Transcoded video source.
        /// </summary>
        ///
        VIDEO_SOURCE_TRANSCODED = 10,

        ///
        /// <summary>
        /// 11: (Android, Windows, and macOS only) Video source is the third camera.
        /// </summary>
        ///
        VIDEO_SOURCE_CAMERA_THIRD = 11,

        ///
        /// <summary>
        /// 12: (Android, Windows, and macOS only) Video source is the fourth camera.
        /// </summary>
        ///
        VIDEO_SOURCE_CAMERA_FOURTH = 12,

        ///
        /// <summary>
        /// 13: (Windows and macOS only) Video source is the third screen.
        /// </summary>
        ///
        VIDEO_SOURCE_SCREEN_THIRD = 13,

        ///
        /// <summary>
        /// 14: (Windows and macOS only) Video source is the fourth screen.
        /// </summary>
        ///
        VIDEO_SOURCE_SCREEN_FOURTH = 14,

        ///
        /// <summary>
        /// 15: Video source is video processed by a speech-driven plugin.
        /// </summary>
        ///
        VIDEO_SOURCE_SPEECH_DRIVEN = 15,

        ///
        /// <summary>
        /// 100: Unknown video source.
        /// </summary>
        ///
        VIDEO_SOURCE_UNKNOWN = 100,

    }

    ///
    /// <summary>
    /// Audio source type.
    /// </summary>
    ///
    public enum AUDIO_SOURCE_TYPE
    {
        ///
        /// <summary>
        /// 0: (Default) Microphone.
        /// </summary>
        ///
        AUDIO_SOURCE_MICROPHONE = 0,

        ///
        /// <summary>
        /// 1: Custom captured audio stream.
        /// </summary>
        ///
        AUDIO_SOURCE_CUSTOM = 1,

        ///
        /// <summary>
        /// 2: Media player.
        /// </summary>
        ///
        AUDIO_SOURCE_MEDIA_PLAYER = 2,

        ///
        /// <summary>
        /// 3: System audio stream captured during screen sharing.
        /// </summary>
        ///
        AUDIO_SOURCE_LOOPBACK_RECORDING = 3,

        ///
        /// @ignore
        ///
        AUDIO_SOURCE_MIXED_STREAM = 4,

        ///
        /// <summary>
        /// 5: Audio stream from a specified remote user.
        /// </summary>
        ///
        AUDIO_SOURCE_REMOTE_USER = 5,

        ///
        /// <summary>
        /// 6: Mixed audio stream of all audio in the current channel.
        /// </summary>
        ///
        AUDIO_SOURCE_REMOTE_CHANNEL = 6,

        ///
        /// <summary>
        /// 100: Unknown audio source.
        /// </summary>
        ///
        AUDIO_SOURCE_UNKNOWN = 100,

    }

    ///
    /// <summary>
    /// Type of audio route.
    /// </summary>
    ///
    public enum AudioRoute
    {
        ///
        /// <summary>
        /// -1: Use the default audio route.
        /// </summary>
        ///
        ROUTE_DEFAULT = -1,

        ///
        /// <summary>
        /// 0: Audio routed to headset with microphone.
        /// </summary>
        ///
        ROUTE_HEADSET = 0,

        ///
        /// <summary>
        /// 1: Audio routed to earpiece.
        /// </summary>
        ///
        ROUTE_EARPIECE = 1,

        ///
        /// <summary>
        /// 2: Audio routed to headset without microphone.
        /// </summary>
        ///
        ROUTE_HEADSETNOMIC = 2,

        ///
        /// <summary>
        /// 3: Audio routed to the device's built-in speaker.
        /// </summary>
        ///
        ROUTE_SPEAKERPHONE = 3,

        ///
        /// <summary>
        /// 4: Audio routed to external speaker. (iOS and macOS only)
        /// </summary>
        ///
        ROUTE_LOUDSPEAKER = 4,

        ///
        /// <summary>
        /// 5: Audio routed to Bluetooth device using HFP protocol.
        /// </summary>
        ///
        ROUTE_BLUETOOTH_DEVICE_HFP = 5,

        ///
        /// <summary>
        /// 6: Audio routed to USB peripheral device. (macOS only)
        /// </summary>
        ///
        ROUTE_USB = 6,

        ///
        /// <summary>
        /// 7: Audio routed to HDMI peripheral device. (macOS only)
        /// </summary>
        ///
        ROUTE_HDMI = 7,

        ///
        /// <summary>
        /// 8: Audio routed to DisplayPort peripheral device. (macOS only)
        /// </summary>
        ///
        ROUTE_DISPLAYPORT = 8,

        ///
        /// <summary>
        /// 9: Audio routed to Apple AirPlay. (macOS only)
        /// </summary>
        ///
        ROUTE_AIRPLAY = 9,

        ///
        /// <summary>
        /// 10: Audio routed to Bluetooth device using A2DP protocol.
        /// </summary>
        ///
        ROUTE_BLUETOOTH_DEVICE_A2DP = 10,

    }

    ///
    /// @ignore
    ///
    public enum BYTES_PER_SAMPLE
    {
        ///
        /// @ignore
        ///
        TWO_BYTES_PER_SAMPLE = 2,

    }

    ///
    /// @ignore
    ///
    public class AudioParameters
    {
        ///
        /// @ignore
        ///
        public int sample_rate;

        ///
        /// @ignore
        ///
        public ulong channels;

        ///
        /// @ignore
        ///
        public ulong frames_per_buffer;

        public AudioParameters()
        {
            this.sample_rate = 0;
            this.channels = 0;
            this.frames_per_buffer = 0;
        }

        public AudioParameters(int sample_rate, ulong channels, ulong frames_per_buffer)
        {
            this.sample_rate = sample_rate;
            this.channels = channels;
            this.frames_per_buffer = frames_per_buffer;
        }
    }

    ///
    /// <summary>
    /// Usage mode of audio data.
    /// </summary>
    ///
    public enum RAW_AUDIO_FRAME_OP_MODE_TYPE
    {
        ///
        /// <summary>
        /// 0: (Default) Read-only mode. For example, if you use the SDK to capture data and perform CDN streaming yourself, you can use this mode.
        /// </summary>
        ///
        RAW_AUDIO_FRAME_OP_MODE_READ_ONLY = 0,

        ///
        /// <summary>
        /// 2: Read-write mode. For example, if you have your own audio effects module and want to pre-process the data (e.g., voice changing), you can use this mode.
        /// </summary>
        ///
        RAW_AUDIO_FRAME_OP_MODE_READ_WRITE = 2,

    }

    ///
    /// <summary>
    /// Media source type.
    /// </summary>
    ///
    public enum MEDIA_SOURCE_TYPE
    {
        ///
        /// <summary>
        /// 0: Audio playback device.
        /// </summary>
        ///
        AUDIO_PLAYOUT_SOURCE = 0,

        ///
        /// <summary>
        /// 1: Audio recording device.
        /// </summary>
        ///
        AUDIO_RECORDING_SOURCE = 1,

        ///
        /// <summary>
        /// 2: Primary camera.
        /// </summary>
        ///
        PRIMARY_CAMERA_SOURCE = 2,

        ///
        /// <summary>
        /// 3: Secondary camera.
        /// </summary>
        ///
        SECONDARY_CAMERA_SOURCE = 3,

        ///
        /// @ignore
        ///
        PRIMARY_SCREEN_SOURCE = 4,

        ///
        /// @ignore
        ///
        SECONDARY_SCREEN_SOURCE = 5,

        ///
        /// <summary>
        /// 6: Custom video capture source.
        /// </summary>
        ///
        CUSTOM_VIDEO_SOURCE = 6,

        ///
        /// @ignore
        ///
        MEDIA_PLAYER_SOURCE = 7,

        ///
        /// @ignore
        ///
        RTC_IMAGE_PNG_SOURCE = 8,

        ///
        /// @ignore
        ///
        RTC_IMAGE_JPEG_SOURCE = 9,

        ///
        /// @ignore
        ///
        RTC_IMAGE_GIF_SOURCE = 10,

        ///
        /// @ignore
        ///
        REMOTE_VIDEO_SOURCE = 11,

        ///
        /// @ignore
        ///
        TRANSCODED_VIDEO_SOURCE = 12,

        ///
        /// <summary>
        /// 13: Video source processed by speech-driven plugin.
        /// </summary>
        ///
        SPEECH_DRIVEN_VIDEO_SOURCE = 13,

        ///
        /// <summary>
        /// 100: Unknown media source.
        /// </summary>
        ///
        UNKNOWN_MEDIA_SOURCE = 100,

    }

    ///
    /// @ignore
    ///
    public class PacketOptions
    {
        ///
        /// @ignore
        ///
        public uint timestamp;

        ///
        /// @ignore
        ///
        public uint8_t audioLevelIndication;

        public PacketOptions()
        {
            this.timestamp = 0;
            this.audioLevelIndication = 127;
        }

        public PacketOptions(uint timestamp, uint8_t audioLevelIndication)
        {
            this.timestamp = timestamp;
            this.audioLevelIndication = audioLevelIndication;
        }
    }

    ///
    /// @ignore
    ///
    public class AudioEncodedFrameInfo
    {
        ///
        /// @ignore
        ///
        public ulong sendTs;

        ///
        /// @ignore
        ///
        public uint8_t codec;

        public AudioEncodedFrameInfo()
        {
            this.sendTs = 0;
            this.codec = 0;
        }

        public AudioEncodedFrameInfo(ulong sendTs, uint8_t codec)
        {
            this.sendTs = sendTs;
            this.codec = codec;
        }
    }

    ///
    /// <summary>
    /// Information about external PCM format audio frames.
    /// </summary>
    ///
    public class AudioPcmFrame
    {
        ///
        /// <summary>
        /// Timestamp of the audio frame (ms).
        /// </summary>
        ///
        public long capture_timestamp;

        ///
        /// <summary>
        /// Number of samples per channel.
        /// </summary>
        ///
        public ulong samples_per_channel_;

        ///
        /// <summary>
        /// Audio sampling rate (Hz).
        /// </summary>
        ///
        public int sample_rate_hz_;

        ///
        /// <summary>
        /// Number of audio channels.
        /// </summary>
        ///
        public ulong num_channels_;

        ///
        /// @ignore
        ///
        public int audio_track_number_;

        ///
        /// <summary>
        /// Number of bytes in the audio data.
        /// </summary>
        ///
        public BYTES_PER_SAMPLE bytes_per_sample;

        ///
        /// <summary>
        /// Audio frame data.
        /// </summary>
        ///
        public int16_t[] data_;

        ///
        /// @ignore
        ///
        public bool is_stereo_;

        public AudioPcmFrame()
        {
            this.capture_timestamp = 0;
            this.samples_per_channel_ = 0;
            this.sample_rate_hz_ = 0;
            this.num_channels_ = 0;
            this.audio_track_number_ = 0;
            this.bytes_per_sample = BYTES_PER_SAMPLE.TWO_BYTES_PER_SAMPLE;
            this.is_stereo_ = false;
        }

        public AudioPcmFrame(AudioPcmFrame src)
        {
            this.capture_timestamp = src.capture_timestamp;
            this.samples_per_channel_ = src.samples_per_channel_;
            this.sample_rate_hz_ = src.sample_rate_hz_;
            this.num_channels_ = src.num_channels_;
            this.audio_track_number_ = src.audio_track_number_;
            this.bytes_per_sample = src.bytes_per_sample;
            this.is_stereo_ = src.is_stereo_;
        }

        public AudioPcmFrame(long capture_timestamp, ulong samples_per_channel_, int sample_rate_hz_, ulong num_channels_, int audio_track_number_, BYTES_PER_SAMPLE bytes_per_sample, int16_t[] data_, bool is_stereo_)
        {
            this.capture_timestamp = capture_timestamp;
            this.samples_per_channel_ = samples_per_channel_;
            this.sample_rate_hz_ = sample_rate_hz_;
            this.num_channels_ = num_channels_;
            this.audio_track_number_ = audio_track_number_;
            this.bytes_per_sample = bytes_per_sample;
            this.data_ = data_;
            this.is_stereo_ = is_stereo_;
        }
    }

    ///
    /// <summary>
    /// Channel mode.
    /// </summary>
    ///
    public enum AUDIO_DUAL_MONO_MODE
    {
        ///
        /// <summary>
        /// 0: Original mode.
        /// </summary>
        ///
        AUDIO_DUAL_MONO_STEREO = 0,

        ///
        /// <summary>
        /// 1: Left channel mode. This mode replaces the right channel audio with the left channel audio, so the user only hears the left channel.
        /// </summary>
        ///
        AUDIO_DUAL_MONO_L = 1,

        ///
        /// <summary>
        /// 2: Right channel mode. This mode replaces the left channel audio with the right channel audio, so the user only hears the right channel.
        /// </summary>
        ///
        AUDIO_DUAL_MONO_R = 2,

        ///
        /// <summary>
        /// 3: Mixed mode. This mode overlays the left and right channel data, so the user hears both channels simultaneously.
        /// </summary>
        ///
        AUDIO_DUAL_MONO_MIX = 3,

    }

    ///
    /// <summary>
    /// Video pixel formats.
    /// </summary>
    ///
    public enum VIDEO_PIXEL_FORMAT
    {
        ///
        /// <summary>
        /// 0: Raw video pixel format.
        /// </summary>
        ///
        VIDEO_PIXEL_DEFAULT = 0,

        ///
        /// <summary>
        /// 1: I420 format.
        /// </summary>
        ///
        VIDEO_PIXEL_I420 = 1,

        ///
        /// @ignore
        ///
        VIDEO_PIXEL_BGRA = 2,

        ///
        /// @ignore
        ///
        VIDEO_PIXEL_NV21 = 3,

        ///
        /// <summary>
        /// 4: RGBA format.
        /// </summary>
        ///
        VIDEO_PIXEL_RGBA = 4,

        ///
        /// @ignore
        ///
        VIDEO_PIXEL_NV12 = 8,

        ///
        /// @ignore
        ///
        VIDEO_TEXTURE_2D = 10,

        ///
        /// @ignore
        ///
        VIDEO_TEXTURE_OES = 11,

        ///
        /// @ignore
        ///
        VIDEO_CVPIXEL_NV12 = 12,

        ///
        /// @ignore
        ///
        VIDEO_CVPIXEL_I420 = 13,

        ///
        /// @ignore
        ///
        VIDEO_CVPIXEL_BGRA = 14,

        ///
        /// @ignore
        ///
        VIDEO_CVPIXEL_P010 = 15,

        ///
        /// <summary>
        /// 16: I422 format.
        /// </summary>
        ///
        VIDEO_PIXEL_I422 = 16,

        ///
        /// <summary>
        /// 17: ID3D11TEXTURE2D format. Currently supported types include DXGI_FORMAT_B8G8R8A8_UNORM, DXGI_FORMAT_B8G8R8A8_TYPELESS, and DXGI_FORMAT_NV12.
        /// </summary>
        ///
        VIDEO_TEXTURE_ID3D11TEXTURE2D = 17,

        ///
        /// @ignore
        ///
        VIDEO_PIXEL_I010 = 18,

    }

    ///
    /// <summary>
    /// Video display mode.
    /// </summary>
    ///
    public enum RENDER_MODE_TYPE
    {
        ///
        /// <summary>
        /// 1: Video is scaled proportionally. Prioritizes filling the view. Excess video outside the view due to size mismatch is cropped.
        /// </summary>
        ///
        RENDER_MODE_HIDDEN = 1,

        ///
        /// <summary>
        /// 2: Video is scaled proportionally. Prioritizes showing the entire video content. Black bars fill the unused view area due to size mismatch.
        /// </summary>
        ///
        RENDER_MODE_FIT = 2,

        ///
        /// <summary>
        /// 3: Adaptive mode. Deprecated: This enum is deprecated and not recommended for use.
        /// </summary>
        ///
        RENDER_MODE_ADAPTIVE = 3,

    }

    ///
    /// @ignore
    ///
    public enum CAMERA_VIDEO_SOURCE_TYPE
    {
        ///
        /// @ignore
        ///
        CAMERA_SOURCE_FRONT = 0,

        ///
        /// @ignore
        ///
        CAMERA_SOURCE_BACK = 1,

        ///
        /// @ignore
        ///
        VIDEO_SOURCE_UNSPECIFIED = 2,

    }

    ///
    /// @ignore
    ///
    public enum META_INFO_KEY
    {
        ///
        /// @ignore
        ///
        KEY_FACE_CAPTURE = 0,

    }

    ///
    /// @ignore
    ///
    public class ColorSpace
    {
        ///
        /// @ignore
        ///
        public PrimaryID primaries;

        ///
        /// @ignore
        ///
        public TransferID transfer;

        ///
        /// @ignore
        ///
        public MatrixID matrix;

        ///
        /// @ignore
        ///
        public RangeID range;

        public ColorSpace()
        {
            this.primaries = PrimaryID.PRIMARYID_UNSPECIFIED;
            this.transfer = TransferID.TRANSFERID_UNSPECIFIED;
            this.matrix = MatrixID.MATRIXID_UNSPECIFIED;
            this.range = RangeID.RANGEID_INVALID;
        }

        public ColorSpace(PrimaryID primaries, TransferID transfer, MatrixID matrix, RangeID range)
        {
            this.primaries = primaries;
            this.transfer = transfer;
            this.matrix = matrix;
            this.range = range;
        }
    }

    ///
    /// @ignore
    ///
    public enum PrimaryID
    {
        ///
        /// @ignore
        ///
        PRIMARYID_BT709 = 1,

        ///
        /// @ignore
        ///
        PRIMARYID_UNSPECIFIED = 2,

        ///
        /// @ignore
        ///
        PRIMARYID_BT470M = 4,

        ///
        /// @ignore
        ///
        PRIMARYID_BT470BG = 5,

        ///
        /// @ignore
        ///
        PRIMARYID_SMPTE170M = 6,

        ///
        /// @ignore
        ///
        PRIMARYID_SMPTE240M = 7,

        ///
        /// @ignore
        ///
        PRIMARYID_FILM = 8,

        ///
        /// @ignore
        ///
        PRIMARYID_BT2020 = 9,

        ///
        /// @ignore
        ///
        PRIMARYID_SMPTEST428 = 10,

        ///
        /// @ignore
        ///
        PRIMARYID_SMPTEST431 = 11,

        ///
        /// @ignore
        ///
        PRIMARYID_SMPTEST432 = 12,

        ///
        /// @ignore
        ///
        PRIMARYID_JEDECP22 = 22,

    }

    ///
    /// @ignore
    ///
    public enum RangeID
    {
        ///
        /// @ignore
        ///
        RANGEID_INVALID = 0,

        ///
        /// @ignore
        ///
        RANGEID_LIMITED = 1,

        ///
        /// @ignore
        ///
        RANGEID_FULL = 2,

        ///
        /// @ignore
        ///
        RANGEID_DERIVED = 3,

    }

    ///
    /// @ignore
    ///
    public enum MatrixID
    {
        ///
        /// @ignore
        ///
        MATRIXID_RGB = 0,

        ///
        /// @ignore
        ///
        MATRIXID_BT709 = 1,

        ///
        /// @ignore
        ///
        MATRIXID_UNSPECIFIED = 2,

        ///
        /// @ignore
        ///
        MATRIXID_FCC = 4,

        ///
        /// @ignore
        ///
        MATRIXID_BT470BG = 5,

        ///
        /// @ignore
        ///
        MATRIXID_SMPTE170M = 6,

        ///
        /// @ignore
        ///
        MATRIXID_SMPTE240M = 7,

        ///
        /// @ignore
        ///
        MATRIXID_YCOCG = 8,

        ///
        /// @ignore
        ///
        MATRIXID_BT2020_NCL = 9,

        ///
        /// @ignore
        ///
        MATRIXID_BT2020_CL = 10,

        ///
        /// @ignore
        ///
        MATRIXID_SMPTE2085 = 11,

        ///
        /// @ignore
        ///
        MATRIXID_CDNCLS = 12,

        ///
        /// @ignore
        ///
        MATRIXID_CDCLS = 13,

        ///
        /// @ignore
        ///
        MATRIXID_BT2100_ICTCP = 14,

    }

    ///
    /// @ignore
    ///
    public enum TransferID
    {
        ///
        /// @ignore
        ///
        TRANSFERID_BT709 = 1,

        ///
        /// @ignore
        ///
        TRANSFERID_UNSPECIFIED = 2,

        ///
        /// @ignore
        ///
        TRANSFERID_GAMMA22 = 4,

        ///
        /// @ignore
        ///
        TRANSFERID_GAMMA28 = 5,

        ///
        /// @ignore
        ///
        TRANSFERID_SMPTE170M = 6,

        ///
        /// @ignore
        ///
        TRANSFERID_SMPTE240M = 7,

        ///
        /// @ignore
        ///
        TRANSFERID_LINEAR = 8,

        ///
        /// @ignore
        ///
        TRANSFERID_LOG = 9,

        ///
        /// @ignore
        ///
        TRANSFERID_LOG_SQRT = 10,

        ///
        /// @ignore
        ///
        TRANSFERID_IEC61966_2_4 = 11,

        ///
        /// @ignore
        ///
        TRANSFERID_BT1361_ECG = 12,

        ///
        /// @ignore
        ///
        TRANSFERID_IEC61966_2_1 = 13,

        ///
        /// @ignore
        ///
        TRANSFERID_BT2020_10 = 14,

        ///
        /// @ignore
        ///
        TRANSFERID_BT2020_12 = 15,

        ///
        /// @ignore
        ///
        TRANSFERID_SMPTEST2084 = 16,

        ///
        /// @ignore
        ///
        TRANSFERID_SMPTEST428 = 17,

        ///
        /// @ignore
        ///
        TRANSFERID_ARIB_STD_B67 = 18,

    }

    ///
    /// @ignore
    ///
    public class Hdr10MetadataInfo
    {
        ///
        /// @ignore
        ///
        public ushort redPrimaryX;

        ///
        /// @ignore
        ///
        public ushort redPrimaryY;

        ///
        /// @ignore
        ///
        public ushort greenPrimaryX;

        ///
        /// @ignore
        ///
        public ushort greenPrimaryY;

        ///
        /// @ignore
        ///
        public ushort bluePrimaryX;

        ///
        /// @ignore
        ///
        public ushort bluePrimaryY;

        ///
        /// @ignore
        ///
        public ushort whitePointX;

        ///
        /// @ignore
        ///
        public ushort whitePointY;

        ///
        /// @ignore
        ///
        public uint maxMasteringLuminance;

        ///
        /// @ignore
        ///
        public uint minMasteringLuminance;

        ///
        /// @ignore
        ///
        public ushort maxContentLightLevel;

        ///
        /// @ignore
        ///
        public ushort maxFrameAverageLightLevel;

        public Hdr10MetadataInfo()
        {
            this.redPrimaryX = 0;
            this.redPrimaryY = 0;
            this.greenPrimaryX = 0;
            this.greenPrimaryY = 0;
            this.bluePrimaryX = 0;
            this.bluePrimaryY = 0;
            this.whitePointX = 0;
            this.whitePointY = 0;
            this.maxMasteringLuminance = 0;
            this.minMasteringLuminance = 0;
            this.maxContentLightLevel = 0;
            this.maxFrameAverageLightLevel = 0;
        }

        public Hdr10MetadataInfo(ushort redPrimaryX, ushort redPrimaryY, ushort greenPrimaryX, ushort greenPrimaryY, ushort bluePrimaryX, ushort bluePrimaryY, ushort whitePointX, ushort whitePointY, uint maxMasteringLuminance, uint minMasteringLuminance, ushort maxContentLightLevel, ushort maxFrameAverageLightLevel)
        {
            this.redPrimaryX = redPrimaryX;
            this.redPrimaryY = redPrimaryY;
            this.greenPrimaryX = greenPrimaryX;
            this.greenPrimaryY = greenPrimaryY;
            this.bluePrimaryX = bluePrimaryX;
            this.bluePrimaryY = bluePrimaryY;
            this.whitePointX = whitePointX;
            this.whitePointY = whitePointY;
            this.maxMasteringLuminance = maxMasteringLuminance;
            this.minMasteringLuminance = minMasteringLuminance;
            this.maxContentLightLevel = maxContentLightLevel;
            this.maxFrameAverageLightLevel = maxFrameAverageLightLevel;
        }
    }

    ///
    /// <summary>
    /// Relative position of alphaBuffer and the video frame.
    /// </summary>
    ///
    public enum ALPHA_STITCH_MODE
    {
        ///
        /// <summary>
        /// 0: (Default) Only the video frame, i.e., alphaBuffer is not stitched with the video frame.
        /// </summary>
        ///
        NO_ALPHA_STITCH = 0,

        ///
        /// <summary>
        /// 1: alphaBuffer is above the video frame.
        /// </summary>
        ///
        ALPHA_STITCH_UP = 1,

        ///
        /// <summary>
        /// 2: alphaBuffer is below the video frame.
        /// </summary>
        ///
        ALPHA_STITCH_BELOW = 2,

        ///
        /// <summary>
        /// 3: alphaBuffer is to the left of the video frame.
        /// </summary>
        ///
        ALPHA_STITCH_LEFT = 3,

        ///
        /// <summary>
        /// 4: alphaBuffer is to the right of the video frame.
        /// </summary>
        ///
        ALPHA_STITCH_RIGHT = 4,

    }

    ///
    /// <summary>
    /// External video frame.
    /// </summary>
    ///
    public class ExternalVideoFrame
    {
        ///
        /// <summary>
        /// Video type. See VIDEO_BUFFER_TYPE.
        /// </summary>
        ///
        public VIDEO_BUFFER_TYPE type;

        ///
        /// <summary>
        /// Pixel format. See VIDEO_PIXEL_FORMAT.
        /// </summary>
        ///
        public VIDEO_PIXEL_FORMAT format;

        ///
        /// <summary>
        /// Video buffer.
        /// </summary>
        ///
        public byte[] buffer;

        ///
        /// <summary>
        /// Stride of the input video frame, in pixels (not bytes). For Texture, this refers to the width of the Texture.
        /// </summary>
        ///
        public int stride;

        ///
        /// <summary>
        /// Height of the input video frame.
        /// </summary>
        ///
        public int height;

        ///
        /// <summary>
        /// (Raw video only)
        /// </summary>
        ///
        public int cropLeft;

        ///
        /// <summary>
        /// (Raw video only)
        /// </summary>
        ///
        public int cropTop;

        ///
        /// <summary>
        /// (Raw video only)
        /// </summary>
        ///
        public int cropRight;

        ///
        /// <summary>
        /// (Raw video only)
        /// </summary>
        ///
        public int cropBottom;

        ///
        /// <summary>
        /// Raw data field. Specifies clockwise rotation of the input video group. Options: 0, 90, 180, 270. Default is 0.
        /// </summary>
        ///
        public int rotation;

        ///
        /// <summary>
        /// Timestamp of the input video frame in milliseconds. Incorrect timestamps may cause frame drops or AV sync issues.
        /// </summary>
        ///
        public long timestamp;

        ///
        /// <summary>
        /// (Texture only)
        ///  When using the Khronos-defined OpenGL interface (javax.microedition.khronos.egl.*), set eglContext to this field.
        ///  When using the Android-defined OpenGL interface (android.opengl.*), set eglContext to this field.
        /// </summary>
        ///
        public IntPtr eglContext;

        ///
        /// <summary>
        /// (Texture only) Type of the Texture ID for the video frame.
        /// </summary>
        ///
        public EGL_CONTEXT_TYPE eglType;

        ///
        /// <summary>
        /// (Texture only) A 4x4 transformation matrix input, typically an identity matrix.
        /// </summary>
        ///
        public int textureId;

        ///
        /// @ignore
        ///
        public long fenceObject;

        ///
        /// @ignore
        ///
        public float[] matrix;

        ///
        /// <summary>
        /// (Texture only) Metadata buffer. Default is NULL.
        /// </summary>
        ///
        public byte[] metadataBuffer;

        ///
        /// <summary>
        /// (Texture only) Size of the metadata. Default is 0.
        /// </summary>
        ///
        public int metadataSize;

        ///
        /// <summary>
        /// Alpha channel data output by portrait segmentation algorithm. The data matches the video frame size, with pixel values ranging from [0,255], where 0 represents background and 255 represents foreground (portrait).
        /// You can use this parameter to render the video background with various effects, such as transparency, solid color, image, or video. In custom video rendering scenarios, ensure that both the video frame and alphaBuffer are Full Range. Other types may cause abnormal Alpha rendering.
        /// </summary>
        ///
        public byte[] alphaBuffer;

        ///
        /// <summary>
        /// For BGRA or RGBA video data, you can set Alpha channel data using either of the following:
        ///  Automatically fill by setting this parameter to true.
        ///  Set via the alphaBuffer parameter. (BGRA or RGBA only) Whether to extract the Alpha channel data from the video frame and automatically fill it into alphaBuffer : true : Extract and fill Alpha channel data. false : (default) Do not extract or fill Alpha channel data.
        /// </summary>
        ///
        public bool fillAlphaBuffer;

        ///
        /// <summary>
        /// When the video frame contains Alpha channel data, sets the relative position of alphaBuffer and the video frame. See ALPHA_STITCH_MODE.
        /// </summary>
        ///
        public ALPHA_STITCH_MODE alphaStitchMode;

        ///
        /// <summary>
        /// (Windows Texture only) A pointer to an ID3D11Texture2D object used by the video frame.
        /// </summary>
        ///
        public IntPtr d3d11Texture2d;

        ///
        /// <summary>
        /// (Windows Texture only) Index of the ID3D11Texture2D texture object used by the video frame in the ID3D11Texture2D array.
        /// </summary>
        ///
        public int textureSliceIndex;

        ///
        /// @ignore
        ///
        public Hdr10MetadataInfo hdr10MetadataInfo;

        ///
        /// <summary>
        /// Color space property of the video frame. By default, Full Range and BT.709 configurations are applied. You can customize this based on custom capture or rendering needs. See [VideoColorSpace](https://developer.mozilla.org/en-US/docs/Web/API/VideoColorSpace).
        /// </summary>
        ///
        public ColorSpace colorSpace;

        public ExternalVideoFrame()
        {
            this.type = VIDEO_BUFFER_TYPE.VIDEO_BUFFER_RAW_DATA;
            this.format = VIDEO_PIXEL_FORMAT.VIDEO_PIXEL_DEFAULT;
            this.buffer = null;
            this.stride = 0;
            this.height = 0;
            this.cropLeft = 0;
            this.cropTop = 0;
            this.cropRight = 0;
            this.cropBottom = 0;
            this.rotation = 0;
            this.timestamp = 0;
            this.eglContext = IntPtr.Zero;
            this.eglType = EGL_CONTEXT_TYPE.EGL_CONTEXT10;
            this.textureId = 0;
            this.fenceObject = 0;
            this.metadataBuffer = null;
            this.metadataSize = 0;
            this.alphaBuffer = null;
            this.fillAlphaBuffer = false;
            this.alphaStitchMode = ALPHA_STITCH_MODE.NO_ALPHA_STITCH;
            this.d3d11Texture2d = IntPtr.Zero;
            this.textureSliceIndex = 0;
        }

        public ExternalVideoFrame(VIDEO_BUFFER_TYPE type, VIDEO_PIXEL_FORMAT format, byte[] buffer, int stride, int height, int cropLeft, int cropTop, int cropRight, int cropBottom, int rotation, long timestamp, IntPtr eglContext, EGL_CONTEXT_TYPE eglType, int textureId, long fenceObject, float[] matrix, byte[] metadataBuffer, int metadataSize, byte[] alphaBuffer, bool fillAlphaBuffer, ALPHA_STITCH_MODE alphaStitchMode, IntPtr d3d11Texture2d, int textureSliceIndex, Hdr10MetadataInfo hdr10MetadataInfo, ColorSpace colorSpace)
        {
            this.type = type;
            this.format = format;
            this.buffer = buffer;
            this.stride = stride;
            this.height = height;
            this.cropLeft = cropLeft;
            this.cropTop = cropTop;
            this.cropRight = cropRight;
            this.cropBottom = cropBottom;
            this.rotation = rotation;
            this.timestamp = timestamp;
            this.eglContext = eglContext;
            this.eglType = eglType;
            this.textureId = textureId;
            this.fenceObject = fenceObject;
            this.matrix = matrix;
            this.metadataBuffer = metadataBuffer;
            this.metadataSize = metadataSize;
            this.alphaBuffer = alphaBuffer;
            this.fillAlphaBuffer = fillAlphaBuffer;
            this.alphaStitchMode = alphaStitchMode;
            this.d3d11Texture2d = d3d11Texture2d;
            this.textureSliceIndex = textureSliceIndex;
            this.hdr10MetadataInfo = hdr10MetadataInfo;
            this.colorSpace = colorSpace;
        }
    }

    ///
    /// @ignore
    ///
    public enum EGL_CONTEXT_TYPE
    {
        ///
        /// @ignore
        ///
        EGL_CONTEXT10 = 0,

        ///
        /// @ignore
        ///
        EGL_CONTEXT14 = 1,

    }

    ///
    /// <summary>
    /// Video buffer type.
    /// </summary>
    ///
    public enum VIDEO_BUFFER_TYPE
    {
        ///
        /// <summary>
        /// 1: Type is raw data.
        /// </summary>
        ///
        VIDEO_BUFFER_RAW_DATA = 1,

        ///
        /// <summary>
        /// 2: Type is raw data.
        /// </summary>
        ///
        VIDEO_BUFFER_ARRAY = 2,

        ///
        /// <summary>
        /// 3: Type is Texture.
        /// </summary>
        ///
        VIDEO_BUFFER_TEXTURE = 3,

    }

    ///
    /// <summary>
    /// Video frame property settings.
    /// 
    /// The buffer is a pointer to a pointer. This interface cannot modify the buffer pointer, only the buffer content.
    /// </summary>
    ///
    public class VideoFrame
    {
        ///
        /// @ignore
        ///
        public IntPtr yBufferPtr;

        ///
        /// @ignore
        ///
        public IntPtr uBufferPtr;

        ///
        /// @ignore
        ///
        public IntPtr vBufferPtr;

        ///
        /// @ignore
        ///
        public IntPtr alphaBufferPtr;

        ///
        /// <summary>
        /// Pixel format. See VIDEO_PIXEL_FORMAT.
        /// </summary>
        ///
        public VIDEO_PIXEL_FORMAT type;

        ///
        /// <summary>
        /// Video pixel width.
        /// </summary>
        ///
        public int width;

        ///
        /// <summary>
        /// Video pixel height.
        /// </summary>
        ///
        public int height;

        ///
        /// <summary>
        /// For YUV data, indicates the stride of the Y buffer; for RGBA data, indicates the total data length. When processing video data, you need to handle the offset between each row of pixel data according to this parameter, otherwise image distortion may occur.
        /// </summary>
        ///
        public int yStride;

        ///
        /// <summary>
        /// For YUV data, indicates the stride of the U buffer; for RGBA data, the value is 0. When processing video data, you need to handle the offset between each row of pixel data according to this parameter, otherwise image distortion may occur.
        /// </summary>
        ///
        public int uStride;

        ///
        /// <summary>
        /// For YUV data, indicates the stride of the V buffer; for RGBA data, the value is 0. When processing video data, you need to handle the offset between each row of pixel data according to this parameter, otherwise image distortion may occur.
        /// </summary>
        ///
        public int vStride;

        ///
        /// <summary>
        /// For YUV data, indicates the pointer to the Y buffer; for RGBA data, indicates the data buffer.
        /// </summary>
        ///
        public byte[] yBuffer;

        ///
        /// <summary>
        /// For YUV data, indicates the pointer to the U buffer; for RGBA data, the value is null.
        /// </summary>
        ///
        public byte[] uBuffer;

        ///
        /// <summary>
        /// For YUV data, indicates the pointer to the V buffer; for RGBA data, the value is null.
        /// </summary>
        ///
        public byte[] vBuffer;

        ///
        /// <summary>
        /// Set the clockwise rotation angle of the frame before rendering the video. Currently supports 0, 90, 180, and 270 degrees.
        /// </summary>
        ///
        public int rotation;

        ///
        /// <summary>
        /// The Unix timestamp (in milliseconds) when the video frame is rendered. This timestamp can be used to guide the rendering of the video frame. This parameter is required.
        /// </summary>
        ///
        public long renderTimeMs;

        ///
        /// <summary>
        /// Reserved parameter.
        /// </summary>
        ///
        public int avsync_type;

        ///
        /// <summary>
        /// This parameter is only applicable to video data in Texture format. Refers to the metadata buffer. The default value is NULL.
        /// </summary>
        ///
        public IntPtr metadata_buffer;

        ///
        /// <summary>
        /// This parameter is only applicable to video data in Texture format. Refers to the size of the metadata. The default value is 0.
        /// </summary>
        ///
        public int metadata_size;

        ///
        /// <summary>
        /// This parameter is only applicable to video data in Texture format. EGL Context.
        /// </summary>
        ///
        public IntPtr sharedContext;

        ///
        /// <summary>
        /// This parameter is only applicable to video data in Texture format. Texture ID.
        /// </summary>
        ///
        public int textureId;

        ///
        /// <summary>
        /// This parameter is only applicable to video data in Windows Texture format. Represents a pointer to an object of type ID3D11Texture2D, which is used by the video frame.
        /// </summary>
        ///
        public IntPtr d3d11Texture2d;

        ///
        /// <summary>
        /// This parameter is only applicable to video data in Texture format. It is an input 4x4 transformation matrix, typically an identity matrix.
        /// </summary>
        ///
        public float[] matrix;

        ///
        /// <summary>
        /// Alpha channel data output by the portrait segmentation algorithm. This data matches the size of the video frame. Each pixel value ranges from [0, 255], where 0 represents the background and 255 represents the foreground (portrait).
        /// You can use this parameter to render the video background with various effects, such as transparency, solid color, image, video, etc.
        ///  In custom video rendering scenarios, ensure that both the video frame and alphaBuffer are of Full Range type; other types may cause abnormal Alpha data rendering.
        ///  Make sure that alphaBuffer exactly matches the size of the video frame (width Ã— height), otherwise the app may crash.
        /// </summary>
        ///
        public byte[] alphaBuffer;

        ///
        /// <summary>
        /// When the video frame contains Alpha channel data, sets the relative position of alphaBuffer and the video frame. See ALPHA_STITCH_MODE.
        /// </summary>
        ///
        public ALPHA_STITCH_MODE alphaStitchMode;

        ///
        /// @ignore
        ///
        public IntPtr pixelBuffer;

        ///
        /// <summary>
        /// Metadata in the video frame. This parameter requires [contacting technical support](https://ticket.shengwang.cn/) for use.
        /// </summary>
        ///
        public IVideoFrameMetaInfo metaInfo;

        ///
        /// @ignore
        ///
        public Hdr10MetadataInfo hdr10MetadataInfo;

        ///
        /// <summary>
        /// Color space attributes of the video frame. By default, Full Range and BT.709 standard configurations are applied. You can customize the settings based on your custom capture and rendering requirements. See [VideoColorSpace](https://developer.mozilla.org/en-US/docs/Web/API/VideoColorSpace).
        /// </summary>
        ///
        public ColorSpace colorSpace;

        public VideoFrame()
        {
            this.type = VIDEO_PIXEL_FORMAT.VIDEO_PIXEL_DEFAULT;
            this.width = 0;
            this.height = 0;
            this.yStride = 0;
            this.uStride = 0;
            this.vStride = 0;
            this.yBuffer = new byte[0];
            this.uBuffer = new byte[0];
            this.vBuffer = new byte[0];
            this.rotation = 0;
            this.renderTimeMs = 0;
            this.avsync_type = 0;
            this.metadata_buffer = IntPtr.Zero;
            this.metadata_size = 0;
            this.sharedContext = IntPtr.Zero;
            this.textureId = 0;
            this.d3d11Texture2d = IntPtr.Zero;
            this.alphaBuffer = new byte[0];
            this.alphaStitchMode = ALPHA_STITCH_MODE.NO_ALPHA_STITCH;
            this.pixelBuffer = IntPtr.Zero;
            this.metaInfo = null;
        }

        public VideoFrame(VIDEO_PIXEL_FORMAT type, int width, int height, int yStride, int uStride, int vStride, byte[] yBuffer, byte[] uBuffer, byte[] vBuffer, int rotation, long renderTimeMs, int avsync_type, IntPtr metadata_buffer, int metadata_size, IntPtr sharedContext, int textureId, IntPtr d3d11Texture2d, float[] matrix, byte[] alphaBuffer, ALPHA_STITCH_MODE alphaStitchMode, IntPtr pixelBuffer, IVideoFrameMetaInfo metaInfo, Hdr10MetadataInfo hdr10MetadataInfo, ColorSpace colorSpace)
        {
            this.type = type;
            this.width = width;
            this.height = height;
            this.yStride = yStride;
            this.uStride = uStride;
            this.vStride = vStride;
            this.yBuffer = yBuffer;
            this.uBuffer = uBuffer;
            this.vBuffer = vBuffer;
            this.rotation = rotation;
            this.renderTimeMs = renderTimeMs;
            this.avsync_type = avsync_type;
            this.metadata_buffer = metadata_buffer;
            this.metadata_size = metadata_size;
            this.sharedContext = sharedContext;
            this.textureId = textureId;
            this.d3d11Texture2d = d3d11Texture2d;
            this.matrix = matrix;
            this.alphaBuffer = alphaBuffer;
            this.alphaStitchMode = alphaStitchMode;
            this.pixelBuffer = pixelBuffer;
            this.metaInfo = metaInfo;
            this.hdr10MetadataInfo = hdr10MetadataInfo;
            this.colorSpace = colorSpace;
        }
    }

    ///
    /// @ignore
    ///
    public enum MEDIA_PLAYER_SOURCE_TYPE
    {
        ///
        /// @ignore
        ///
        MEDIA_PLAYER_SOURCE_DEFAULT,

        ///
        /// @ignore
        ///
        MEDIA_PLAYER_SOURCE_FULL_FEATURED,

        ///
        /// @ignore
        ///
        MEDIA_PLAYER_SOURCE_SIMPLE,

    }

    ///
    /// <summary>
    /// Video observation position.
    /// </summary>
    ///
    [Flags]
    public enum VIDEO_MODULE_POSITION
    {
        ///
        /// <summary>
        /// 1: Position after local video capture and preprocessing, corresponding to the OnCaptureVideoFrame callback. The observed video includes preprocessing effects, which can be verified by enabling beauty effects, virtual background, or watermark.
        /// </summary>
        ///
        POSITION_POST_CAPTURER = 1 << 0,

        ///
        /// <summary>
        /// 2: Position before rendering the received remote video, corresponding to the OnRenderVideoFrame callback.
        /// </summary>
        ///
        POSITION_PRE_RENDERER = 1 << 1,

        ///
        /// <summary>
        /// 4: Position before local video encoding, corresponding to the OnPreEncodeVideoFrame callback. The observed video includes preprocessing and pre-encoding effects:
        ///  Preprocessing effects can be verified by enabling beauty effects, virtual background, or watermark.
        ///  Pre-encoding effects can be verified by setting a low frame rate (e.g., 5 fps).
        /// </summary>
        ///
        POSITION_PRE_ENCODER = 1 << 2,

        ///
        /// <summary>
        /// 8: Position after local video capture and before preprocessing. The observed video does not include preprocessing effects, which can be verified by enabling beauty effects, virtual background, or watermark.
        /// </summary>
        ///
        POSITION_POST_CAPTURER_ORIGIN = 1 << 3,

    }

    ///
    /// @ignore
    ///
    public enum CONTENT_INSPECT_RESULT
    {
        ///
        /// @ignore
        ///
        CONTENT_INSPECT_NEUTRAL = 1,

        ///
        /// @ignore
        ///
        CONTENT_INSPECT_SEXY = 2,

        ///
        /// @ignore
        ///
        CONTENT_INSPECT_PORN = 3,

    }

    ///
    /// <summary>
    /// Type of video content inspection module.
    /// </summary>
    ///
    public enum CONTENT_INSPECT_TYPE
    {
        ///
        /// <summary>
        /// 0: (Default) This module has no actual function. Do not set type to this value.
        /// </summary>
        ///
        CONTENT_INSPECT_INVALID = 0,

        ///
        /// @ignore
        ///
        [Obsolete("Content inspect type moderation")]
        CONTENT_INSPECT_MODERATION = 1,

        ///
        /// <summary>
        /// 2: Uses Agora's proprietary plugin for screenshot and upload. The SDK captures and uploads screenshots of the video stream.
        /// </summary>
        ///
        CONTENT_INSPECT_SUPERVISION = 2,

        ///
        /// <summary>
        /// 3: Uses cloud marketplace plugin for screenshot and upload. The SDK uses the cloud marketplace video moderation plugin to capture and upload screenshots of the video stream.
        /// </summary>
        ///
        CONTENT_INSPECT_IMAGE_MODERATION = 3,

    }

    ///
    /// <summary>
    /// The ContentInspectModule struct is used to configure the frequency of local screenshot uploads.
    /// </summary>
    ///
    public class ContentInspectModule
    {
        ///
        /// <summary>
        /// Type of functional module. See CONTENT_INSPECT_TYPE.
        /// </summary>
        ///
        public CONTENT_INSPECT_TYPE type;

        ///
        /// <summary>
        /// Interval for local screenshot upload, in seconds. The value must be greater than 0. Default is 0, which means no screenshot upload. Recommended value is 10 seconds, but you can adjust it based on your business needs.
        /// </summary>
        ///
        public uint interval;

        ///
        /// <summary>
        /// Position of the video observer. See VIDEO_MODULE_POSITION.
        /// </summary>
        ///
        public VIDEO_MODULE_POSITION position;

        public ContentInspectModule()
        {
        }

        public ContentInspectModule(CONTENT_INSPECT_TYPE type, uint interval, VIDEO_MODULE_POSITION position)
        {
            this.type = type;
            this.interval = interval;
            this.position = position;
        }
    }

    ///
    /// <summary>
    /// Local screenshot upload configuration.
    /// </summary>
    ///
    public class ContentInspectConfig
    {
        ///
        /// <summary>
        /// Additional information, with a maximum length of 1024 bytes.
        /// The SDK uploads the additional information along with the screenshot to the Agora server. After the screenshot is completed, the Agora server sends the additional information to your server along with the callback notification.
        /// </summary>
        ///
        public string extraInfo;

        ///
        /// <summary>
        /// (Optional) Cloud Marketplace video moderation related server configuration. This parameter takes effect only when the type in ContentInspectModule is set to CONTENT_INSPECT_IMAGE_MODERATION. To use this feature, [contact technical support](https://ticket.shengwang.cn/).
        /// </summary>
        ///
        public string serverConfig;

        ///
        /// <summary>
        /// Functional modules. See ContentInspectModule.
        /// Supports up to 32 ContentInspectModule instances. The value range for MAX_CONTENT_INSPECT_MODULE_COUNT is an integer between [1, 32]. Only one instance can be configured for each functional module. Currently, only screenshot upload is supported.
        /// </summary>
        ///
        public ContentInspectModule[] modules;

        ///
        /// <summary>
        /// Number of functional modules, i.e., the number of configured ContentInspectModule instances. Must match the number of instances configured in modules. Maximum value is 32.
        /// </summary>
        ///
        public int moduleCount;

        public ContentInspectConfig()
        {
            this.extraInfo = "";
            this.serverConfig = "";
            this.moduleCount = 0;
        }

        public ContentInspectConfig(string extraInfo, string serverConfig, ContentInspectModule[] modules, int moduleCount)
        {
            this.extraInfo = extraInfo;
            this.serverConfig = serverConfig;
            this.modules = modules;
            this.moduleCount = moduleCount;
        }
    }

    ///
    /// <summary>
    /// Video snapshot settings.
    /// </summary>
    ///
    public class SnapshotConfig
    {
        ///
        /// <summary>
        /// Make sure the directory exists and is writable. The local path to save the snapshot, including the file name and format. For example:
        ///  Windows: C:\Users\<user_name>\AppData\Local\Agora\<process_name>\example.jpg
        ///  iOS: /App Sandbox/Library/Caches/example.jpg
        ///  macOS: ï½ž/Library/Logs/example.jpg
        ///  Android: /storage/emulated/0/Android/data/<package name>/files/example.jpg
        /// </summary>
        ///
        public string filePath;

        ///
        /// <summary>
        /// The position of the video frame in the video pipeline to capture the snapshot. See VIDEO_MODULE_POSITION.
        /// </summary>
        ///
        public VIDEO_MODULE_POSITION position;

        public SnapshotConfig()
        {
            this.filePath = "";
            this.position = VIDEO_MODULE_POSITION.POSITION_PRE_ENCODER;
        }

        public SnapshotConfig(string filePath, VIDEO_MODULE_POSITION position)
        {
            this.filePath = filePath;
            this.position = position;
        }
    }

    ///
    /// <summary>
    /// Audio frame type.
    /// </summary>
    ///
    public enum AUDIO_FRAME_TYPE
    {
        ///
        /// <summary>
        /// 0: PCM 16
        /// </summary>
        ///
        FRAME_TYPE_PCM16 = 0,

    }

    ///
    /// <summary>
    /// Raw audio data.
    /// </summary>
    ///
    public class AudioFrame
    {
        ///
        /// <summary>
        /// Audio data buffer (for stereo, data is interleaved).
        /// Buffer size buffer = samples Ã— channels Ã— bytesPerSample.
        /// </summary>
        ///
        public byte[] RawBuffer = new byte[0];

        ///
        /// <summary>
        /// Audio frame type. See AUDIO_FRAME_TYPE.
        /// </summary>
        ///
        public AUDIO_FRAME_TYPE type;

        ///
        /// <summary>
        /// Number of samples per channel.
        /// </summary>
        ///
        public int samplesPerChannel;

        ///
        /// <summary>
        /// Number of bytes per sample. For PCM, typically 16 bits, i.e., two bytes.
        /// </summary>
        ///
        public BYTES_PER_SAMPLE bytesPerSample;

        ///
        /// <summary>
        /// Number of channels (for stereo, data is interleaved).
        ///  1: Mono
        ///  2: Stereo
        /// </summary>
        ///
        public int channels;

        ///
        /// <summary>
        /// Number of samples per second per channel.
        /// </summary>
        ///
        public int samplesPerSec;

        ///
        /// @ignore
        ///
        public IntPtr buffer;

        ///
        /// <summary>
        /// Render timestamp of the external audio frame.
        /// You can use this timestamp to restore the order of audio frames. In scenarios with video (including those using external video sources), this parameter can be used to achieve audio-video synchronization.
        /// </summary>
        ///
        public long renderTimeMs;

        ///
        /// <summary>
        /// Reserved parameter.
        /// </summary>
        ///
        public int avsync_type;

        ///
        /// @ignore
        ///
        public long presentationMs;

        ///
        /// @ignore
        ///
        public int audioTrackNumber;

        ///
        /// @ignore
        ///
        public uint rtpTimestamp;

        public AudioFrame()
        {
            this.type = AUDIO_FRAME_TYPE.FRAME_TYPE_PCM16;
            this.samplesPerChannel = 0;
            this.bytesPerSample = BYTES_PER_SAMPLE.TWO_BYTES_PER_SAMPLE;
            this.channels = 0;
            this.samplesPerSec = 0;
            this.buffer = IntPtr.Zero;
            this.renderTimeMs = 0;
            this.avsync_type = 0;
            this.presentationMs = 0;
            this.audioTrackNumber = 0;
            this.rtpTimestamp = 0;
        }

        public AudioFrame(AUDIO_FRAME_TYPE type, int samplesPerChannel, BYTES_PER_SAMPLE bytesPerSample, int channels, int samplesPerSec, IntPtr buffer, long renderTimeMs, int avsync_type, long presentationMs, int audioTrackNumber, uint rtpTimestamp)
        {
            this.type = type;
            this.samplesPerChannel = samplesPerChannel;
            this.bytesPerSample = bytesPerSample;
            this.channels = channels;
            this.samplesPerSec = samplesPerSec;
            this.buffer = buffer;
            this.renderTimeMs = renderTimeMs;
            this.avsync_type = avsync_type;
            this.presentationMs = presentationMs;
            this.audioTrackNumber = audioTrackNumber;
            this.rtpTimestamp = rtpTimestamp;
        }
    }

    ///
    /// @ignore
    ///
    [Flags]
    public enum AUDIO_FRAME_POSITION
    {
        ///
        /// @ignore
        ///
        AUDIO_FRAME_POSITION_NONE = 0x0000,

        ///
        /// @ignore
        ///
        AUDIO_FRAME_POSITION_PLAYBACK = 0x0001,

        ///
        /// @ignore
        ///
        AUDIO_FRAME_POSITION_RECORD = 0x0002,

        ///
        /// @ignore
        ///
        AUDIO_FRAME_POSITION_MIXED = 0x0004,

        ///
        /// @ignore
        ///
        AUDIO_FRAME_POSITION_BEFORE_MIXING = 0x0008,

        ///
        /// @ignore
        ///
        AUDIO_FRAME_POSITION_EAR_MONITORING = 0x0010,

    }

    ///
    /// <summary>
    /// Audio data format.
    /// 
    /// You can pass an AudioParams object in the following APIs to set the audio data format for the corresponding callback: SetRecordingAudioFrameParameters : Sets the data format for the OnRecordAudioFrame callback. SetPlaybackAudioFrameParameters : Sets the data format for the OnPlaybackAudioFrame callback. SetMixedAudioFrameParameters : Sets the data format for the OnMixedAudioFrame callback. SetEarMonitoringAudioFrameParameters : Sets the data format for the OnEarMonitoringAudioFrame callback.
    ///  The SDK calculates the sampling interval using the samplesPerCall, sampleRate, and channel parameters in AudioParams, and triggers the OnRecordAudioFrame, OnPlaybackAudioFrame, OnMixedAudioFrame, and OnEarMonitoringAudioFrame callbacks based on this interval.
    ///  Sampling interval = samplesPerCall / (sampleRate Ã— channel).
    ///  Make sure the sampling interval is not less than 0.01 (s).
    /// </summary>
    ///
    public class AudioParams
    {
        ///
        /// <summary>
        /// Sampling rate of the data in Hz. Valid values:
        ///  8000
        ///  16000 (default)
        ///  32000
        ///  44100
        ///  48000
        /// </summary>
        ///
        public int sample_rate;

        ///
        /// <summary>
        /// Number of audio channels. Valid values:
        ///  1: Mono (default)
        ///  2: Stereo
        /// </summary>
        ///
        public int channels;

        ///
        /// <summary>
        /// Usage mode of the data. See RAW_AUDIO_FRAME_OP_MODE_TYPE.
        /// </summary>
        ///
        public RAW_AUDIO_FRAME_OP_MODE_TYPE mode;

        ///
        /// <summary>
        /// Number of samples per call. Typically 1024 in scenarios like media stream relay.
        /// </summary>
        ///
        public int samples_per_call;

        public AudioParams()
        {
            this.sample_rate = 0;
            this.channels = 0;
            this.mode = RAW_AUDIO_FRAME_OP_MODE_TYPE.RAW_AUDIO_FRAME_OP_MODE_READ_ONLY;
            this.samples_per_call = 0;
        }

        public AudioParams(int samplerate, int channel, RAW_AUDIO_FRAME_OP_MODE_TYPE type, int samplesPerCall)
        {
            this.sample_rate = samplerate;
            this.channels = channel;
            this.mode = type;
            this.samples_per_call = samplesPerCall;
        }

    }

    ///
    /// <summary>
    /// Audio spectrum data.
    /// </summary>
    ///
    public class AudioSpectrumData
    {
        ///
        /// <summary>
        /// Audio spectrum data. Agora divides the audio frequency into 256 frequency bands and reports the energy value of each band through this parameter. The value range of each energy is [-300, 1], in dBFS.
        /// </summary>
        ///
        public float[] audioSpectrumData;

        ///
        /// <summary>
        /// The length of the audio spectrum data is 256.
        /// </summary>
        ///
        public int dataLength;

        public AudioSpectrumData()
        {
            this.audioSpectrumData = null;
            this.dataLength = 0;
        }

        public AudioSpectrumData(float[] data, int length)
        {
            this.audioSpectrumData = data;
            this.dataLength = length;
        }

    }

    ///
    /// <summary>
    /// Audio spectrum information of a remote user.
    /// </summary>
    ///
    public class UserAudioSpectrumInfo
    {
        ///
        /// <summary>
        /// Remote user ID.
        /// </summary>
        ///
        public uint uid;

        ///
        /// <summary>
        /// Audio spectrum data of the remote user. See AudioSpectrumData.
        /// </summary>
        ///
        public AudioSpectrumData spectrumData;

        public UserAudioSpectrumInfo()
        {
            this.uid = 0;
        }

        public UserAudioSpectrumInfo(uint uid, float[] data, int length)
        {
            this.uid = uid;
            this.spectrumData = new AudioSpectrumData(data, length);
        }

        public UserAudioSpectrumInfo(uint uid, AudioSpectrumData spectrumData)
        {
            this.uid = uid;
            this.spectrumData = spectrumData;
        }
    }

    ///
    /// @ignore
    ///
    public enum VIDEO_FRAME_PROCESS_MODE
    {
        ///
        /// @ignore
        ///
        PROCESS_MODE_READ_ONLY,

        ///
        /// @ignore
        ///
        PROCESS_MODE_READ_WRITE,

    }

    ///
    /// <summary>
    /// Encoding type of external video frames.
    /// </summary>
    ///
    public enum EXTERNAL_VIDEO_SOURCE_TYPE
    {
        ///
        /// <summary>
        /// 0: Unencoded video frame.
        /// </summary>
        ///
        VIDEO_FRAME = 0,

        ///
        /// <summary>
        /// 1: Encoded video frame.
        /// </summary>
        ///
        ENCODED_VIDEO_FRAME,

    }

    ///
    /// @ignore
    ///
    public enum MediaRecorderContainerFormat
    {
        ///
        /// @ignore
        ///
        FORMAT_MP4 = 1,

    }

    ///
    /// @ignore
    ///
    public enum MediaRecorderStreamType
    {
        ///
        /// @ignore
        ///
        STREAM_TYPE_AUDIO = 0x01,

        ///
        /// @ignore
        ///
        STREAM_TYPE_VIDEO = 0x02,

        ///
        /// @ignore
        ///
        STREAM_TYPE_BOTH = STREAM_TYPE_AUDIO | STREAM_TYPE_VIDEO,

    }

    ///
    /// <summary>
    /// Current recording state.
    /// </summary>
    ///
    public enum RecorderState
    {
        ///
        /// <summary>
        /// -1: Audio/video stream recording error. See RecorderReasonCode.
        /// </summary>
        ///
        RECORDER_STATE_ERROR = -1,

        ///
        /// <summary>
        /// 2: Audio/video stream recording starts.
        /// </summary>
        ///
        RECORDER_STATE_START = 2,

        ///
        /// <summary>
        /// 3: Audio/video stream recording stops.
        /// </summary>
        ///
        RECORDER_STATE_STOP = 3,

    }

    ///
    /// <summary>
    /// Reasons for recording state errors.
    /// </summary>
    ///
    public enum RecorderReasonCode
    {
        ///
        /// <summary>
        /// 0: Everything is normal.
        /// </summary>
        ///
        RECORDER_REASON_NONE = 0,

        ///
        /// <summary>
        /// 1: Failed to write to recording file.
        /// </summary>
        ///
        RECORDER_REASON_WRITE_FAILED = 1,

        ///
        /// <summary>
        /// 2: No audio/video stream to record or the stream was interrupted for more than 5 seconds.
        /// </summary>
        ///
        RECORDER_REASON_NO_STREAM = 2,

        ///
        /// <summary>
        /// 3: Recording duration exceeds the maximum limit.
        /// </summary>
        ///
        RECORDER_REASON_OVER_MAX_DURATION = 3,

        ///
        /// <summary>
        /// 4: Recording configuration changed.
        /// </summary>
        ///
        RECORDER_REASON_CONFIG_CHANGED = 4,

    }

    ///
    /// @ignore
    ///
    public class MediaRecorderConfiguration
    {
        ///
        /// @ignore
        ///
        public string storagePath;

        ///
        /// @ignore
        ///
        public MediaRecorderContainerFormat containerFormat;

        ///
        /// @ignore
        ///
        public MediaRecorderStreamType streamType;

        ///
        /// @ignore
        ///
        public int maxDurationMs;

        ///
        /// @ignore
        ///
        public int recorderInfoUpdateInterval;

        ///
        /// @ignore
        ///
        public int width;

        ///
        /// @ignore
        ///
        public int height;

        ///
        /// @ignore
        ///
        public int fps;

        ///
        /// @ignore
        ///
        public int sample_rate;

        ///
        /// @ignore
        ///
        public int channel_num;

        ///
        /// @ignore
        ///
        public VIDEO_SOURCE_TYPE videoSourceType;

        public MediaRecorderConfiguration()
        {
            this.storagePath = "";
            this.containerFormat = MediaRecorderContainerFormat.FORMAT_MP4;
            this.streamType = MediaRecorderStreamType.STREAM_TYPE_BOTH;
            this.maxDurationMs = 120000;
            this.recorderInfoUpdateInterval = 0;
            this.width = 1280;
            this.height = 720;
            this.fps = 30;
            this.sample_rate = 48000;
            this.channel_num = 1;
            this.videoSourceType = VIDEO_SOURCE_TYPE.VIDEO_SOURCE_CAMERA_PRIMARY;
        }

        public MediaRecorderConfiguration(string path, MediaRecorderContainerFormat format, MediaRecorderStreamType type, int duration, int interval)
        {
            this.storagePath = path;
            this.containerFormat = format;
            this.streamType = type;
            this.maxDurationMs = duration;
            this.recorderInfoUpdateInterval = interval;
            this.width = 1280;
            this.height = 720;
            this.fps = 30;
            this.sample_rate = 48000;
            this.channel_num = 1;
            this.videoSourceType = VIDEO_SOURCE_TYPE.VIDEO_SOURCE_CAMERA_PRIMARY;
        }

        public MediaRecorderConfiguration(string storagePath, MediaRecorderContainerFormat containerFormat, MediaRecorderStreamType streamType, int maxDurationMs, int recorderInfoUpdateInterval, int width, int height, int fps, int sample_rate, int channel_num, VIDEO_SOURCE_TYPE videoSourceType)
        {
            this.storagePath = storagePath;
            this.containerFormat = containerFormat;
            this.streamType = streamType;
            this.maxDurationMs = maxDurationMs;
            this.recorderInfoUpdateInterval = recorderInfoUpdateInterval;
            this.width = width;
            this.height = height;
            this.fps = fps;
            this.sample_rate = sample_rate;
            this.channel_num = channel_num;
            this.videoSourceType = videoSourceType;
        }
    }

    ///
    /// @ignore
    ///
    public class RecorderInfo
    {
        ///
        /// @ignore
        ///
        public string fileName;

        ///
        /// @ignore
        ///
        public uint durationMs;

        ///
        /// @ignore
        ///
        public uint fileSize;

        public RecorderInfo()
        {
            this.fileName = "";
            this.durationMs = 0;
            this.fileSize = 0;
        }

        public RecorderInfo(string name, uint dur, uint size)
        {
            this.fileName = name;
            this.durationMs = dur;
            this.fileSize = size;
        }

    }

}
