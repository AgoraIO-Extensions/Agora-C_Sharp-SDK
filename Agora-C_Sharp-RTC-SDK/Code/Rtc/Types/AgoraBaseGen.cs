#region Generated by `terra/node/src/rtc/struct_enumz/renderers.ts`. DO NOT MODIFY BY HAND.
#endregion

using System;
using Agora.Rtc.LitJson;
using view_t = System.UInt64;
using uint8_t = System.Byte;
using int16_t = System.Int16;

namespace Agora.Rtc
{
    ///
    /// <summary>
    /// Channel profile.
    /// </summary>
    ///
    public enum CHANNEL_PROFILE_TYPE
    {
        ///
        /// <summary>
        /// 0: Communication profile. Agora recommends using the live broadcasting profile for better audio and video experience.
        /// </summary>
        ///
        CHANNEL_PROFILE_COMMUNICATION = 0,

        ///
        /// <summary>
        /// 1: (Default) Live broadcasting profile.
        /// </summary>
        ///
        CHANNEL_PROFILE_LIVE_BROADCASTING = 1,

        ///
        /// <summary>
        /// 2: Gaming profile. Deprecated: Use CHANNEL_PROFILE_LIVE_BROADCASTING instead.
        /// </summary>
        ///
        [Obsolete("This profile is deprecated.")]
        CHANNEL_PROFILE_GAME = 2,

        ///
        /// <summary>
        /// 3: Interactive profile. This profile is optimized for low latency. If your scenario involves frequent user interaction, it is recommended. Deprecated: Use CHANNEL_PROFILE_LIVE_BROADCASTING instead.
        /// </summary>
        ///
        [Obsolete("This profile is deprecated.")]
        CHANNEL_PROFILE_CLOUD_GAMING = 3,

        ///
        /// @ignore
        ///
        [Obsolete("This profile is deprecated.")]
        CHANNEL_PROFILE_COMMUNICATION_1v1 = 4,

    }

    ///
    /// @ignore
    ///
    public enum WARN_CODE_TYPE
    {
        ///
        /// @ignore
        ///
        WARN_INVALID_VIEW = 8,

        ///
        /// @ignore
        ///
        WARN_INIT_VIDEO = 16,

        ///
        /// @ignore
        ///
        WARN_PENDING = 20,

        ///
        /// @ignore
        ///
        WARN_NO_AVAILABLE_CHANNEL = 103,

        ///
        /// @ignore
        ///
        WARN_LOOKUP_CHANNEL_TIMEOUT = 104,

        ///
        /// @ignore
        ///
        WARN_LOOKUP_CHANNEL_REJECTED = 105,

        ///
        /// @ignore
        ///
        WARN_OPEN_CHANNEL_TIMEOUT = 106,

        ///
        /// @ignore
        ///
        WARN_OPEN_CHANNEL_REJECTED = 107,

        ///
        /// @ignore
        ///
        WARN_SWITCH_LIVE_VIDEO_TIMEOUT = 111,

        ///
        /// @ignore
        ///
        WARN_SET_CLIENT_ROLE_TIMEOUT = 118,

        ///
        /// @ignore
        ///
        WARN_OPEN_CHANNEL_INVALID_TICKET = 121,

        ///
        /// @ignore
        ///
        WARN_OPEN_CHANNEL_TRY_NEXT_VOS = 122,

        ///
        /// @ignore
        ///
        WARN_CHANNEL_CONNECTION_UNRECOVERABLE = 131,

        ///
        /// @ignore
        ///
        WARN_CHANNEL_CONNECTION_IP_CHANGED = 132,

        ///
        /// @ignore
        ///
        WARN_CHANNEL_CONNECTION_PORT_CHANGED = 133,

        ///
        /// @ignore
        ///
        WARN_CHANNEL_SOCKET_ERROR = 134,

        ///
        /// @ignore
        ///
        WARN_AUDIO_MIXING_OPEN_ERROR = 701,

        ///
        /// @ignore
        ///
        WARN_ADM_RUNTIME_PLAYOUT_WARNING = 1014,

        ///
        /// @ignore
        ///
        WARN_ADM_RUNTIME_RECORDING_WARNING = 1016,

        ///
        /// @ignore
        ///
        WARN_ADM_RECORD_AUDIO_SILENCE = 1019,

        ///
        /// @ignore
        ///
        WARN_ADM_PLAYOUT_MALFUNCTION = 1020,

        ///
        /// @ignore
        ///
        WARN_ADM_RECORD_MALFUNCTION = 1021,

        ///
        /// @ignore
        ///
        WARN_ADM_RECORD_AUDIO_LOWLEVEL = 1031,

        ///
        /// @ignore
        ///
        WARN_ADM_PLAYOUT_AUDIO_LOWLEVEL = 1032,

        ///
        /// @ignore
        ///
        WARN_ADM_WINDOWS_NO_DATA_READY_EVENT = 1040,

        ///
        /// @ignore
        ///
        WARN_APM_HOWLING = 1051,

        ///
        /// @ignore
        ///
        WARN_ADM_GLITCH_STATE = 1052,

        ///
        /// @ignore
        ///
        WARN_ADM_IMPROPER_SETTINGS = 1053,

        ///
        /// @ignore
        ///
        WARN_ADM_POP_STATE = 1055,

        ///
        /// @ignore
        ///
        WARN_ADM_WIN_CORE_NO_RECORDING_DEVICE = 1322,

        ///
        /// @ignore
        ///
        WARN_ADM_WIN_CORE_NO_PLAYOUT_DEVICE = 1323,

        ///
        /// @ignore
        ///
        WARN_ADM_WIN_CORE_IMPROPER_CAPTURE_RELEASE = 1324,

    }

    ///
    /// <summary>
    /// Error codes.
    /// 
    /// Error codes indicate that the SDK has encountered an unrecoverable error and requires intervention from the application. For example, an error is returned when the camera fails to open, and the app needs to notify the user that the camera cannot be used.
    /// </summary>
    ///
    public enum ERROR_CODE_TYPE
    {
        ///
        /// <summary>
        /// 0: No error.
        /// </summary>
        ///
        ERR_OK = 0,

        ///
        /// <summary>
        /// 1: A general error (no specific classification of the cause). Please try calling the method again.
        /// </summary>
        ///
        ERR_FAILED = 1,

        ///
        /// <summary>
        /// 2: Invalid parameter set in the method. For example, the specified channel name contains illegal characters. Please reset the parameters.
        /// </summary>
        ///
        ERR_INVALID_ARGUMENT = 2,

        ///
        /// <summary>
        /// 3: SDK is not ready. Possible reasons include: IRtcEngine initialization failed. Please reinitialize IRtcEngine.
        ///  User has not joined the channel when calling the method. Please check the method call logic.
        ///  User has not left the channel when calling Rate or Complain. Please check the method call logic.
        ///  Audio module is not enabled.
        ///  Incomplete assembly.
        /// </summary>
        ///
        ERR_NOT_READY = 3,

        ///
        /// <summary>
        /// 4: The current state of IRtcEngine does not support the operation. Possible reasons include:
        ///  When using built-in encryption, the encryption mode is incorrect, or loading the external encryption library failed. Please check whether the encryption enum value is correct or reload the external encryption library.
        /// </summary>
        ///
        ERR_NOT_SUPPORTED = 4,

        ///
        /// <summary>
        /// 5: This method call was rejected. Possible reasons include: IRtcEngine initialization failed. Please reinitialize IRtcEngine.
        ///  Channel name is set to an empty string "" when joining the channel. Please reset the channel name.
        ///  In multi-channel scenarios, the specified channel name already exists when calling JoinChannelEx. Please reset the channel name.
        /// </summary>
        ///
        ERR_REFUSED = 5,

        ///
        /// <summary>
        /// 6: Buffer size is insufficient to hold the returned data.
        /// </summary>
        ///
        ERR_BUFFER_TOO_SMALL = 6,

        ///
        /// <summary>
        /// 7: IRtcEngine method called before initialization. Please ensure that the IRtcEngine object is created and initialized before calling the method.
        /// </summary>
        ///
        ERR_NOT_INITIALIZED = 7,

        ///
        /// <summary>
        /// 8: Invalid current state.
        /// </summary>
        ///
        ERR_INVALID_STATE = 8,

        ///
        /// <summary>
        /// 9: No permission to operate. Please check whether the user has granted the app permission to use audio and video devices.
        /// </summary>
        ///
        ERR_NO_PERMISSION = 9,

        ///
        /// <summary>
        /// 10: Method call timed out. Some method calls require a result from the SDK. If the SDK takes too long (over 10 seconds) to respond, this error occurs.
        /// </summary>
        ///
        ERR_TIMEDOUT = 10,

        ///
        /// @ignore
        ///
        ERR_CANCELED = 11,

        ///
        /// @ignore
        ///
        ERR_TOO_OFTEN = 12,

        ///
        /// @ignore
        ///
        ERR_BIND_SOCKET = 13,

        ///
        /// @ignore
        ///
        ERR_NET_DOWN = 14,

        ///
        /// <summary>
        /// 17: Join channel request rejected. Possible reasons include:
        ///  User is already in the channel. It is recommended to use the OnConnectionStateChanged callback to determine whether the user is in the channel. Do not call this method again to join the channel unless receiving CONNECTION_STATE_DISCONNECTED (1).
        ///  After calling StartEchoTest for a call test, the user tries to join a channel without first calling StopEchoTest to end the test. You must call StopEchoTest before joining a channel.
        /// </summary>
        ///
        ERR_JOIN_CHANNEL_REJECTED = 17,

        ///
        /// <summary>
        /// 18: Failed to leave the channel. Possible reasons include:
        ///  User has already left the channel before calling LeaveChannel [2/2]. You can stop calling this method.
        ///  User has not joined the channel but calls LeaveChannel [2/2]. No additional action is required in this case.
        /// </summary>
        ///
        ERR_LEAVE_CHANNEL_REJECTED = 18,

        ///
        /// <summary>
        /// 19: Resource already in use and cannot be reused.
        /// </summary>
        ///
        ERR_ALREADY_IN_USE = 19,

        ///
        /// <summary>
        /// 20: SDK aborted the request, possibly due to too many requests.
        /// </summary>
        ///
        ERR_ABORTED = 20,

        ///
        /// <summary>
        /// 21: On Windows, specific firewall settings cause IRtcEngine initialization to fail and crash.
        /// </summary>
        ///
        ERR_INIT_NET_ENGINE = 21,

        ///
        /// <summary>
        /// 22: SDK failed to allocate resources, possibly due to excessive app resource usage or system resource exhaustion.
        /// </summary>
        ///
        ERR_RESOURCE_LIMITED = 22,

        ///
        /// @ignore
        ///
        ERR_FUNC_IS_PROHIBITED = 23,

        ///
        /// <summary>
        /// 101: Invalid App ID. Please use a valid App ID to rejoin the channel.
        /// </summary>
        ///
        ERR_INVALID_APP_ID = 101,

        ///
        /// <summary>
        /// 102: Invalid channel name. Possible reason is incorrect data type of the parameter. Please use a valid channel name to rejoin the channel.
        /// </summary>
        ///
        ERR_INVALID_CHANNEL_NAME = 102,

        ///
        /// <summary>
        /// 103: Unable to acquire server resources in the current region. Try specifying a different region when initializing IRtcEngine.
        /// </summary>
        ///
        ERR_NO_SERVER_RESOURCES = 103,

        ///
        /// <summary>
        /// 109: The current Token has expired and is no longer valid. Please request a new Token from the server and call RenewToken to update it. Deprecated: This enum is deprecated. Use CONNECTION_CHANGED_TOKEN_EXPIRED (9) in the OnConnectionStateChanged callback instead.
        /// </summary>
        ///
        ERR_TOKEN_EXPIRED = 109,

        ///
        /// <summary>
        /// Deprecated: This enum is deprecated. Use CONNECTION_CHANGED_INVALID_TOKEN (8) in the OnConnectionStateChanged callback instead. 110: Invalid Token. Possible reasons include:
        ///  App certificate is enabled in the console, but App ID + Token authentication is not used. When the project enables the App certificate, Token authentication must be used.
        ///  The uid field used to generate the Token does not match the uid used when the user joins the channel.
        /// </summary>
        ///
        ERR_INVALID_TOKEN = 110,

        ///
        /// <summary>
        /// 111: Network connection interrupted. SDK lost connection for more than 4 seconds after establishing a connection with the server.
        /// </summary>
        ///
        ERR_CONNECTION_INTERRUPTED = 111,

        ///
        /// <summary>
        /// 112: Network connection lost. Connection interrupted and SDK fails to reconnect to the server within 10 seconds.
        /// </summary>
        ///
        ERR_CONNECTION_LOST = 112,

        ///
        /// <summary>
        /// 113: User is not in the channel when calling SendStreamMessage.
        /// </summary>
        ///
        ERR_NOT_IN_CHANNEL = 113,

        ///
        /// <summary>
        /// 114: Data length exceeds 1 KB when calling SendStreamMessage.
        /// </summary>
        ///
        ERR_SIZE_TOO_LARGE = 114,

        ///
        /// <summary>
        /// 115: Data sending frequency exceeds the limit (6 KB/s) when calling SendStreamMessage.
        /// </summary>
        ///
        ERR_BITRATE_LIMIT = 115,

        ///
        /// <summary>
        /// 116: Number of data streams exceeds the limit (5) when calling CreateDataStream [2/2].
        /// </summary>
        ///
        ERR_TOO_MANY_DATA_STREAMS = 116,

        ///
        /// <summary>
        /// 117: Data stream sending timed out.
        /// </summary>
        ///
        ERR_STREAM_MESSAGE_TIMEOUT = 117,

        ///
        /// <summary>
        /// 119: Failed to switch user role. Please try rejoining the channel.
        /// </summary>
        ///
        ERR_SET_CLIENT_ROLE_NOT_AUTHORIZED = 119,

        ///
        /// <summary>
        /// 120: Media stream decryption failed. Possibly due to incorrect key used when joining the channel. Please check the key entered when joining or guide the user to rejoin the channel.
        /// </summary>
        ///
        ERR_DECRYPTION_FAILED = 120,

        ///
        /// <summary>
        /// 121: Invalid user ID.
        /// </summary>
        ///
        ERR_INVALID_USER_ID = 121,

        ///
        /// <summary>
        /// 122: Data stream decryption failed. Possibly due to incorrect key used when joining the channel. Please check the key entered when joining or guide the user to rejoin the channel.
        /// </summary>
        ///
        ERR_DATASTREAM_DECRYPTION_FAILED = 122,

        ///
        /// <summary>
        /// 123: This user is banned by the server.
        /// </summary>
        ///
        ERR_CLIENT_IS_BANNED_BY_SERVER = 123,

        ///
        /// <summary>
        /// 130: SDK does not support publishing encrypted streams to CDN.
        /// </summary>
        ///
        ERR_ENCRYPTED_STREAM_NOT_ALLOWED_PUBLISH = 130,

        ///
        /// @ignore
        ///
        ERR_LICENSE_CREDENTIAL_INVALID = 131,

        ///
        /// <summary>
        /// 134: Invalid user account, possibly due to invalid parameters.
        /// </summary>
        ///
        ERR_INVALID_USER_ACCOUNT = 134,

        ///
        /// @ignore
        ///
        ERR_MODULE_NOT_FOUND = 157,

        ///
        /// @ignore
        ///
        ERR_CERT_RAW = 157,

        ///
        /// @ignore
        ///
        ERR_CERT_JSON_PART = 158,

        ///
        /// @ignore
        ///
        ERR_CERT_JSON_INVAL = 159,

        ///
        /// @ignore
        ///
        ERR_CERT_JSON_NOMEM = 160,

        ///
        /// @ignore
        ///
        ERR_CERT_CUSTOM = 161,

        ///
        /// @ignore
        ///
        ERR_CERT_CREDENTIAL = 162,

        ///
        /// @ignore
        ///
        ERR_CERT_SIGN = 163,

        ///
        /// @ignore
        ///
        ERR_CERT_FAIL = 164,

        ///
        /// @ignore
        ///
        ERR_CERT_BUF = 165,

        ///
        /// @ignore
        ///
        ERR_CERT_NULL = 166,

        ///
        /// @ignore
        ///
        ERR_CERT_DUEDATE = 167,

        ///
        /// @ignore
        ///
        ERR_CERT_REQUEST = 168,

        ///
        /// <summary>
        /// 200: Unsupported PCM format.
        /// </summary>
        ///
        ERR_PCMSEND_FORMAT = 200,

        ///
        /// <summary>
        /// 201: Buffer overflow due to PCM send rate being too fast.
        /// </summary>
        ///
        ERR_PCMSEND_BUFFEROVERFLOW = 201,

        ///
        /// @ignore
        ///
        ERR_RDT_USER_NOT_EXIST = 250,

        ///
        /// @ignore
        ///
        ERR_RDT_USER_NOT_READY = 251,

        ///
        /// @ignore
        ///
        ERR_RDT_DATA_BLOCKED = 252,

        ///
        /// @ignore
        ///
        ERR_RDT_CMD_EXCEED_LIMIT = 253,

        ///
        /// @ignore
        ///
        ERR_RDT_DATA_EXCEED_LIMIT = 254,

        ///
        /// @ignore
        ///
        ERR_RDT_ENCRYPTION = 255,

        ///
        /// @ignore
        ///
        ERR_LOGIN_ALREADY_LOGIN = 428,

        ///
        /// <summary>
        /// 1001: Failed to load media engine.
        /// </summary>
        ///
        ERR_LOAD_MEDIA_ENGINE = 1001,

        ///
        /// <summary>
        /// 1005: Audio device error (unspecified). Please check whether the audio device is occupied by another application or try rejoining the channel.
        /// </summary>
        ///
        ERR_ADM_GENERAL_ERROR = 1005,

        ///
        /// <summary>
        /// 1008: Error initializing playback device. Please check whether the playback device is occupied by another application or try rejoining the channel.
        /// </summary>
        ///
        ERR_ADM_INIT_PLAYOUT = 1008,

        ///
        /// <summary>
        /// 1009: Error starting playback device. Please check whether the playback device is functioning properly.
        /// </summary>
        ///
        ERR_ADM_START_PLAYOUT = 1009,

        ///
        /// <summary>
        /// 1010: Error stopping playback device.
        /// </summary>
        ///
        ERR_ADM_STOP_PLAYOUT = 1010,

        ///
        /// <summary>
        /// 1011: Error initializing recording device. Please check whether the recording device is functioning properly or try rejoining the channel.
        /// </summary>
        ///
        ERR_ADM_INIT_RECORDING = 1011,

        ///
        /// <summary>
        /// 1012: Error starting recording device. Please check whether the recording device is functioning properly.
        /// </summary>
        ///
        ERR_ADM_START_RECORDING = 1012,

        ///
        /// <summary>
        /// 1013: Error stopping recording device.
        /// </summary>
        ///
        ERR_ADM_STOP_RECORDING = 1013,

        ///
        /// <summary>
        /// 1501: No permission to use the camera. Please check whether camera access is enabled.
        /// </summary>
        ///
        ERR_VDM_CAMERA_NOT_AUTHORIZED = 1501,

    }

    ///
    /// @ignore
    ///
    public enum LICENSE_ERROR_TYPE
    {
        ///
        /// @ignore
        ///
        LICENSE_ERR_INVALID = 1,

        ///
        /// @ignore
        ///
        LICENSE_ERR_EXPIRE = 2,

        ///
        /// @ignore
        ///
        LICENSE_ERR_MINUTES_EXCEED = 3,

        ///
        /// @ignore
        ///
        LICENSE_ERR_LIMITED_PERIOD = 4,

        ///
        /// @ignore
        ///
        LICENSE_ERR_DIFF_DEVICES = 5,

        ///
        /// @ignore
        ///
        LICENSE_ERR_INTERNAL = 99,

    }

    ///
    /// <summary>
    /// SDK operation permissions for Audio Session.
    /// </summary>
    ///
    public enum AUDIO_SESSION_OPERATION_RESTRICTION
    {
        ///
        /// <summary>
        /// 0: No restriction. SDK can modify the Audio Session.
        /// </summary>
        ///
        AUDIO_SESSION_OPERATION_RESTRICTION_NONE = 0,

        ///
        /// <summary>
        /// 1: SDK cannot change the Audio Session category.
        /// </summary>
        ///
        AUDIO_SESSION_OPERATION_RESTRICTION_SET_CATEGORY = 1,

        ///
        /// <summary>
        /// 2: SDK cannot change the Audio Session's category, mode, or categoryOptions.
        /// </summary>
        ///
        AUDIO_SESSION_OPERATION_RESTRICTION_CONFIGURE_SESSION = 1 << 1,

        ///
        /// <summary>
        /// 4: When leaving the channel, SDK keeps the Audio Session active, e.g., for background audio playback.
        /// </summary>
        ///
        AUDIO_SESSION_OPERATION_RESTRICTION_DEACTIVATE_SESSION = 1 << 2,

        ///
        /// <summary>
        /// 128: Fully restricts SDK from modifying Audio Session. SDK can no longer make any changes.
        /// </summary>
        ///
        AUDIO_SESSION_OPERATION_RESTRICTION_ALL = 1 << 7,

    }

    ///
    /// <summary>
    /// Reason for user going offline.
    /// </summary>
    ///
    public enum USER_OFFLINE_REASON_TYPE
    {
        ///
        /// <summary>
        /// 0: User left voluntarily.
        /// </summary>
        ///
        USER_OFFLINE_QUIT = 0,

        ///
        /// <summary>
        /// 1: Timed out due to not receiving packets from the peer for a long time. Since the SDK uses an unreliable channel, it is also possible that the peer left the channel voluntarily, but the local side did not receive the leave message and mistakenly judged it as a timeout.
        /// </summary>
        ///
        USER_OFFLINE_DROPPED = 1,

        ///
        /// <summary>
        /// 2: The user's role switched from host to audience.
        /// </summary>
        ///
        USER_OFFLINE_BECOME_AUDIENCE = 2,

    }

    ///
    /// @ignore
    ///
    public enum INTERFACE_ID_TYPE
    {
        ///
        /// @ignore
        ///
        AGORA_IID_AUDIO_DEVICE_MANAGER = 1,

        ///
        /// @ignore
        ///
        AGORA_IID_VIDEO_DEVICE_MANAGER = 2,

        ///
        /// @ignore
        ///
        AGORA_IID_PARAMETER_ENGINE = 3,

        ///
        /// @ignore
        ///
        AGORA_IID_MEDIA_ENGINE = 4,

        ///
        /// @ignore
        ///
        AGORA_IID_AUDIO_ENGINE = 5,

        ///
        /// @ignore
        ///
        AGORA_IID_VIDEO_ENGINE = 6,

        ///
        /// @ignore
        ///
        AGORA_IID_RTC_CONNECTION = 7,

        ///
        /// @ignore
        ///
        AGORA_IID_SIGNALING_ENGINE = 8,

        ///
        /// @ignore
        ///
        AGORA_IID_MEDIA_ENGINE_REGULATOR = 9,

        ///
        /// @ignore
        ///
        AGORA_IID_LOCAL_SPATIAL_AUDIO = 11,

        ///
        /// @ignore
        ///
        AGORA_IID_STATE_SYNC = 13,

        ///
        /// @ignore
        ///
        AGORA_IID_META_SERVICE = 14,

        ///
        /// @ignore
        ///
        AGORA_IID_MUSIC_CONTENT_CENTER = 15,

        ///
        /// @ignore
        ///
        AGORA_IID_H265_TRANSCODER = 16,

    }

    ///
    /// <summary>
    /// Network quality.
    /// </summary>
    ///
    public enum QUALITY_TYPE
    {
        ///
        /// <summary>
        /// 0: Network quality unknown.
        /// </summary>
        ///
        [Obsolete("This member is deprecated.")]
        QUALITY_UNKNOWN = 0,

        ///
        /// <summary>
        /// 1: Excellent network quality.
        /// </summary>
        ///
        QUALITY_EXCELLENT = 1,

        ///
        /// <summary>
        /// 2: Subjectively similar to excellent, but bitrate may be slightly lower.
        /// </summary>
        ///
        QUALITY_GOOD = 2,

        ///
        /// <summary>
        /// 3: Slight issues in user experience but communication is not affected.
        /// </summary>
        ///
        QUALITY_POOR = 3,

        ///
        /// <summary>
        /// 4: Communication is possible but not smooth.
        /// </summary>
        ///
        QUALITY_BAD = 4,

        ///
        /// <summary>
        /// 5: Very poor network quality, communication is barely possible.
        /// </summary>
        ///
        QUALITY_VBAD = 5,

        ///
        /// <summary>
        /// 6: Communication is not possible.
        /// </summary>
        ///
        QUALITY_DOWN = 6,

        ///
        /// @ignore
        ///
        QUALITY_UNSUPPORTED = 7,

        ///
        /// <summary>
        /// 8: Network quality detection in progress.
        /// </summary>
        ///
        QUALITY_DETECTING = 8,

    }

    ///
    /// @ignore
    ///
    public enum FIT_MODE_TYPE
    {
        ///
        /// @ignore
        ///
        MODE_COVER = 1,

        ///
        /// @ignore
        ///
        MODE_CONTAIN = 2,

    }

    ///
    /// <summary>
    /// Clockwise video rotation information.
    /// </summary>
    ///
    public enum VIDEO_ORIENTATION
    {
        ///
        /// <summary>
        /// 0: (Default) Rotated 0 degrees clockwise.
        /// </summary>
        ///
        VIDEO_ORIENTATION_0 = 0,

        ///
        /// <summary>
        /// 90: Rotated 90 degrees clockwise.
        /// </summary>
        ///
        VIDEO_ORIENTATION_90 = 90,

        ///
        /// <summary>
        /// 180: Rotated 180 degrees clockwise.
        /// </summary>
        ///
        VIDEO_ORIENTATION_180 = 180,

        ///
        /// <summary>
        /// 270: Rotated 270 degrees clockwise.
        /// </summary>
        ///
        VIDEO_ORIENTATION_270 = 270,

    }

    ///
    /// <summary>
    /// Video frame rate.
    /// </summary>
    ///
    public enum FRAME_RATE
    {
        ///
        /// <summary>
        /// 1: 1 fps.
        /// </summary>
        ///
        FRAME_RATE_FPS_1 = 1,

        ///
        /// <summary>
        /// 7: 7 fps.
        /// </summary>
        ///
        FRAME_RATE_FPS_7 = 7,

        ///
        /// <summary>
        /// 10: 10 fps.
        /// </summary>
        ///
        FRAME_RATE_FPS_10 = 10,

        ///
        /// <summary>
        /// 15: 15 fps.
        /// </summary>
        ///
        FRAME_RATE_FPS_15 = 15,

        ///
        /// <summary>
        /// 24: 24 fps.
        /// </summary>
        ///
        FRAME_RATE_FPS_24 = 24,

        ///
        /// <summary>
        /// 30: 30 fps.
        /// </summary>
        ///
        FRAME_RATE_FPS_30 = 30,

        ///
        /// <summary>
        /// 60: 60 fps. (Windows and macOS only)
        /// </summary>
        ///
        FRAME_RATE_FPS_60 = 60,

    }

    ///
    /// @ignore
    ///
    public enum FRAME_WIDTH
    {
        ///
        /// @ignore
        ///
        FRAME_WIDTH_960 = 960,

    }

    ///
    /// @ignore
    ///
    public enum FRAME_HEIGHT
    {
        ///
        /// @ignore
        ///
        FRAME_HEIGHT_540 = 540,

    }

    ///
    /// <summary>
    /// Video frame type.
    /// </summary>
    ///
    public enum VIDEO_FRAME_TYPE
    {
        ///
        /// <summary>
        /// 0: Blank frame.
        /// </summary>
        ///
        VIDEO_FRAME_TYPE_BLANK_FRAME = 0,

        ///
        /// <summary>
        /// 3: Key frame.
        /// </summary>
        ///
        VIDEO_FRAME_TYPE_KEY_FRAME = 3,

        ///
        /// <summary>
        /// 4: Delta frame.
        /// </summary>
        ///
        VIDEO_FRAME_TYPE_DELTA_FRAME = 4,

        ///
        /// <summary>
        /// 5: B frame.
        /// </summary>
        ///
        VIDEO_FRAME_TYPE_B_FRAME = 5,

        ///
        /// <summary>
        /// 6: Droppable frame.
        /// </summary>
        ///
        VIDEO_FRAME_TYPE_DROPPABLE_FRAME = 6,

        ///
        /// <summary>
        /// Unknown frame.
        /// </summary>
        ///
        VIDEO_FRAME_TYPE_UNKNOW,

    }

    ///
    /// <summary>
    /// Video encoding orientation mode.
    /// </summary>
    ///
    public enum ORIENTATION_MODE
    {
        ///
        /// <summary>
        /// 0: (Default) In this mode, the SDK outputs video with the same orientation as the captured video. The receiver rotates the video based on the rotation information. This mode is suitable when the receiver can adjust the video orientation.
        ///  If the captured video is in landscape mode, the output video is also in landscape mode.
        ///  If the captured video is in portrait mode, the output video is also in portrait mode.
        /// </summary>
        ///
        ORIENTATION_MODE_ADAPTIVE = 0,

        ///
        /// <summary>
        /// 1: In this mode, the SDK outputs video in fixed landscape mode. If the captured video is in portrait mode, the video encoder crops it. This mode is suitable when the receiver cannot adjust the video orientation, such as in CDN streaming scenarios.
        /// </summary>
        ///
        ORIENTATION_MODE_FIXED_LANDSCAPE = 1,

        ///
        /// <summary>
        /// 2: In this mode, the SDK outputs video in fixed portrait mode. If the captured video is in landscape mode, the video encoder crops it. This mode is suitable when the receiver cannot adjust the video orientation, such as in CDN streaming scenarios.
        /// </summary>
        ///
        ORIENTATION_MODE_FIXED_PORTRAIT = 2,

    }

    ///
    /// <summary>
    /// Video encoding degradation preference when bandwidth is limited.
    /// </summary>
    ///
    public enum DEGRADATION_PREFERENCE
    {
        ///
        /// <summary>
        /// -1: (Default) Auto mode. The SDK automatically selects MAINTAIN_FRAMERATE, MAINTAIN_BALANCED, or MAINTAIN_RESOLUTION based on your video scenario to achieve optimal overall quality experience (QoE).
        /// </summary>
        ///
        MAINTAIN_AUTO = -1,

        ///
        /// <summary>
        /// 0: When bandwidth is limited, video encoding prioritizes reducing frame rate while maintaining resolution. This preference is suitable for scenarios prioritizing image quality. Deprecated: This enumeration is deprecated. Use other enumerations instead.
        /// </summary>
        ///
        MAINTAIN_QUALITY = 0,

        ///
        /// <summary>
        /// 1: When bandwidth is limited, video encoding prioritizes reducing resolution while maintaining frame rate. This preference suits scenarios prioritizing smoothness and tolerating reduced image quality.
        /// </summary>
        ///
        MAINTAIN_FRAMERATE = 1,

        ///
        /// <summary>
        /// 2: When bandwidth is limited, video encoding reduces both frame rate and resolution. The degradation level of MAINTAIN_BALANCED is lower than that of MAINTAIN_QUALITY and MAINTAIN_FRAMERATE, suitable for scenarios with limited smoothness and image quality. The resolution of the locally sent video may change. Remote users must be able to handle this. See OnVideoSizeChanged.
        /// </summary>
        ///
        MAINTAIN_BALANCED = 2,

        ///
        /// <summary>
        /// 3: When bandwidth is limited, video encoding prioritizes reducing frame rate while keeping resolution unchanged. This preference is suitable for scenarios prioritizing image quality.
        /// </summary>
        ///
        MAINTAIN_RESOLUTION = 3,

        ///
        /// @ignore
        ///
        DISABLED = 100,

    }

    ///
    /// <summary>
    /// Video dimensions.
    /// </summary>
    ///
    public class VideoDimensions
    {
        ///
        /// <summary>
        /// Video width in pixels.
        /// </summary>
        ///
        public int width;

        ///
        /// <summary>
        /// Video height in pixels.
        /// </summary>
        ///
        public int height;

        public VideoDimensions()
        {
            this.width = 640;
            this.height = 480;
        }

        public VideoDimensions(int w, int h)
        {
            this.width = w;
            this.height = h;
        }

    }

    ///
    /// <summary>
    /// Maximum frame rate supported by the screen sharing device.
    /// </summary>
    ///
    public enum SCREEN_CAPTURE_FRAMERATE_CAPABILITY
    {
        ///
        /// <summary>
        /// 0: Supports up to 15 fps.
        /// </summary>
        ///
        SCREEN_CAPTURE_FRAMERATE_CAPABILITY_15_FPS = 0,

        ///
        /// <summary>
        /// 1: Supports up to 30 fps.
        /// </summary>
        ///
        SCREEN_CAPTURE_FRAMERATE_CAPABILITY_30_FPS = 1,

        ///
        /// <summary>
        /// 2: Supports up to 60 fps.
        /// </summary>
        ///
        SCREEN_CAPTURE_FRAMERATE_CAPABILITY_60_FPS = 2,

    }

    ///
    /// <summary>
    /// Video codec capability level.
    /// </summary>
    ///
    public enum VIDEO_CODEC_CAPABILITY_LEVEL
    {
        ///
        /// <summary>
        /// -1: Unsupported video type. Currently, only video in H.264 and H.265 formats is supported for querying. If the video is in another format, this value is returned.
        /// </summary>
        ///
        CODEC_CAPABILITY_LEVEL_UNSPECIFIED = -1,

        ///
        /// <summary>
        /// 5: Basic codec support, i.e., encoding and decoding for video up to 1080p and 30 fps.
        /// </summary>
        ///
        CODEC_CAPABILITY_LEVEL_BASIC_SUPPORT = 5,

        ///
        /// <summary>
        /// 10: Supports encoding and decoding video up to 1080p and 30 fps.
        /// </summary>
        ///
        CODEC_CAPABILITY_LEVEL_1080P30FPS = 10,

        ///
        /// <summary>
        /// 20: Supports encoding and decoding video up to 1080p and 60 fps.
        /// </summary>
        ///
        CODEC_CAPABILITY_LEVEL_1080P60FPS = 20,

        ///
        /// <summary>
        /// 30: Supports encoding and decoding video up to 4K and 30 fps.
        /// </summary>
        ///
        CODEC_CAPABILITY_LEVEL_4K60FPS = 30,

    }

    ///
    /// <summary>
    /// Video codec format.
    /// </summary>
    ///
    public enum VIDEO_CODEC_TYPE
    {
        ///
        /// <summary>
        /// 0: (Default) No specific codec format. The SDK automatically selects a suitable codec format based on the current video stream resolution and device performance.
        /// </summary>
        ///
        VIDEO_CODEC_NONE = 0,

        ///
        /// <summary>
        /// 1: Standard VP8.
        /// </summary>
        ///
        VIDEO_CODEC_VP8 = 1,

        ///
        /// <summary>
        /// 2: Standard H.264.
        /// </summary>
        ///
        VIDEO_CODEC_H264 = 2,

        ///
        /// <summary>
        /// 3: Standard H.265.
        /// </summary>
        ///
        VIDEO_CODEC_H265 = 3,

        ///
        /// <summary>
        /// 6: Generic. This type is mainly used for transmitting raw video data (e.g., user-encrypted video frames). These frames are returned via callback and require you to decode and render them.
        /// </summary>
        ///
        VIDEO_CODEC_GENERIC = 6,

        ///
        /// @ignore
        ///
        [Obsolete("This codec type is deprecated.")]
        VIDEO_CODEC_GENERIC_H264 = 7,

        ///
        /// @ignore
        ///
        VIDEO_CODEC_AV1 = 12,

        ///
        /// @ignore
        ///
        VIDEO_CODEC_VP9 = 13,

        ///
        /// <summary>
        /// 20: Generic JPEG. Requires relatively low computational power and is suitable for IoT devices with limited resources.
        /// </summary>
        ///
        VIDEO_CODEC_GENERIC_JPEG = 20,

    }

    ///
    /// <summary>
    /// Camera focal length type.
    /// 
    /// (Android and iOS only)
    /// </summary>
    ///
    public enum CAMERA_FOCAL_LENGTH_TYPE
    {
        ///
        /// <summary>
        /// 0: (Default) Standard lens.
        /// </summary>
        ///
        CAMERA_FOCAL_LENGTH_DEFAULT = 0,

        ///
        /// <summary>
        /// 1: Wide-angle lens.
        /// </summary>
        ///
        CAMERA_FOCAL_LENGTH_WIDE_ANGLE = 1,

        ///
        /// <summary>
        /// 2: Ultra wide-angle lens.
        /// </summary>
        ///
        CAMERA_FOCAL_LENGTH_ULTRA_WIDE = 2,

        ///
        /// <summary>
        /// 3: (iOS only) Telephoto lens.
        /// </summary>
        ///
        CAMERA_FOCAL_LENGTH_TELEPHOTO = 3,

    }

    ///
    /// @ignore
    ///
    public enum TCcMode
    {
        ///
        /// @ignore
        ///
        CC_ENABLED,

        ///
        /// @ignore
        ///
        CC_DISABLED,

    }

    ///
    /// @ignore
    ///
    public class SenderOptions
    {
        ///
        /// @ignore
        ///
        public TCcMode ccMode;

        ///
        /// @ignore
        ///
        public VIDEO_CODEC_TYPE codecType;

        ///
        /// @ignore
        ///
        public int targetBitrate;

        public SenderOptions()
        {
            this.ccMode = TCcMode.CC_ENABLED;
            this.codecType = VIDEO_CODEC_TYPE.VIDEO_CODEC_H265;
            this.targetBitrate = 6500;
        }

        public SenderOptions(TCcMode ccMode, VIDEO_CODEC_TYPE codecType, int targetBitrate)
        {
            this.ccMode = ccMode;
            this.codecType = codecType;
            this.targetBitrate = targetBitrate;
        }
    }

    ///
    /// <summary>
    /// Audio codec format.
    /// </summary>
    ///
    public enum AUDIO_CODEC_TYPE
    {
        ///
        /// <summary>
        /// 1: OPUS.
        /// </summary>
        ///
        AUDIO_CODEC_OPUS = 1,

        ///
        /// <summary>
        /// 3: PCMA.
        /// </summary>
        ///
        AUDIO_CODEC_PCMA = 3,

        ///
        /// <summary>
        /// 4: PCMU.
        /// </summary>
        ///
        AUDIO_CODEC_PCMU = 4,

        ///
        /// <summary>
        /// 5: G722.
        /// </summary>
        ///
        AUDIO_CODEC_G722 = 5,

        ///
        /// <summary>
        /// 8: LC-AAC.
        /// </summary>
        ///
        AUDIO_CODEC_AACLC = 8,

        ///
        /// <summary>
        /// 9: HE-AAC.
        /// </summary>
        ///
        AUDIO_CODEC_HEAAC = 9,

        ///
        /// <summary>
        /// 10: JC1.
        /// </summary>
        ///
        AUDIO_CODEC_JC1 = 10,

        ///
        /// <summary>
        /// 11: HE-AAC v2.
        /// </summary>
        ///
        AUDIO_CODEC_HEAAC2 = 11,

        ///
        /// @ignore
        ///
        AUDIO_CODEC_LPCNET = 12,

        ///
        /// @ignore
        ///
        AUDIO_CODEC_OPUSMC = 13,

    }

    ///
    /// <summary>
    /// Audio encoding type.
    /// </summary>
    ///
    public enum AUDIO_ENCODING_TYPE
    {
        ///
        /// <summary>
        /// 0x010101: AAC encoding format, 16000 Hz sampling rate, low quality. A 10-minute audio file is approximately 1.2 MB after encoding.
        /// </summary>
        ///
        AUDIO_ENCODING_TYPE_AAC_16000_LOW = 0x010101,

        ///
        /// <summary>
        /// 0x010102: AAC encoding format, 16000 Hz sampling rate, medium quality. A 10-minute audio file is approximately 2 MB after encoding.
        /// </summary>
        ///
        AUDIO_ENCODING_TYPE_AAC_16000_MEDIUM = 0x010102,

        ///
        /// <summary>
        /// 0x010201: AAC encoding format, 32000 Hz sampling rate, low quality. A 10-minute audio file is approximately 1.2 MB after encoding.
        /// </summary>
        ///
        AUDIO_ENCODING_TYPE_AAC_32000_LOW = 0x010201,

        ///
        /// <summary>
        /// 0x010202: AAC encoding format, 32000 Hz sampling rate, medium quality. A 10-minute audio file is approximately 2 MB after encoding.
        /// </summary>
        ///
        AUDIO_ENCODING_TYPE_AAC_32000_MEDIUM = 0x010202,

        ///
        /// <summary>
        /// 0x010203: AAC encoding format, 32000 Hz sampling rate, high quality. A 10-minute audio file is approximately 3.5 MB after encoding.
        /// </summary>
        ///
        AUDIO_ENCODING_TYPE_AAC_32000_HIGH = 0x010203,

        ///
        /// <summary>
        /// 0x010302: AAC encoding format, 48000 Hz sampling rate, medium quality. A 10-minute audio file is approximately 2 MB after encoding.
        /// </summary>
        ///
        AUDIO_ENCODING_TYPE_AAC_48000_MEDIUM = 0x010302,

        ///
        /// <summary>
        /// 0x010303: AAC encoding format, 48000 Hz sampling rate, high quality. A 10-minute audio file is approximately 3.5 MB after encoding.
        /// </summary>
        ///
        AUDIO_ENCODING_TYPE_AAC_48000_HIGH = 0x010303,

        ///
        /// <summary>
        /// 0x020101: OPUS encoding format, 16000 Hz sampling rate, low quality. A 10-minute audio file is approximately 2 MB after encoding.
        /// </summary>
        ///
        AUDIO_ENCODING_TYPE_OPUS_16000_LOW = 0x020101,

        ///
        /// <summary>
        /// 0x020102: OPUS encoding format, 16000 Hz sampling rate, medium quality. A 10-minute audio file is approximately 2 MB after encoding.
        /// </summary>
        ///
        AUDIO_ENCODING_TYPE_OPUS_16000_MEDIUM = 0x020102,

        ///
        /// <summary>
        /// 0x020302: OPUS encoding format, 48000 Hz sampling rate, medium quality. A 10-minute audio file is approximately 2 MB after encoding.
        /// </summary>
        ///
        AUDIO_ENCODING_TYPE_OPUS_48000_MEDIUM = 0x020302,

        ///
        /// <summary>
        /// 0x020303: OPUS encoding format, 48000 Hz sampling rate, high quality. A 10-minute audio file is approximately 3.5 MB after encoding.
        /// </summary>
        ///
        AUDIO_ENCODING_TYPE_OPUS_48000_HIGH = 0x020303,

    }

    ///
    /// <summary>
    /// Watermark fit mode.
    /// </summary>
    ///
    public enum WATERMARK_FIT_MODE
    {
        ///
        /// <summary>
        /// 0: Uses the positionInLandscapeMode and positionInPortraitMode values set in WatermarkOptions. The settings in WatermarkRatio are ignored.
        /// </summary>
        ///
        FIT_MODE_COVER_POSITION = 0,

        ///
        /// <summary>
        /// 1: Uses the values set in WatermarkRatio. The positionInLandscapeMode and positionInPortraitMode settings in WatermarkOptions are ignored.
        /// </summary>
        ///
        FIT_MODE_USE_IMAGE_RATIO = 1,

    }

    ///
    /// @ignore
    ///
    public class EncodedAudioFrameAdvancedSettings
    {
        ///
        /// @ignore
        ///
        public bool speech;

        ///
        /// @ignore
        ///
        public bool sendEvenIfEmpty;

        public EncodedAudioFrameAdvancedSettings()
        {
            this.speech = true;
            this.sendEvenIfEmpty = true;
        }

        public EncodedAudioFrameAdvancedSettings(bool speech, bool sendEvenIfEmpty)
        {
            this.speech = speech;
            this.sendEvenIfEmpty = sendEvenIfEmpty;
        }
    }

    ///
    /// <summary>
    /// Information about encoded audio.
    /// </summary>
    ///
    public class EncodedAudioFrameInfo
    {
        ///
        /// <summary>
        /// Audio codec specification: AUDIO_CODEC_TYPE.
        /// </summary>
        ///
        public AUDIO_CODEC_TYPE codec;

        ///
        /// <summary>
        /// Audio sample rate (Hz).
        /// </summary>
        ///
        public int sampleRateHz;

        ///
        /// <summary>
        /// Number of audio samples per channel.
        /// </summary>
        ///
        public int samplesPerChannel;

        ///
        /// <summary>
        /// Number of channels.
        /// </summary>
        ///
        public int numberOfChannels;

        ///
        /// <summary>
        /// This feature is not supported yet.
        /// </summary>
        ///
        public EncodedAudioFrameAdvancedSettings advancedSettings;

        ///
        /// <summary>
        /// Unix timestamp (ms) when the external encoded video frame was captured.
        /// </summary>
        ///
        public long captureTimeMs;

        public EncodedAudioFrameInfo()
        {
            this.codec = AUDIO_CODEC_TYPE.AUDIO_CODEC_AACLC;
            this.sampleRateHz = 0;
            this.samplesPerChannel = 0;
            this.numberOfChannels = 0;
            this.captureTimeMs = 0;
        }

        public EncodedAudioFrameInfo(EncodedAudioFrameInfo rhs)
        {
            this.codec = rhs.codec;
            this.sampleRateHz = rhs.sampleRateHz;
            this.samplesPerChannel = rhs.samplesPerChannel;
            this.numberOfChannels = rhs.numberOfChannels;
            this.advancedSettings = rhs.advancedSettings;
            this.captureTimeMs = rhs.captureTimeMs;
        }

        public EncodedAudioFrameInfo(AUDIO_CODEC_TYPE codec, int sampleRateHz, int samplesPerChannel, int numberOfChannels, EncodedAudioFrameAdvancedSettings advancedSettings, long captureTimeMs)
        {
            this.codec = codec;
            this.sampleRateHz = sampleRateHz;
            this.samplesPerChannel = samplesPerChannel;
            this.numberOfChannels = numberOfChannels;
            this.advancedSettings = advancedSettings;
            this.captureTimeMs = captureTimeMs;
        }
    }

    ///
    /// @ignore
    ///
    public class AudioPcmDataInfo
    {
        ///
        /// @ignore
        ///
        public ulong samplesPerChannel;

        ///
        /// @ignore
        ///
        public short channelNum;

        ///
        /// @ignore
        ///
        public ulong samplesOut;

        ///
        /// @ignore
        ///
        public long elapsedTimeMs;

        ///
        /// @ignore
        ///
        public long ntpTimeMs;

        public AudioPcmDataInfo()
        {
            this.samplesPerChannel = 0;
            this.channelNum = 0;
            this.samplesOut = 0;
            this.elapsedTimeMs = 0;
            this.ntpTimeMs = 0;
        }

        public AudioPcmDataInfo(AudioPcmDataInfo rhs)
        {
            this.samplesPerChannel = rhs.samplesPerChannel;
            this.channelNum = rhs.channelNum;
            this.samplesOut = rhs.samplesOut;
            this.elapsedTimeMs = rhs.elapsedTimeMs;
            this.ntpTimeMs = rhs.ntpTimeMs;
        }

        public AudioPcmDataInfo(ulong samplesPerChannel, short channelNum, ulong samplesOut, long elapsedTimeMs, long ntpTimeMs)
        {
            this.samplesPerChannel = samplesPerChannel;
            this.channelNum = channelNum;
            this.samplesOut = samplesOut;
            this.elapsedTimeMs = elapsedTimeMs;
            this.ntpTimeMs = ntpTimeMs;
        }
    }

    ///
    /// @ignore
    ///
    public enum H264PacketizeMode
    {
        ///
        /// @ignore
        ///
        NonInterleaved = 0,

        ///
        /// @ignore
        ///
        SingleNalUnit,

    }

    ///
    /// <summary>
    /// Video stream types.
    /// </summary>
    ///
    public enum VIDEO_STREAM_TYPE
    {
        ///
        /// <summary>
        /// 0: High stream, i.e., high resolution and high bitrate video stream.
        /// </summary>
        ///
        VIDEO_STREAM_HIGH = 0,

        ///
        /// <summary>
        /// 1: Low stream, i.e., low resolution and low bitrate video stream.
        /// </summary>
        ///
        VIDEO_STREAM_LOW = 1,

        ///
        /// <summary>
        /// 4: Video quality layer 1. The resolution of this layer is only lower than VIDEO_STREAM_HIGH.
        /// </summary>
        ///
        VIDEO_STREAM_LAYER_1 = 4,

        ///
        /// <summary>
        /// 5: Video quality layer 2. The resolution of this layer is only lower than VIDEO_STREAM_LAYER_1.
        /// </summary>
        ///
        VIDEO_STREAM_LAYER_2 = 5,

        ///
        /// <summary>
        /// 6: Video quality layer 3. The resolution of this layer is only lower than VIDEO_STREAM_LAYER_2.
        /// </summary>
        ///
        VIDEO_STREAM_LAYER_3 = 6,

        ///
        /// <summary>
        /// 7: Video quality layer 4. The resolution of this layer is only lower than VIDEO_STREAM_LAYER_3.
        /// </summary>
        ///
        VIDEO_STREAM_LAYER_4 = 7,

        ///
        /// <summary>
        /// 8: Video quality layer 5. The resolution of this layer is only lower than VIDEO_STREAM_LAYER_4.
        /// </summary>
        ///
        VIDEO_STREAM_LAYER_5 = 8,

        ///
        /// <summary>
        /// 9: Video quality layer 6. The resolution of this layer is only lower than VIDEO_STREAM_LAYER_5.
        /// </summary>
        ///
        VIDEO_STREAM_LAYER_6 = 9,

    }

    ///
    /// <summary>
    /// Video subscription settings.
    /// </summary>
    ///
    public class VideoSubscriptionOptions : IOptionalJsonParse
    {
        ///
        /// <summary>
        /// Type of video stream to subscribe to. The default value is VIDEO_STREAM_HIGH, which subscribes to the high-quality video stream. See VIDEO_STREAM_TYPE.
        /// </summary>
        ///
        public Optional<VIDEO_STREAM_TYPE> type = new Optional<VIDEO_STREAM_TYPE>();

        ///
        /// <summary>
        /// Whether to subscribe only to the encoded video stream: true : Subscribe only to encoded video data (structured data). The SDK does not decode or render this video data. false : (Default) Subscribe to both raw and encoded video data.
        /// </summary>
        ///
        public Optional<bool> encodedFrameOnly = new Optional<bool>();

        public VideoSubscriptionOptions()
        {
        }

        public VideoSubscriptionOptions(Optional<VIDEO_STREAM_TYPE> type, Optional<bool> encodedFrameOnly)
        {
            this.type = type;
            this.encodedFrameOnly = encodedFrameOnly;
        }

        ///
        /// @ignore
        ///
        public virtual void ToJson(JsonWriter writer)
        {
            writer.WriteObjectStart();

            if (this.type.HasValue())
            {
                writer.WritePropertyName("type");
                AgoraJson.WriteEnum(writer, this.type.GetValue());
            }

            if (this.encodedFrameOnly.HasValue())
            {
                writer.WritePropertyName("encodedFrameOnly");
                writer.Write(this.encodedFrameOnly.GetValue());
            }

            writer.WriteObjectEnd();
        }
    }

    ///
    /// <summary>
    /// Maximum length of the user account.
    /// </summary>
    ///
    public enum MAX_USER_ACCOUNT_LENGTH_TYPE
    {
        ///
        /// <summary>
        /// The maximum length of the user account is 255 characters.
        /// </summary>
        ///
        MAX_USER_ACCOUNT_LENGTH = 256,

    }

    ///
    /// <summary>
    /// Information about external encoded video frames.
    /// </summary>
    ///
    public class EncodedVideoFrameInfo
    {
        ///
        /// <summary>
        /// Video codec type. See VIDEO_CODEC_TYPE. Default is VIDEO_CODEC_H264 (2).
        /// </summary>
        ///
        public VIDEO_CODEC_TYPE codecType;

        ///
        /// <summary>
        /// Width of the video frame (px).
        /// </summary>
        ///
        public int width;

        ///
        /// <summary>
        /// Height of the video frame (px).
        /// </summary>
        ///
        public int height;

        ///
        /// <summary>
        /// Frames per second of the video.
        /// When this parameter is not 0, you can use it to calculate the Unix timestamp of the external encoded video frame.
        /// </summary>
        ///
        public int framesPerSecond;

        ///
        /// <summary>
        /// Type of the video frame. See VIDEO_FRAME_TYPE.
        /// </summary>
        ///
        public VIDEO_FRAME_TYPE frameType;

        ///
        /// <summary>
        /// Rotation information of the video frame. See VIDEO_ORIENTATION.
        /// </summary>
        ///
        public VIDEO_ORIENTATION rotation;

        ///
        /// <summary>
        /// Reserved parameter.
        /// </summary>
        ///
        public int trackId;

        ///
        /// <summary>
        /// Unix timestamp (ms) when the external encoded video frame was captured.
        /// </summary>
        ///
        public long captureTimeMs;

        ///
        /// @ignore
        ///
        public long decodeTimeMs;

        ///
        /// <summary>
        /// Type of video stream. See VIDEO_STREAM_TYPE.
        /// </summary>
        ///
        public VIDEO_STREAM_TYPE streamType;

        ///
        /// @ignore
        ///
        public long presentationMs;

        public EncodedVideoFrameInfo()
        {
            this.codecType = VIDEO_CODEC_TYPE.VIDEO_CODEC_H264;
            this.width = 0;
            this.height = 0;
            this.framesPerSecond = 0;
            this.frameType = VIDEO_FRAME_TYPE.VIDEO_FRAME_TYPE_BLANK_FRAME;
            this.rotation = VIDEO_ORIENTATION.VIDEO_ORIENTATION_0;
            this.trackId = 0;
            this.captureTimeMs = 0;
            this.decodeTimeMs = 0;
            this.streamType = VIDEO_STREAM_TYPE.VIDEO_STREAM_HIGH;
            this.presentationMs = -1;
        }

        public EncodedVideoFrameInfo(EncodedVideoFrameInfo rhs)
        {
            this.codecType = rhs.codecType;
            this.width = rhs.width;
            this.height = rhs.height;
            this.framesPerSecond = rhs.framesPerSecond;
            this.frameType = rhs.frameType;
            this.rotation = rhs.rotation;
            this.trackId = rhs.trackId;
            this.captureTimeMs = rhs.captureTimeMs;
            this.decodeTimeMs = rhs.decodeTimeMs;
            this.streamType = rhs.streamType;
            this.presentationMs = rhs.presentationMs;
        }

        public EncodedVideoFrameInfo(VIDEO_CODEC_TYPE codecType, int width, int height, int framesPerSecond, VIDEO_FRAME_TYPE frameType, VIDEO_ORIENTATION rotation, int trackId, long captureTimeMs, long decodeTimeMs, VIDEO_STREAM_TYPE streamType, long presentationMs)
        {
            this.codecType = codecType;
            this.width = width;
            this.height = height;
            this.framesPerSecond = framesPerSecond;
            this.frameType = frameType;
            this.rotation = rotation;
            this.trackId = trackId;
            this.captureTimeMs = captureTimeMs;
            this.decodeTimeMs = decodeTimeMs;
            this.streamType = streamType;
            this.presentationMs = presentationMs;
        }
    }

    ///
    /// <summary>
    /// Compression preference type for video encoding.
    /// </summary>
    ///
    public enum COMPRESSION_PREFERENCE
    {
        ///
        /// <summary>
        /// -1: (Default) Auto mode. The SDK automatically selects PREFER_LOW_LATENCY or PREFER_QUALITY based on your video scenario settings to provide the best user experience.
        /// </summary>
        ///
        PREFER_COMPRESSION_AUTO = -1,

        ///
        /// <summary>
        /// 0: Low latency preference. The SDK compresses video frames to reduce latency. This is suitable for scenarios where smoothness is prioritized over quality.
        /// </summary>
        ///
        PREFER_LOW_LATENCY = 0,

        ///
        /// <summary>
        /// 1: High quality preference. The SDK compresses video frames while maintaining video quality. This is suitable for scenarios where video quality is prioritized.
        /// </summary>
        ///
        PREFER_QUALITY = 1,

    }

    ///
    /// <summary>
    /// Video encoder preference.
    /// </summary>
    ///
    public enum ENCODING_PREFERENCE
    {
        ///
        /// <summary>
        /// -1: Adaptive preference. The SDK automatically selects the optimal encoder based on platform, device type, and other factors.
        /// </summary>
        ///
        PREFER_AUTO = -1,

        ///
        /// <summary>
        /// 0: Software encoder preference. The SDK prioritizes using a software encoder for video encoding.
        /// </summary>
        ///
        PREFER_SOFTWARE = 0,

        ///
        /// <summary>
        /// 1: Hardware encoder preference. The SDK prioritizes using a hardware encoder for video encoding. If the device does not support hardware encoding, the SDK automatically falls back to software encoding and reports the encoder type used via the hwEncoderAccelerating field in the OnLocalVideoStats callback.
        /// </summary>
        ///
        PREFER_HARDWARE = 1,

    }

    ///
    /// <summary>
    /// Advanced options for video encoding.
    /// </summary>
    ///
    public class AdvanceOptions
    {
        ///
        /// <summary>
        /// Video encoder preference. See ENCODING_PREFERENCE.
        /// </summary>
        ///
        public ENCODING_PREFERENCE encodingPreference;

        ///
        /// <summary>
        /// Compression preference for video encoding. See COMPRESSION_PREFERENCE.
        /// </summary>
        ///
        public COMPRESSION_PREFERENCE compressionPreference;

        ///
        /// <summary>
        /// When the video frame contains Alpha channel data, sets whether to encode and send the Alpha data to the remote end: true : Encode and send Alpha data. false : (Default) Do not encode and send Alpha data.
        /// </summary>
        ///
        public bool encodeAlpha;

        public AdvanceOptions()
        {
            this.encodingPreference = ENCODING_PREFERENCE.PREFER_AUTO;
            this.compressionPreference = COMPRESSION_PREFERENCE.PREFER_COMPRESSION_AUTO;
            this.encodeAlpha = false;
        }

        public AdvanceOptions(ENCODING_PREFERENCE encoding_preference, COMPRESSION_PREFERENCE compression_preference, bool encode_alpha)
        {
            this.encodingPreference = encoding_preference;
            this.compressionPreference = compression_preference;
            this.encodeAlpha = encode_alpha;
        }

    }

    ///
    /// <summary>
    /// Mirror mode type.
    /// </summary>
    ///
    public enum VIDEO_MIRROR_MODE_TYPE
    {
        ///
        /// <summary>
        /// 0: Mirror mode is determined by the SDK.
        ///  Local view mirror mode: If you use the front camera, local view mirror mode is enabled by default; if you use the rear camera, it is disabled by default.
        ///  Remote user view mirror mode: Disabled by default.
        /// </summary>
        ///
        VIDEO_MIRROR_MODE_AUTO = 0,

        ///
        /// <summary>
        /// 1: Enable mirror mode.
        /// </summary>
        ///
        VIDEO_MIRROR_MODE_ENABLED = 1,

        ///
        /// <summary>
        /// 2: Disable mirror mode.
        /// </summary>
        ///
        VIDEO_MIRROR_MODE_DISABLED = 2,

    }

    ///
    /// @ignore
    ///
    public enum CAMERA_FORMAT_TYPE
    {
        ///
        /// @ignore
        ///
        CAMERA_FORMAT_NV12,

        ///
        /// @ignore
        ///
        CAMERA_FORMAT_BGRA,

    }

    ///
    /// @ignore
    ///
    public enum VIDEO_MODULE_TYPE
    {
        ///
        /// @ignore
        ///
        VIDEO_MODULE_CAPTURER = 0,

        ///
        /// @ignore
        ///
        VIDEO_MODULE_SOFTWARE_ENCODER = 1,

        ///
        /// @ignore
        ///
        VIDEO_MODULE_HARDWARE_ENCODER = 2,

        ///
        /// @ignore
        ///
        VIDEO_MODULE_SOFTWARE_DECODER = 3,

        ///
        /// @ignore
        ///
        VIDEO_MODULE_HARDWARE_DECODER = 4,

        ///
        /// @ignore
        ///
        VIDEO_MODULE_RENDERER = 5,

    }

    ///
    /// @ignore
    ///
    public enum HDR_CAPABILITY
    {
        ///
        /// @ignore
        ///
        HDR_CAPABILITY_UNKNOWN = -1,

        ///
        /// @ignore
        ///
        HDR_CAPABILITY_UNSUPPORTED = 0,

        ///
        /// @ignore
        ///
        HDR_CAPABILITY_SUPPORTED = 1,

    }

    ///
    /// <summary>
    /// Codec type bit mask.
    /// </summary>
    ///
    public enum CODEC_CAP_MASK
    {
        ///
        /// <summary>
        /// (0): Codec not supported.
        /// </summary>
        ///
        CODEC_CAP_MASK_NONE = 0,

        ///
        /// <summary>
        /// (1 << 0): Supports hardware decoding.
        /// </summary>
        ///
        CODEC_CAP_MASK_HW_DEC = 1 << 0,

        ///
        /// <summary>
        /// (1 << 1): Supports hardware encoding.
        /// </summary>
        ///
        CODEC_CAP_MASK_HW_ENC = 1 << 1,

        ///
        /// <summary>
        /// (1 << 2): Supports software decoding.
        /// </summary>
        ///
        CODEC_CAP_MASK_SW_DEC = 1 << 2,

        ///
        /// <summary>
        /// (1 << 3): Supports software encoding.
        /// </summary>
        ///
        CODEC_CAP_MASK_SW_ENC = 1 << 3,

    }

    ///
    /// <summary>
    /// Codec capability levels.
    /// </summary>
    ///
    public class CodecCapLevels
    {
        ///
        /// <summary>
        /// Hardware decoding capability level, indicating the device's ability to decode videos of different qualities using hardware. See VIDEO_CODEC_CAPABILITY_LEVEL.
        /// </summary>
        ///
        public VIDEO_CODEC_CAPABILITY_LEVEL hwDecodingLevel;

        ///
        /// <summary>
        /// Software decoding capability level, indicating the device's ability to decode videos of different qualities using software. See VIDEO_CODEC_CAPABILITY_LEVEL.
        /// </summary>
        ///
        public VIDEO_CODEC_CAPABILITY_LEVEL swDecodingLevel;

        public CodecCapLevels()
        {
            this.hwDecodingLevel = VIDEO_CODEC_CAPABILITY_LEVEL.CODEC_CAPABILITY_LEVEL_UNSPECIFIED;
            this.swDecodingLevel = VIDEO_CODEC_CAPABILITY_LEVEL.CODEC_CAPABILITY_LEVEL_UNSPECIFIED;
        }

        public CodecCapLevels(VIDEO_CODEC_CAPABILITY_LEVEL hwDecodingLevel, VIDEO_CODEC_CAPABILITY_LEVEL swDecodingLevel)
        {
            this.hwDecodingLevel = hwDecodingLevel;
            this.swDecodingLevel = swDecodingLevel;
        }
    }

    ///
    /// <summary>
    /// Information about codec capabilities supported by the SDK.
    /// </summary>
    ///
    public class CodecCapInfo
    {
        ///
        /// <summary>
        /// Video codec type. See VIDEO_CODEC_TYPE.
        /// </summary>
        ///
        public VIDEO_CODEC_TYPE codecType;

        ///
        /// <summary>
        /// Bit mask of codec types supported by the SDK. See CODEC_CAP_MASK.
        /// </summary>
        ///
        public int codecCapMask;

        ///
        /// <summary>
        /// Codec capability levels supported by the SDK. See CodecCapLevels.
        /// </summary>
        ///
        public CodecCapLevels codecLevels;

        public CodecCapInfo()
        {
            this.codecType = VIDEO_CODEC_TYPE.VIDEO_CODEC_NONE;
            this.codecCapMask = 0;
        }

        public CodecCapInfo(VIDEO_CODEC_TYPE codecType, int codecCapMask, CodecCapLevels codecLevels)
        {
            this.codecType = codecType;
            this.codecCapMask = codecCapMask;
            this.codecLevels = codecLevels;
        }
    }

    ///
    /// <summary>
    /// Focal length information supported by the camera, including camera direction and focal length type.
    /// 
    /// (Android and iOS only)
    /// </summary>
    ///
    public class FocalLengthInfo
    {
        ///
        /// <summary>
        /// Camera direction. See CAMERA_DIRECTION.
        /// </summary>
        ///
        public int cameraDirection;

        ///
        /// <summary>
        /// Focal length type. See CAMERA_FOCAL_LENGTH_TYPE.
        /// </summary>
        ///
        public CAMERA_FOCAL_LENGTH_TYPE focalLengthType;

        public FocalLengthInfo(int cameraDirection, CAMERA_FOCAL_LENGTH_TYPE focalLengthType)
        {
            this.cameraDirection = cameraDirection;
            this.focalLengthType = focalLengthType;
        }
        public FocalLengthInfo()
        {
        }

    }

    ///
    /// <summary>
    /// Configuration for the video encoder.
    /// </summary>
    ///
    public class VideoEncoderConfiguration
    {
        ///
        /// <summary>
        /// Video codec type. See VIDEO_CODEC_TYPE.
        /// </summary>
        ///
        public VIDEO_CODEC_TYPE codecType;

        ///
        /// <summary>
        /// Resolution (px) for video encoding. See VideoDimensions. This parameter is used to measure encoding quality, expressed as width  height. The default value is 960  540. You can set the resolution as needed.
        /// </summary>
        ///
        public VideoDimensions dimensions;

        ///
        /// <summary>
        /// Frame rate (fps) for video encoding. Default is 15. See FRAME_RATE.
        /// </summary>
        ///
        public int frameRate;

        ///
        /// <summary>
        /// Bitrate for video encoding in Kbps. See BITRATE. You don't need to set this parameter; keep the default value STANDARD_BITRATE. The SDK automatically selects the optimal bitrate based on your configured video resolution and frame rate. For details on the relationship between resolution and frame rate, see [Video Profile](https://doc.shengwang.cn/doc/rtc/unity/basic-features/video-profile#%E8%A7%86%E9%A2%91%E5%B1%9E%E6%80%A7%E5%8F%82%E8%80%83).
        /// </summary>
        ///
        public int bitrate;

        ///
        /// <summary>
        /// Minimum encoding bitrate in Kbps.
        /// The SDK automatically adjusts the video encoding bitrate based on network conditions. Setting this parameter higher than the default forces the encoder to output high-quality images, but may cause packet loss and video stuttering under poor network conditions. Therefore, unless you have specific quality requirements, Agora recommends not modifying this parameter. This parameter applies to live streaming only.
        /// </summary>
        ///
        public int minBitrate;

        ///
        /// <summary>
        /// Orientation mode for video encoding. See ORIENTATION_MODE.
        /// </summary>
        ///
        public ORIENTATION_MODE orientationMode;

        ///
        /// <summary>
        /// Video degradation preference when bandwidth is limited. See DEGRADATION_PREFERENCE. When this parameter is set to MAINTAIN_FRAMERATE (1) or MAINTAIN_BALANCED (2), you must also set orientationMode to ORIENTATION_MODE_ADAPTIVE (0), otherwise the setting will not take effect.
        /// </summary>
        ///
        public DEGRADATION_PREFERENCE degradationPreference;

        ///
        /// <summary>
        /// Whether to enable mirror mode when sending encoded video. This only affects what remote users see. See VIDEO_MIRROR_MODE_TYPE. Mirror mode is disabled by default.
        /// </summary>
        ///
        public VIDEO_MIRROR_MODE_TYPE mirrorMode;

        ///
        /// <summary>
        /// Advanced options for video encoding. See AdvanceOptions.
        /// </summary>
        ///
        public AdvanceOptions advanceOptions;

        public VideoEncoderConfiguration(VideoDimensions d, int f, int b, ORIENTATION_MODE m, VIDEO_MIRROR_MODE_TYPE mirror = VIDEO_MIRROR_MODE_TYPE.VIDEO_MIRROR_MODE_DISABLED)
        {
            this.codecType = VIDEO_CODEC_TYPE.VIDEO_CODEC_NONE;
            this.dimensions = d;
            this.frameRate = f;
            this.bitrate = b;
            this.minBitrate = (int)BITRATE.DEFAULT_MIN_BITRATE;
            this.orientationMode = m;
            this.degradationPreference = DEGRADATION_PREFERENCE.MAINTAIN_AUTO;
            this.mirrorMode = mirror;
            this.advanceOptions = new AdvanceOptions(ENCODING_PREFERENCE.PREFER_AUTO, COMPRESSION_PREFERENCE.PREFER_COMPRESSION_AUTO, false);
        }

        public VideoEncoderConfiguration(int width, int height, int f, int b, ORIENTATION_MODE m, VIDEO_MIRROR_MODE_TYPE mirror = VIDEO_MIRROR_MODE_TYPE.VIDEO_MIRROR_MODE_DISABLED)
        {
            this.codecType = VIDEO_CODEC_TYPE.VIDEO_CODEC_NONE;
            this.dimensions = new VideoDimensions(width, height);
            this.frameRate = f;
            this.bitrate = b;
            this.minBitrate = (int)BITRATE.DEFAULT_MIN_BITRATE;
            this.orientationMode = m;
            this.degradationPreference = DEGRADATION_PREFERENCE.MAINTAIN_AUTO;
            this.mirrorMode = mirror;
            this.advanceOptions = new AdvanceOptions(ENCODING_PREFERENCE.PREFER_AUTO, COMPRESSION_PREFERENCE.PREFER_COMPRESSION_AUTO, false);
        }

        public VideoEncoderConfiguration(VideoEncoderConfiguration config)
        {
            this.codecType = config.codecType;
            this.dimensions = config.dimensions;
            this.frameRate = config.frameRate;
            this.bitrate = config.bitrate;
            this.minBitrate = config.minBitrate;
            this.orientationMode = config.orientationMode;
            this.degradationPreference = config.degradationPreference;
            this.mirrorMode = config.mirrorMode;
            this.advanceOptions = config.advanceOptions;
        }

        public VideoEncoderConfiguration()
        {
            this.codecType = VIDEO_CODEC_TYPE.VIDEO_CODEC_NONE;
            this.dimensions = new VideoDimensions((int)FRAME_WIDTH.FRAME_WIDTH_960, (int)FRAME_HEIGHT.FRAME_HEIGHT_540);
            this.frameRate = (int)FRAME_RATE.FRAME_RATE_FPS_15;
            this.bitrate = (int)BITRATE.STANDARD_BITRATE;
            this.minBitrate = (int)BITRATE.DEFAULT_MIN_BITRATE;
            this.orientationMode = ORIENTATION_MODE.ORIENTATION_MODE_ADAPTIVE;
            this.degradationPreference = DEGRADATION_PREFERENCE.MAINTAIN_AUTO;
            this.mirrorMode = VIDEO_MIRROR_MODE_TYPE.VIDEO_MIRROR_MODE_DISABLED;
            this.advanceOptions = new AdvanceOptions(ENCODING_PREFERENCE.PREFER_AUTO, COMPRESSION_PREFERENCE.PREFER_COMPRESSION_AUTO, false);
        }

        public VideoEncoderConfiguration(VIDEO_CODEC_TYPE codecType, VideoDimensions dimensions, int frameRate, int bitrate, int minBitrate, ORIENTATION_MODE orientationMode, DEGRADATION_PREFERENCE degradationPreference, VIDEO_MIRROR_MODE_TYPE mirrorMode, AdvanceOptions advanceOptions)
        {
            this.codecType = codecType;
            this.dimensions = dimensions;
            this.frameRate = frameRate;
            this.bitrate = bitrate;
            this.minBitrate = minBitrate;
            this.orientationMode = orientationMode;
            this.degradationPreference = degradationPreference;
            this.mirrorMode = mirrorMode;
            this.advanceOptions = advanceOptions;
        }
    }

    ///
    /// <summary>
    /// Data stream settings.
    /// 
    /// The table below shows the SDK behavior for different parameter settings: syncWithAudio ordered
    /// SDK Behavior false false
    /// The SDK immediately triggers the OnStreamMessage callback upon receiving the data packet. true false
    /// If the data packet delay is within the audio delay range, the SDK triggers the OnStreamMessage callback synchronized with the audio packet during playback. If the delay exceeds the audio delay, the SDK triggers the callback immediately upon receiving the data packet; this may cause desynchronization between audio and data packets. false true
    /// If the data packet delay is within 5 seconds, the SDK corrects the out-of-order issue. If the delay exceeds 5 seconds, the SDK discards the data packet. true true
    /// If the data packet delay is within the audio delay range, the SDK corrects the out-of-order issue. If the delay exceeds the audio delay, the SDK discards the data packet.
    /// </summary>
    ///
    public class DataStreamConfig
    {
        ///
        /// <summary>
        /// Whether to synchronize with the locally sent audio stream. true : The data stream is synchronized with the audio stream. This setting is suitable for special scenarios such as lyrics synchronization. false : The data stream is not synchronized with the audio stream. This setting is suitable for scenarios where data packets need to reach the receiver immediately. When synchronization is enabled, if the data packet delay is within the audio delay range, the SDK triggers the OnStreamMessage callback synchronized with the audio packet during playback.
        /// </summary>
        ///
        public bool syncWithAudio;

        ///
        /// <summary>
        /// Whether to ensure that received data is in the same order as sent. true : The SDK outputs data packets in the same order as sent by the sender. false : The SDK does not guarantee the order of data packets. When data packets need to reach the receiver immediately, this parameter should not be set to true.
        /// </summary>
        ///
        public bool ordered;

        public DataStreamConfig(bool syncWithAudio, bool ordered)
        {
            this.syncWithAudio = syncWithAudio;
            this.ordered = ordered;
        }
        public DataStreamConfig()
        {
        }

    }

    ///
    /// <summary>
    /// Mode for sending video streams.
    /// </summary>
    ///
    public enum SIMULCAST_STREAM_MODE
    {
        ///
        /// <summary>
        /// -1: By default, small streams are not sent until a subscription request for a small stream is received from the receiver, at which point small streams are sent automatically.
        /// </summary>
        ///
        AUTO_SIMULCAST_STREAM = -1,

        ///
        /// <summary>
        /// 0: Never send small streams.
        /// </summary>
        ///
        DISABLE_SIMULCAST_STREAM = 0,

        ///
        /// <summary>
        /// 1: Always send small streams.
        /// </summary>
        ///
        ENABLE_SIMULCAST_STREAM = 1,

    }

    ///
    /// <summary>
    /// Configuration for video low stream.
    /// </summary>
    ///
    public class SimulcastStreamConfig
    {
        ///
        /// <summary>
        /// Video resolution. See VideoDimensions. Default is 50% of the high stream resolution.
        /// </summary>
        ///
        public VideoDimensions dimensions;

        ///
        /// <summary>
        /// Video bitrate (Kbps), default is -1. This parameter does not need to be set; the SDK automatically matches the optimal bitrate based on the configured resolution and frame rate.
        /// </summary>
        ///
        public int kBitrate;

        ///
        /// <summary>
        /// Video frame rate (fps). Default is 5.
        /// </summary>
        ///
        public int framerate;

        public SimulcastStreamConfig()
        {
            this.dimensions = new VideoDimensions(160, 120);
            this.kBitrate = 65;
            this.framerate = 5;
        }

        public SimulcastStreamConfig(SimulcastStreamConfig other)
        {
            this.dimensions = other.dimensions;
            this.kBitrate = other.kBitrate;
            this.framerate = other.framerate;
        }

        public SimulcastStreamConfig(VideoDimensions dimensions, int kBitrate, int framerate)
        {
            this.dimensions = dimensions;
            this.kBitrate = kBitrate;
            this.framerate = framerate;
        }
    }

    ///
    /// @ignore
    ///
    public class SimulcastConfig
    {
        ///
        /// @ignore
        ///
        public StreamLayerConfig[] configs;

        ///
        /// @ignore
        ///
        public bool publish_fallback_enable;

        public SimulcastConfig()
        {
            this.publish_fallback_enable = false;
        }

        public SimulcastConfig(StreamLayerConfig[] configs, bool publish_fallback_enable)
        {
            this.configs = configs;
            this.publish_fallback_enable = publish_fallback_enable;
        }
    }

    ///
    /// @ignore
    ///
    public enum StreamLayerIndex
    {
        ///
        /// @ignore
        ///
        STREAM_LAYER_1 = 0,

        ///
        /// @ignore
        ///
        STREAM_LAYER_2 = 1,

        ///
        /// @ignore
        ///
        STREAM_LAYER_3 = 2,

        ///
        /// @ignore
        ///
        STREAM_LAYER_4 = 3,

        ///
        /// @ignore
        ///
        STREAM_LAYER_5 = 4,

        ///
        /// @ignore
        ///
        STREAM_LAYER_6 = 5,

        ///
        /// @ignore
        ///
        STREAM_LOW = 6,

        ///
        /// @ignore
        ///
        STREAM_LAYER_COUNT_MAX = 7,

    }

    ///
    /// @ignore
    ///
    public class StreamLayerConfig
    {
        ///
        /// @ignore
        ///
        public VideoDimensions dimensions;

        ///
        /// @ignore
        ///
        public int framerate;

        ///
        /// @ignore
        ///
        public bool enable;

        public StreamLayerConfig()
        {
            this.dimensions = new VideoDimensions(0, 0);
            this.framerate = 0;
            this.enable = false;
        }

        public StreamLayerConfig(VideoDimensions dimensions, int framerate, bool enable)
        {
            this.dimensions = dimensions;
            this.framerate = framerate;
            this.enable = enable;
        }
    }

    ///
    /// <summary>
    /// Position of the target area relative to the entire screen or window. If not set, it refers to the entire screen or window.
    /// </summary>
    ///
    public class Rectangle
    {
        ///
        /// <summary>
        /// Horizontal offset of the top-left corner.
        /// </summary>
        ///
        public int x;

        ///
        /// <summary>
        /// Vertical offset of the top-left corner.
        /// </summary>
        ///
        public int y;

        ///
        /// <summary>
        /// Width of the target area.
        /// </summary>
        ///
        public int width;

        ///
        /// <summary>
        /// Height of the target area.
        /// </summary>
        ///
        public int height;

        public Rectangle()
        {
            this.x = 0;
            this.y = 0;
            this.width = 0;
            this.height = 0;
        }

        public Rectangle(int xx, int yy, int ww, int hh)
        {
            this.x = xx;
            this.y = yy;
            this.width = ww;
            this.height = hh;
        }

    }

    ///
    /// <summary>
    /// Watermark position and size on the screen.
    /// 
    /// The position and size of the watermark on the screen are determined by xRatio, yRatio, and widthRatio :
    ///  (xRatio, yRatio) specifies the coordinates of the top-left corner of the watermark, determining its distance from the top-left corner of the screen. widthRatio specifies the width of the watermark.
    /// </summary>
    ///
    public class WatermarkRatio
    {
        ///
        /// <summary>
        /// The x-coordinate of the top-left corner of the watermark. With the top-left corner of the screen as the origin, the x-coordinate is the horizontal offset of the watermark's top-left corner relative to the origin. Value range is [0.0,1.0], default is 0.
        /// </summary>
        ///
        public float xRatio;

        ///
        /// <summary>
        /// The y-coordinate of the top-left corner of the watermark. With the top-left corner of the screen as the origin, the y-coordinate is the vertical offset of the watermark's top-left corner relative to the origin. Value range is [0.0,1.0], default is 0.
        /// </summary>
        ///
        public float yRatio;

        ///
        /// <summary>
        /// The width of the watermark. The SDK calculates the proportional height based on this value to ensure the watermark image is not distorted when scaled. Value range is [0.0,1.0], default is 0, which means the watermark is not displayed.
        /// </summary>
        ///
        public float widthRatio;

        public WatermarkRatio()
        {
            this.xRatio = 0.0f;
            this.yRatio = 0.0f;
            this.widthRatio = 0.0f;
        }

        public WatermarkRatio(float x, float y, float width)
        {
            this.xRatio = x;
            this.yRatio = y;
            this.widthRatio = width;
        }

    }

    ///
    /// <summary>
    /// Configure watermark image.
    /// 
    /// Used to configure the watermark image to be added.
    /// </summary>
    ///
    public class WatermarkOptions
    {
        ///
        /// <summary>
        /// Whether the watermark is visible in the local preview view: true : (Default) The watermark is visible in the local preview view. false : The watermark is not visible in the local preview view.
        /// </summary>
        ///
        public bool visibleInPreview;

        ///
        /// <summary>
        /// When the watermark fit mode is FIT_MODE_COVER_POSITION, this sets the area of the watermark image in landscape mode. See Rectangle.
        /// </summary>
        ///
        public Rectangle positionInLandscapeMode;

        ///
        /// <summary>
        /// When the watermark fit mode is FIT_MODE_COVER_POSITION, this sets the area of the watermark image in portrait mode. See Rectangle.
        /// </summary>
        ///
        public Rectangle positionInPortraitMode;

        ///
        /// <summary>
        /// When the watermark fit mode is FIT_MODE_USE_IMAGE_RATIO, this parameter sets the coordinates of the watermark in scale mode. See WatermarkRatio.
        /// </summary>
        ///
        public WatermarkRatio watermarkRatio;

        ///
        /// <summary>
        /// Watermark fit mode. See WATERMARK_FIT_MODE.
        /// </summary>
        ///
        public WATERMARK_FIT_MODE mode;

        ///
        /// <summary>
        /// Layer order of the watermark image. Default value is 0.
        /// </summary>
        ///
        public int zOrder;

        public WatermarkOptions()
        {
            this.visibleInPreview = true;
            this.positionInLandscapeMode = new Rectangle(0, 0, 0, 0);
            this.positionInPortraitMode = new Rectangle(0, 0, 0, 0);
            this.mode = WATERMARK_FIT_MODE.FIT_MODE_COVER_POSITION;
            this.zOrder = 0;
        }

        public WatermarkOptions(bool visibleInPreview, Rectangle positionInLandscapeMode, Rectangle positionInPortraitMode, WatermarkRatio watermarkRatio, WATERMARK_FIT_MODE mode, int zOrder)
        {
            this.visibleInPreview = visibleInPreview;
            this.positionInLandscapeMode = positionInLandscapeMode;
            this.positionInPortraitMode = positionInPortraitMode;
            this.watermarkRatio = watermarkRatio;
            this.mode = mode;
            this.zOrder = zOrder;
        }
    }

    ///
    /// <summary>
    /// Watermark source type.
    /// 
    /// Since Available since v4.6.2.
    /// </summary>
    ///
    public enum WATERMARK_SOURCE_TYPE
    {
        ///
        /// <summary>
        /// (0): The watermark source is an image.
        /// </summary>
        ///
        IMAGE = 0,

        ///
        /// <summary>
        /// (1): The watermark source is a buffer.
        /// </summary>
        ///
        BUFFER = 1,

        ///
        /// <summary>
        /// (2): The watermark source is a text literal. Supported on Linux only.
        /// </summary>
        ///
        LITERAL = 2,

        ///
        /// <summary>
        /// (3): The watermark source is a timestamp. Supported on Linux only.
        /// </summary>
        ///
        TIMESTAMPS = 3,

    }

    ///
    /// <summary>
    /// Used to configure timestamp watermark.
    /// 
    /// Since Available since v4.6.2. (Linux only)
    /// </summary>
    ///
    public class WatermarkTimestamp
    {
        ///
        /// <summary>
        /// Font size of the timestamp. Default is 10.
        /// </summary>
        ///
        public int fontSize;

        ///
        /// <summary>
        /// Path to the font file for the timestamp. Default is NULL. The font file must be in .ttf format. If not set, the SDK uses the system default font (if available). If used asynchronously, copy the path to memory that will not be released.
        /// </summary>
        ///
        public string fontFilePath;

        ///
        /// <summary>
        /// Stroke width of the timestamp. Default is 1.
        /// </summary>
        ///
        public int strokeWidth;

        ///
        /// <summary>
        /// Format of the timestamp. Default is %F %X. The format follows the C standard library function strftime. See strftime. If used asynchronously, copy the format string to memory that will not be released.
        /// </summary>
        ///
        public string format;

        public WatermarkTimestamp()
        {
            this.fontSize = 10;
            this.fontFilePath = "";
            this.strokeWidth = 1;
            this.format = "";
        }

        public WatermarkTimestamp(int fontSize, string fontFilePath, int strokeWidth, string format)
        {
            this.fontSize = fontSize;
            this.fontFilePath = fontFilePath;
            this.strokeWidth = strokeWidth;
            this.format = format;
        }
    }

    ///
    /// <summary>
    /// Used to configure text watermark.
    /// 
    /// Since Available since v4.6.2. (Linux only)
    /// </summary>
    ///
    public class WatermarkLiteral
    {
        ///
        /// <summary>
        /// Font size of the text. Default is 10.
        /// </summary>
        ///
        public int fontSize;

        ///
        /// <summary>
        /// Stroke width of the text. Default is 1.
        /// </summary>
        ///
        public int strokeWidth;

        ///
        /// <summary>
        /// Text content of the watermark. Default is NULL. If used asynchronously, copy the string to memory that will not be released.
        /// </summary>
        ///
        public string wmLiteral;

        ///
        /// <summary>
        /// Path to the font file. Default is NULL. The font file should be in .ttf format. If not set, the SDK uses the system default font (if available). If used asynchronously, copy the string to memory that will not be released.
        /// </summary>
        ///
        public string fontFilePath;

        public WatermarkLiteral()
        {
            this.wmLiteral = "";
            this.fontFilePath = "";
            this.fontSize = 10;
            this.strokeWidth = 1;
        }

        public WatermarkLiteral(int fontSize, int strokeWidth, string wmLiteral, string fontFilePath)
        {
            this.fontSize = fontSize;
            this.strokeWidth = strokeWidth;
            this.wmLiteral = wmLiteral;
            this.fontFilePath = fontFilePath;
        }
    }

    ///
    /// <summary>
    /// Used to configure the format, dimensions, and pixel buffer of a watermark image.
    /// 
    /// Since Available since v4.6.2.
    /// </summary>
    ///
    public class WatermarkBuffer
    {
        ///
        /// <summary>
        /// Width of the watermark image in pixels.
        /// </summary>
        ///
        public int width;

        ///
        /// <summary>
        /// Height of the watermark image in pixels.
        /// </summary>
        ///
        public int height;

        ///
        /// <summary>
        /// Length of the watermark image buffer in bytes.
        /// </summary>
        ///
        public int length;

        ///
        /// <summary>
        /// Pixel format of the watermark image. See VIDEO_PIXEL_FORMAT. Default is VIDEO_PIXEL_I420. Currently supported formats include: VIDEO_PIXEL_I420, VIDEO_PIXEL_RGBA, VIDEO_PIXEL_BGRA, and VIDEO_PIXEL_NV21.
        /// </summary>
        ///
        public VIDEO_PIXEL_FORMAT format;

        ///
        /// <summary>
        /// Pixel buffer data of the watermark image.
        /// </summary>
        ///
        public IntPtr buffer;

        public WatermarkBuffer()
        {
            this.buffer = IntPtr.Zero;
            this.width = 0;
            this.height = 0;
            this.length = 0;
            this.format = VIDEO_PIXEL_FORMAT.VIDEO_PIXEL_I420;
        }

        public WatermarkBuffer(int width, int height, int length, VIDEO_PIXEL_FORMAT format, IntPtr buffer)
        {
            this.width = width;
            this.height = height;
            this.length = length;
            this.format = format;
            this.buffer = buffer;
        }
    }

    ///
    /// <summary>
    /// Used to configure watermark information.
    /// 
    /// Since Available since v4.6.2.
    /// </summary>
    ///
    public class WatermarkConfig
    {
        ///
        /// <summary>
        /// Unique identifier for the watermark. It is recommended to use a UUID.
        /// </summary>
        ///
        public string id;

        ///
        /// <summary>
        /// Type of the watermark. See WATERMARK_SOURCE_TYPE.
        /// </summary>
        ///
        public WATERMARK_SOURCE_TYPE type;

        ///
        /// <summary>
        /// Buffer of the watermark. See WatermarkBuffer.
        /// </summary>
        ///
        public WatermarkBuffer buffer;

        ///
        /// <summary>
        /// Timestamp of the watermark. (Linux only)
        /// </summary>
        ///
        public WatermarkTimestamp timestamp;

        ///
        /// <summary>
        /// Text content of the watermark. (Linux only)
        /// </summary>
        ///
        public WatermarkLiteral literal;

        ///
        /// <summary>
        /// URL of the watermark image file. Default is NULL.
        /// </summary>
        ///
        public string imageUrl;

        ///
        /// <summary>
        /// Configuration options for the watermark. See WatermarkOptions.
        /// </summary>
        ///
        public WatermarkOptions options;

        public WatermarkConfig()
        {
            this.id = "";
            this.type = WATERMARK_SOURCE_TYPE.IMAGE;
            this.imageUrl = "";
        }

        public WatermarkConfig(string id, WATERMARK_SOURCE_TYPE type, WatermarkBuffer buffer, WatermarkTimestamp timestamp, WatermarkLiteral literal, string imageUrl, WatermarkOptions options)
        {
            this.id = id;
            this.type = type;
            this.buffer = buffer;
            this.timestamp = timestamp;
            this.literal = literal;
            this.imageUrl = imageUrl;
            this.options = options;
        }
    }

    ///
    /// <summary>
    /// Modes for multipath data transmission.
    /// 
    /// Since Available since v4.6.2.
    /// </summary>
    ///
    public enum MultipathMode
    {
        ///
        /// <summary>
        /// (0): Redundant transmission mode. The same data is redundantly transmitted through all available paths.
        /// </summary>
        ///
        Duplicate = 0,

        ///
        /// <summary>
        /// (1): Dynamic transmission mode. The SDK dynamically selects the optimal path for data transmission based on the current network conditions to improve performance.
        /// </summary>
        ///
        Dynamic,

    }

    ///
    /// <summary>
    /// Network path types used for multipath transmission.
    /// 
    /// Since Available since v4.6.2.
    /// </summary>
    ///
    public enum MultipathType
    {
        ///
        /// <summary>
        /// (0): Local Area Network (LAN) path.
        /// </summary>
        ///
        LAN = 0,

        ///
        /// <summary>
        /// (1): Wi-Fi path.
        /// </summary>
        ///
        WIFI,

        ///
        /// <summary>
        /// (2): Mobile network path.
        /// </summary>
        ///
        Mobile,

        ///
        /// <summary>
        /// (99): Unknown or unspecified network path.
        /// </summary>
        ///
        Unknown = 99,

    }

    ///
    /// <summary>
    /// Used to obtain statistics of a specific network path.
    /// 
    /// Since Available since v4.6.2.
    /// </summary>
    ///
    public class PathStats
    {
        ///
        /// <summary>
        /// Type of network path. See MultipathType.
        /// </summary>
        ///
        public MultipathType type;

        ///
        /// <summary>
        /// Transmission bitrate of the path, in Kbps.
        /// </summary>
        ///
        public int txKBitRate;

        ///
        /// <summary>
        /// Reception bitrate of the path, in Kbps.
        /// </summary>
        ///
        public int rxKBitRate;

        public PathStats()
        {
            this.type = MultipathType.Unknown;
            this.txKBitRate = 0;
            this.rxKBitRate = 0;
        }

        public PathStats(MultipathType t, int tx, int rx)
        {
            this.type = t;
            this.txKBitRate = tx;
            this.rxKBitRate = rx;
        }

    }

    ///
    /// <summary>
    /// Used to aggregate statistics of each network path in multipath transmission.
    /// 
    /// Since Available since v4.6.2.
    /// </summary>
    ///
    public class MultipathStats
    {
        ///
        /// <summary>
        /// Total bytes sent via LAN path.
        /// </summary>
        ///
        public uint lanTxBytes;

        ///
        /// <summary>
        /// Total bytes received via LAN path.
        /// </summary>
        ///
        public uint lanRxBytes;

        ///
        /// <summary>
        /// Total bytes sent via Wi-Fi path.
        /// </summary>
        ///
        public uint wifiTxBytes;

        ///
        /// <summary>
        /// Total bytes received via Wi-Fi path.
        /// </summary>
        ///
        public uint wifiRxBytes;

        ///
        /// <summary>
        /// Total bytes sent via mobile network path.
        /// </summary>
        ///
        public uint mobileTxBytes;

        ///
        /// <summary>
        /// Total bytes received via mobile network path.
        /// </summary>
        ///
        public uint mobileRxBytes;

        ///
        /// <summary>
        /// Number of currently active transmission paths.
        /// </summary>
        ///
        public int activePathNum;

        ///
        /// <summary>
        /// Array of statistics for each active transmission path. See PathStats.
        /// </summary>
        ///
        public PathStats[] pathStats;

        public MultipathStats()
        {
            this.lanTxBytes = 0;
            this.lanRxBytes = 0;
            this.wifiTxBytes = 0;
            this.wifiRxBytes = 0;
            this.mobileTxBytes = 0;
            this.mobileRxBytes = 0;
            this.activePathNum = 0;
            this.pathStats = null;
        }

        public MultipathStats(uint lanTxBytes, uint lanRxBytes, uint wifiTxBytes, uint wifiRxBytes, uint mobileTxBytes, uint mobileRxBytes, int activePathNum, PathStats[] pathStats)
        {
            this.lanTxBytes = lanTxBytes;
            this.lanRxBytes = lanRxBytes;
            this.wifiTxBytes = wifiTxBytes;
            this.wifiRxBytes = wifiRxBytes;
            this.mobileTxBytes = mobileTxBytes;
            this.mobileRxBytes = mobileRxBytes;
            this.activePathNum = activePathNum;
            this.pathStats = pathStats;
        }
    }

    ///
    /// <summary>
    /// Call-related statistics.
    /// </summary>
    ///
    public class RtcStats
    {
        ///
        /// <summary>
        /// Call duration of the local user (seconds), cumulative value.
        /// </summary>
        ///
        public uint duration;

        ///
        /// <summary>
        /// Bytes sent.
        /// </summary>
        ///
        public uint txBytes;

        ///
        /// <summary>
        /// Bytes received.
        /// </summary>
        ///
        public uint rxBytes;

        ///
        /// <summary>
        /// Audio bytes sent, cumulative value.
        /// </summary>
        ///
        public uint txAudioBytes;

        ///
        /// <summary>
        /// Video bytes sent, cumulative value.
        /// </summary>
        ///
        public uint txVideoBytes;

        ///
        /// <summary>
        /// Audio bytes received, cumulative value.
        /// </summary>
        ///
        public uint rxAudioBytes;

        ///
        /// <summary>
        /// Video bytes received, cumulative value.
        /// </summary>
        ///
        public uint rxVideoBytes;

        ///
        /// <summary>
        /// Sending bitrate (Kbps).
        /// </summary>
        ///
        public ushort txKBitRate;

        ///
        /// <summary>
        /// Receiving bitrate (Kbps).
        /// </summary>
        ///
        public ushort rxKBitRate;

        ///
        /// <summary>
        /// Audio receiving bitrate (Kbps).
        /// </summary>
        ///
        public ushort rxAudioKBitRate;

        ///
        /// <summary>
        /// Audio packet sending bitrate (Kbps).
        /// </summary>
        ///
        public ushort txAudioKBitRate;

        ///
        /// <summary>
        /// Video receiving bitrate (Kbps).
        /// </summary>
        ///
        public ushort rxVideoKBitRate;

        ///
        /// <summary>
        /// Video sending bitrate (Kbps).
        /// </summary>
        ///
        public ushort txVideoKBitRate;

        ///
        /// <summary>
        /// Client-to-access-server latency (ms).
        /// </summary>
        ///
        public ushort lastmileDelay;

        ///
        /// <summary>
        /// Number of users in the current channel.
        /// </summary>
        ///
        public uint userCount;

        ///
        /// <summary>
        /// CPU usage of the current app (%).
        ///  The cpuAppUsage reported in the OnLeaveChannel callback is always 0.
        ///  Starting from Android 8.1, due to system restrictions, you may not be able to obtain CPU usage through this property.
        /// </summary>
        ///
        public double cpuAppUsage;

        ///
        /// <summary>
        /// CPU usage of the current system (%).
        /// On Windows, in a multi-core environment, this member represents the average usage of multi-core CPUs. It is calculated as (100 - CPU usage of system idle process shown in Task Manager)/100.
        ///  The cpuTotalUsage reported in the OnLeaveChannel callback is always 0.
        ///  Starting from Android 8.1, due to system restrictions, you cannot obtain CPU usage through this property.
        /// </summary>
        ///
        public double cpuTotalUsage;

        ///
        /// <summary>
        /// Round-trip latency from client to local router (ms). This property is enabled by default on devices running iOS versions earlier than 14 and disabled on iOS 14 and later.
        /// 
        ///  To enable this property on devices running iOS 14 and later, please [contact technical support](https://ticket.shengwang.cn/).
        /// On Android, to retrieve gatewayRtt, make sure to add the android.permission.ACCESS_WIFI_STATE permission after the </application> tag in your project's AndroidManifest.xml file.
        /// </summary>
        ///
        public int gatewayRtt;

        ///
        /// <summary>
        /// Memory usage ratio of the current app (%). This value is for reference only. It may not be available due to system limitations.
        /// </summary>
        ///
        public double memoryAppUsageRatio;

        ///
        /// <summary>
        /// Memory usage ratio of the current system (%). This value is for reference only. It may not be available due to system limitations.
        /// </summary>
        ///
        public double memoryTotalUsageRatio;

        ///
        /// <summary>
        /// Memory size of the current app (KB). This value is for reference only. It may not be available due to system limitations.
        /// </summary>
        ///
        public int memoryAppUsageInKbytes;

        ///
        /// <summary>
        /// Time from connection initiation to successful connection (ms). A value of 0 indicates invalid.
        /// </summary>
        ///
        public int connectTimeMs;

        ///
        /// @ignore
        ///
        public int firstAudioPacketDuration;

        ///
        /// @ignore
        ///
        public int firstVideoPacketDuration;

        ///
        /// @ignore
        ///
        public int firstVideoKeyFramePacketDuration;

        ///
        /// @ignore
        ///
        public int packetsBeforeFirstKeyFramePacket;

        ///
        /// @ignore
        ///
        public int firstAudioPacketDurationAfterUnmute;

        ///
        /// @ignore
        ///
        public int firstVideoPacketDurationAfterUnmute;

        ///
        /// @ignore
        ///
        public int firstVideoKeyFramePacketDurationAfterUnmute;

        ///
        /// @ignore
        ///
        public int firstVideoKeyFrameDecodedDurationAfterUnmute;

        ///
        /// @ignore
        ///
        public int firstVideoKeyFrameRenderedDurationAfterUnmute;

        ///
        /// <summary>
        /// Uplink packet loss rate (%) from client to server before anti-packet-loss technology is applied.
        /// </summary>
        ///
        public int txPacketLossRate;

        ///
        /// <summary>
        /// Downlink packet loss rate (%) from server to client before anti-packet-loss technology is applied.
        /// </summary>
        ///
        public int rxPacketLossRate;

        ///
        /// @ignore
        ///
        public int lanAccelerateState;

        public RtcStats()
        {
            this.duration = 0;
            this.txBytes = 0;
            this.rxBytes = 0;
            this.txAudioBytes = 0;
            this.txVideoBytes = 0;
            this.rxAudioBytes = 0;
            this.rxVideoBytes = 0;
            this.txKBitRate = 0;
            this.rxKBitRate = 0;
            this.rxAudioKBitRate = 0;
            this.txAudioKBitRate = 0;
            this.rxVideoKBitRate = 0;
            this.txVideoKBitRate = 0;
            this.lastmileDelay = 0;
            this.userCount = 0;
            this.cpuAppUsage = 0.0f;
            this.cpuTotalUsage = 0.0f;
            this.gatewayRtt = 0;
            this.memoryAppUsageRatio = 0.0f;
            this.memoryTotalUsageRatio = 0.0f;
            this.memoryAppUsageInKbytes = 0;
            this.connectTimeMs = 0;
            this.firstAudioPacketDuration = 0;
            this.firstVideoPacketDuration = 0;
            this.firstVideoKeyFramePacketDuration = 0;
            this.packetsBeforeFirstKeyFramePacket = 0;
            this.firstAudioPacketDurationAfterUnmute = 0;
            this.firstVideoPacketDurationAfterUnmute = 0;
            this.firstVideoKeyFramePacketDurationAfterUnmute = 0;
            this.firstVideoKeyFrameDecodedDurationAfterUnmute = 0;
            this.firstVideoKeyFrameRenderedDurationAfterUnmute = 0;
            this.txPacketLossRate = 0;
            this.rxPacketLossRate = 0;
            this.lanAccelerateState = 0;
        }

        public RtcStats(uint duration, uint txBytes, uint rxBytes, uint txAudioBytes, uint txVideoBytes, uint rxAudioBytes, uint rxVideoBytes, ushort txKBitRate, ushort rxKBitRate, ushort rxAudioKBitRate, ushort txAudioKBitRate, ushort rxVideoKBitRate, ushort txVideoKBitRate, ushort lastmileDelay, uint userCount, double cpuAppUsage, double cpuTotalUsage, int gatewayRtt, double memoryAppUsageRatio, double memoryTotalUsageRatio, int memoryAppUsageInKbytes, int connectTimeMs, int firstAudioPacketDuration, int firstVideoPacketDuration, int firstVideoKeyFramePacketDuration, int packetsBeforeFirstKeyFramePacket, int firstAudioPacketDurationAfterUnmute, int firstVideoPacketDurationAfterUnmute, int firstVideoKeyFramePacketDurationAfterUnmute, int firstVideoKeyFrameDecodedDurationAfterUnmute, int firstVideoKeyFrameRenderedDurationAfterUnmute, int txPacketLossRate, int rxPacketLossRate, int lanAccelerateState)
        {
            this.duration = duration;
            this.txBytes = txBytes;
            this.rxBytes = rxBytes;
            this.txAudioBytes = txAudioBytes;
            this.txVideoBytes = txVideoBytes;
            this.rxAudioBytes = rxAudioBytes;
            this.rxVideoBytes = rxVideoBytes;
            this.txKBitRate = txKBitRate;
            this.rxKBitRate = rxKBitRate;
            this.rxAudioKBitRate = rxAudioKBitRate;
            this.txAudioKBitRate = txAudioKBitRate;
            this.rxVideoKBitRate = rxVideoKBitRate;
            this.txVideoKBitRate = txVideoKBitRate;
            this.lastmileDelay = lastmileDelay;
            this.userCount = userCount;
            this.cpuAppUsage = cpuAppUsage;
            this.cpuTotalUsage = cpuTotalUsage;
            this.gatewayRtt = gatewayRtt;
            this.memoryAppUsageRatio = memoryAppUsageRatio;
            this.memoryTotalUsageRatio = memoryTotalUsageRatio;
            this.memoryAppUsageInKbytes = memoryAppUsageInKbytes;
            this.connectTimeMs = connectTimeMs;
            this.firstAudioPacketDuration = firstAudioPacketDuration;
            this.firstVideoPacketDuration = firstVideoPacketDuration;
            this.firstVideoKeyFramePacketDuration = firstVideoKeyFramePacketDuration;
            this.packetsBeforeFirstKeyFramePacket = packetsBeforeFirstKeyFramePacket;
            this.firstAudioPacketDurationAfterUnmute = firstAudioPacketDurationAfterUnmute;
            this.firstVideoPacketDurationAfterUnmute = firstVideoPacketDurationAfterUnmute;
            this.firstVideoKeyFramePacketDurationAfterUnmute = firstVideoKeyFramePacketDurationAfterUnmute;
            this.firstVideoKeyFrameDecodedDurationAfterUnmute = firstVideoKeyFrameDecodedDurationAfterUnmute;
            this.firstVideoKeyFrameRenderedDurationAfterUnmute = firstVideoKeyFrameRenderedDurationAfterUnmute;
            this.txPacketLossRate = txPacketLossRate;
            this.rxPacketLossRate = rxPacketLossRate;
            this.lanAccelerateState = lanAccelerateState;
        }
    }

    ///
    /// <summary>
    /// User roles in live broadcasting profile.
    /// </summary>
    ///
    public enum CLIENT_ROLE_TYPE
    {
        ///
        /// <summary>
        /// 1: Broadcaster. A broadcaster can both send and receive streams.
        /// </summary>
        ///
        CLIENT_ROLE_BROADCASTER = 1,

        ///
        /// <summary>
        /// 2: (Default) Audience. An audience can only receive streams but not send.
        /// </summary>
        ///
        CLIENT_ROLE_AUDIENCE = 2,

    }

    ///
    /// <summary>
    /// Local video quality adaptation since last statistics (based on target frame rate and bitrate).
    /// </summary>
    ///
    public enum QUALITY_ADAPT_INDICATION
    {
        ///
        /// <summary>
        /// 0: Local video quality remains unchanged.
        /// </summary>
        ///
        ADAPT_NONE = 0,

        ///
        /// <summary>
        /// 1: Local video quality improves due to increased network bandwidth.
        /// </summary>
        ///
        ADAPT_UP_BANDWIDTH = 1,

        ///
        /// <summary>
        /// 2: Local video quality degrades due to decreased network bandwidth.
        /// </summary>
        ///
        ADAPT_DOWN_BANDWIDTH = 2,

    }

    ///
    /// <summary>
    /// Latency level of audience in a live streaming channel. This enum is effective only when the user role is set to CLIENT_ROLE_AUDIENCE.
    /// </summary>
    ///
    public enum AUDIENCE_LATENCY_LEVEL_TYPE
    {
        ///
        /// <summary>
        /// 1: Low latency.
        /// </summary>
        ///
        AUDIENCE_LATENCY_LEVEL_LOW_LATENCY = 1,

        ///
        /// <summary>
        /// 2: (Default) Ultra-low latency.
        /// </summary>
        ///
        AUDIENCE_LATENCY_LEVEL_ULTRA_LOW_LATENCY = 2,

    }

    ///
    /// <summary>
    /// User role property settings.
    /// </summary>
    ///
    public class ClientRoleOptions
    {
        ///
        /// <summary>
        /// Latency level for audience. See AUDIENCE_LATENCY_LEVEL_TYPE.
        /// </summary>
        ///
        public AUDIENCE_LATENCY_LEVEL_TYPE audienceLatencyLevel;

        public ClientRoleOptions()
        {
            this.audienceLatencyLevel = AUDIENCE_LATENCY_LEVEL_TYPE.AUDIENCE_LATENCY_LEVEL_ULTRA_LOW_LATENCY;
        }

        public ClientRoleOptions(AUDIENCE_LATENCY_LEVEL_TYPE audienceLatencyLevel)
        {
            this.audienceLatencyLevel = audienceLatencyLevel;
        }
    }

    ///
    /// <summary>
    /// The subjective experience quality of the local user when receiving remote audio.
    /// </summary>
    ///
    public enum EXPERIENCE_QUALITY_TYPE
    {
        ///
        /// <summary>
        /// 0: Good subjective experience quality.
        /// </summary>
        ///
        EXPERIENCE_QUALITY_GOOD = 0,

        ///
        /// <summary>
        /// 1: Poor subjective experience quality.
        /// </summary>
        ///
        EXPERIENCE_QUALITY_BAD = 1,

    }

    ///
    /// <summary>
    /// The reason for poor subjective experience quality of the local user when receiving remote audio.
    /// </summary>
    ///
    public enum EXPERIENCE_POOR_REASON
    {
        ///
        /// <summary>
        /// 0: No reason, indicating good subjective experience quality.
        /// </summary>
        ///
        EXPERIENCE_REASON_NONE = 0,

        ///
        /// <summary>
        /// 1: Poor network quality of the remote user.
        /// </summary>
        ///
        REMOTE_NETWORK_QUALITY_POOR = 1,

        ///
        /// <summary>
        /// 2: Poor network quality of the local user.
        /// </summary>
        ///
        LOCAL_NETWORK_QUALITY_POOR = 2,

        ///
        /// <summary>
        /// 4: Weak Wi-Fi or mobile data signal of the local user.
        /// </summary>
        ///
        WIRELESS_SIGNAL_POOR = 4,

        ///
        /// <summary>
        /// 8: Wi-Fi and Bluetooth are enabled simultaneously on the local device, causing signal interference and reduced audio transmission quality.
        /// </summary>
        ///
        WIFI_BLUETOOTH_COEXIST = 8,

    }

    ///
    /// <summary>
    /// Mode of AI noise reduction.
    /// </summary>
    ///
    public enum AUDIO_AINS_MODE
    {
        ///
        /// <summary>
        /// 0: (Default) Balanced noise reduction mode; select this mode if you want a balanced effect between noise suppression and latency.
        /// </summary>
        ///
        AINS_MODE_BALANCED = 0,

        ///
        /// <summary>
        /// 1: Aggressive noise reduction mode; suitable for scenarios with high noise suppression requirements, such as outdoor live streaming. This mode can significantly reduce noise but may also slightly damage the voice.
        /// </summary>
        ///
        AINS_MODE_AGGRESSIVE = 1,

        ///
        /// <summary>
        /// 2: Low-latency aggressive noise reduction mode. The noise reduction latency of this mode is about half that of weak and aggressive modes, suitable for scenarios requiring both noise reduction and low latency, such as real-time chorus.
        /// </summary>
        ///
        AINS_MODE_ULTRALOWLATENCY = 2,

    }

    ///
    /// <summary>
    /// Audio encoding profile.
    /// </summary>
    ///
    public enum AUDIO_PROFILE_TYPE
    {
        ///
        /// <summary>
        /// 0: Default value.
        ///  In live broadcast scenarios: 48 kHz sampling rate, music encoding, mono, max bitrate 64 Kbps.
        ///  In communication scenarios:
        ///  Windows: 16 kHz sampling rate, voice encoding, mono, max bitrate 16 Kbps.
        ///  Android, macOS, iOS: 32 kHz sampling rate, voice encoding, mono, max bitrate 18 Kbps.
        /// </summary>
        ///
        AUDIO_PROFILE_DEFAULT = 0,

        ///
        /// <summary>
        /// 1: Specifies 32 kHz sampling rate, voice encoding, mono, max bitrate 18 Kbps.
        /// </summary>
        ///
        AUDIO_PROFILE_SPEECH_STANDARD = 1,

        ///
        /// <summary>
        /// 2: Specifies 48 kHz sampling rate, music encoding, mono, max bitrate 64 Kbps.
        /// </summary>
        ///
        AUDIO_PROFILE_MUSIC_STANDARD = 2,

        ///
        /// <summary>
        /// 3: Specifies 48 kHz sampling rate, music encoding, stereo, max bitrate 80 Kbps.
        /// To enable stereo, you also need to call SetAdvancedAudioOptions and set audioProcessingChannels in AdvancedAudioOptions to AUDIO_PROCESSING_STEREO.
        /// </summary>
        ///
        AUDIO_PROFILE_MUSIC_STANDARD_STEREO = 3,

        ///
        /// <summary>
        /// 4: Specifies 48 kHz sampling rate, music encoding, mono, max bitrate 96 Kbps.
        /// </summary>
        ///
        AUDIO_PROFILE_MUSIC_HIGH_QUALITY = 4,

        ///
        /// <summary>
        /// 5: Specifies 48 kHz sampling rate, music encoding, stereo, max bitrate 128 Kbps.
        /// To enable stereo, you also need to call SetAdvancedAudioOptions and set audioProcessingChannels in AdvancedAudioOptions to AUDIO_PROCESSING_STEREO.
        /// </summary>
        ///
        AUDIO_PROFILE_MUSIC_HIGH_QUALITY_STEREO = 5,

        ///
        /// <summary>
        /// 6: Specifies 16 kHz sampling rate, voice encoding, mono, applies echo cancellation algorithm AEC.
        /// </summary>
        ///
        AUDIO_PROFILE_IOT = 6,

        ///
        /// <summary>
        /// Enumeration boundary value.
        /// </summary>
        ///
        AUDIO_PROFILE_NUM = 7,

    }

    ///
    /// <summary>
    /// Audio scenario.
    /// </summary>
    ///
    public enum AUDIO_SCENARIO_TYPE
    {
        ///
        /// <summary>
        /// 0: (Default) Auto scenario. Automatically matches appropriate audio quality based on user role and routing.
        /// </summary>
        ///
        AUDIO_SCENARIO_DEFAULT = 0,

        ///
        /// <summary>
        /// 3: High-quality scenario suitable for music-focused use cases. Example: Instrument practice.
        /// </summary>
        ///
        AUDIO_SCENARIO_GAME_STREAMING = 3,

        ///
        /// <summary>
        /// 5: Chatroom scenario, suitable for frequent mic on/off situations. Example: Education.
        /// </summary>
        ///
        AUDIO_SCENARIO_CHATROOM = 5,

        ///
        /// <summary>
        /// 7: Chorus scenario. Suitable for real-time chorus with low latency and good network conditions.
        /// </summary>
        ///
        AUDIO_SCENARIO_CHORUS = 7,

        ///
        /// <summary>
        /// 8: Meeting scenario, suitable for multi-person voice-focused meetings.
        /// </summary>
        ///
        AUDIO_SCENARIO_MEETING = 8,

        ///
        /// @ignore
        ///
        AUDIO_SCENARIO_AI_SERVER = 9,

        ///
        /// <summary>
        /// 10: AI conversation scenario. Only applicable for interaction with [Agora Conversational AI Engine](https://doc.shengwang.cn/doc/convoai/restful/landing-page).
        /// </summary>
        ///
        AUDIO_SCENARIO_AI_CLIENT = 10,

        ///
        /// <summary>
        /// Number of enumerations.
        /// </summary>
        ///
        AUDIO_SCENARIO_NUM = 11,

    }

    ///
    /// <summary>
    /// Video frame format.
    /// </summary>
    ///
    public class VideoFormat
    {
        ///
        /// <summary>
        /// Width of the video frame (px). Default is 960.
        /// </summary>
        ///
        public int width;

        ///
        /// <summary>
        /// Height of the video frame (px). Default is 540.
        /// </summary>
        ///
        public int height;

        ///
        /// <summary>
        /// Frame rate of the video frame. Default is 15.
        /// </summary>
        ///
        public int fps;

        public VideoFormat()
        {
            this.width = (int)FRAME_WIDTH.FRAME_WIDTH_960;
            this.height = (int)FRAME_HEIGHT.FRAME_HEIGHT_540;
            this.fps = (int)FRAME_RATE.FRAME_RATE_FPS_15;
        }

        public VideoFormat(int w, int h, int f)
        {
            this.width = w;
            this.height = h;
            this.fps = f;
        }

    }

    ///
    /// <summary>
    /// Content type for screen sharing.
    /// </summary>
    ///
    public enum VIDEO_CONTENT_HINT
    {
        ///
        /// <summary>
        /// (Default) No specified content type.
        /// </summary>
        ///
        CONTENT_HINT_NONE,

        ///
        /// <summary>
        /// Content type is motion. Recommended when sharing videos, movies, or video games.
        /// </summary>
        ///
        CONTENT_HINT_MOTION,

        ///
        /// <summary>
        /// Content type is details. Recommended when sharing images or text.
        /// </summary>
        ///
        CONTENT_HINT_DETAILS,

    }

    ///
    /// <summary>
    /// Screen sharing scenarios.
    /// </summary>
    ///
    public enum SCREEN_SCENARIO_TYPE
    {
        ///
        /// <summary>
        /// 1: (Default) Document. In this scenario, the shared image quality is prioritized, and the latency for the receiving end to see the shared video is reduced. If you are sharing documents, slides, or spreadsheets, you can set this scenario.
        /// </summary>
        ///
        SCREEN_SCENARIO_DOCUMENT = 1,

        ///
        /// <summary>
        /// 2: Gaming. In this scenario, the smoothness of the shared content is prioritized. If you are sharing games, you can set this scenario.
        /// </summary>
        ///
        SCREEN_SCENARIO_GAMING = 2,

        ///
        /// <summary>
        /// 3: Video. In this scenario, the smoothness of the shared content is prioritized. If you are sharing movies or live videos, you can set this scenario.
        /// </summary>
        ///
        SCREEN_SCENARIO_VIDEO = 3,

        ///
        /// <summary>
        /// 4: Remote control. In this scenario, the shared image quality is prioritized, and the latency for the receiving end to see the shared video is reduced. If you are sharing the desktop of a remotely controlled device, you can set this scenario.
        /// </summary>
        ///
        SCREEN_SCENARIO_RDC = 4,

    }

    ///
    /// <summary>
    /// Video application scenario types.
    /// </summary>
    ///
    public enum VIDEO_APPLICATION_SCENARIO_TYPE
    {
        ///
        /// <summary>
        /// 0: (Default) General scenario.
        /// </summary>
        ///
        APPLICATION_SCENARIO_GENERAL = 0,

        ///
        /// <summary>
        /// 1: Meeting scenario.
        /// </summary>
        ///
        APPLICATION_SCENARIO_MEETING = 1,

        ///
        /// <summary>
        /// 2: 1v1 video call
        /// </summary>
        ///
        APPLICATION_SCENARIO_1V1 = 2,

        ///
        /// <summary>
        /// 3: Live show
        /// </summary>
        ///
        APPLICATION_SCENARIO_LIVESHOW = 3,

    }

    ///
    /// @ignore
    ///
    public enum VIDEO_QOE_PREFERENCE_TYPE
    {
        ///
        /// @ignore
        ///
        VIDEO_QOE_PREFERENCE_BALANCE = 1,

        ///
        /// @ignore
        ///
        VIDEO_QOE_PREFERENCE_DELAY_FIRST = 2,

        ///
        /// @ignore
        ///
        VIDEO_QOE_PREFERENCE_PICTURE_QUALITY_FIRST = 3,

        ///
        /// @ignore
        ///
        VIDEO_QOE_PREFERENCE_FLUENCY_FIRST = 4,

    }

    ///
    /// <summary>
    /// Brightness level of locally captured video quality.
    /// </summary>
    ///
    public enum CAPTURE_BRIGHTNESS_LEVEL_TYPE
    {
        ///
        /// <summary>
        /// -1: SDK has not detected the brightness level of the locally captured video. Please wait a few seconds and get the brightness level from the next captureBrightnessLevel callback.
        /// </summary>
        ///
        CAPTURE_BRIGHTNESS_LEVEL_INVALID = -1,

        ///
        /// <summary>
        /// 0: Normal brightness level of locally captured video.
        /// </summary>
        ///
        CAPTURE_BRIGHTNESS_LEVEL_NORMAL = 0,

        ///
        /// <summary>
        /// 1: Locally captured video is too bright.
        /// </summary>
        ///
        CAPTURE_BRIGHTNESS_LEVEL_BRIGHT = 1,

        ///
        /// <summary>
        /// 2: Locally captured video is too dark.
        /// </summary>
        ///
        CAPTURE_BRIGHTNESS_LEVEL_DARK = 2,

    }

    ///
    /// <summary>
    /// Camera stabilization mode.
    /// 
    /// Camera stabilization effect increases in the order of 1 < 2 < 3, with corresponding increase in latency.
    /// </summary>
    ///
    public enum CAMERA_STABILIZATION_MODE
    {
        ///
        /// <summary>
        /// -1: (Default) Camera stabilization is off.
        /// </summary>
        ///
        CAMERA_STABILIZATION_MODE_OFF = -1,

        ///
        /// <summary>
        /// 0: Camera auto stabilization. The system automatically selects a stabilization mode based on the camera status. However, this mode has higher latency and is not recommended.
        /// </summary>
        ///
        CAMERA_STABILIZATION_MODE_AUTO = 0,

        ///
        /// <summary>
        /// 1: (Recommended) Camera stabilization level 1.
        /// </summary>
        ///
        CAMERA_STABILIZATION_MODE_LEVEL_1 = 1,

        ///
        /// <summary>
        /// 2: Camera stabilization level 2.
        /// </summary>
        ///
        CAMERA_STABILIZATION_MODE_LEVEL_2 = 2,

        ///
        /// <summary>
        /// 3: Camera stabilization level 3.
        /// </summary>
        ///
        CAMERA_STABILIZATION_MODE_LEVEL_3 = 3,

        ///
        /// @ignore
        ///
        CAMERA_STABILIZATION_MODE_MAX_LEVEL = CAMERA_STABILIZATION_MODE_LEVEL_3,

    }

    ///
    /// <summary>
    /// Local audio state.
    /// </summary>
    ///
    public enum LOCAL_AUDIO_STREAM_STATE
    {
        ///
        /// <summary>
        /// 0: Default initial state of local audio.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_STATE_STOPPED = 0,

        ///
        /// <summary>
        /// 1: Local audio capture device started successfully.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_STATE_RECORDING = 1,

        ///
        /// <summary>
        /// 2: First frame of local audio encoded successfully.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_STATE_ENCODING = 2,

        ///
        /// <summary>
        /// 3: Failed to start local audio.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_STATE_FAILED = 3,

    }

    ///
    /// <summary>
    /// Reason for local audio state change.
    /// </summary>
    ///
    public enum LOCAL_AUDIO_STREAM_REASON
    {
        ///
        /// <summary>
        /// 0: Local audio state is normal.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_REASON_OK = 0,

        ///
        /// <summary>
        /// 1: Local audio error with unknown reason. Suggest prompting the user to rejoin the channel.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_REASON_FAILURE = 1,

        ///
        /// <summary>
        /// 2: No permission to start local audio capture device. Prompt the user to grant permission. Deprecated: This enum is obsolete. Use OnPermissionError callback with RECORD_AUDIO instead.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_REASON_DEVICE_NO_PERMISSION = 2,

        ///
        /// <summary>
        /// 3: (Android and iOS only) Local audio capture device is in use. Prompt the user to check if the microphone is occupied by another app. Local audio capture will automatically resume after the microphone is idle for about 5 seconds, or you can try rejoining the channel after it becomes idle.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_REASON_DEVICE_BUSY = 3,

        ///
        /// <summary>
        /// 4: Failed to capture local audio.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_REASON_RECORD_FAILURE = 4,

        ///
        /// <summary>
        /// 5: Failed to encode local audio.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_REASON_ENCODE_FAILURE = 5,

        ///
        /// <summary>
        /// 6: (Windows and macOS only) No local audio capture device. Prompt the user to check if the microphone is properly connected and working in the device control panel.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_REASON_NO_RECORDING_DEVICE = 6,

        ///
        /// <summary>
        /// 7: (Windows and macOS only) No local audio playback device. Prompt the user to check if the speaker is properly connected and working in the device control panel.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_REASON_NO_PLAYOUT_DEVICE = 7,

        ///
        /// <summary>
        /// 8: (Android and iOS only) Local audio capture was interrupted by incoming calls, voice assistants, or alarms. To resume local audio capture, ask the user to end the call, assistant, or alarm.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_REASON_INTERRUPTED = 8,

        ///
        /// <summary>
        /// 9: (Windows only) Invalid ID of the local audio capture device. Prompt the user to check the audio capture device ID.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_REASON_RECORD_INVALID_ID = 9,

        ///
        /// <summary>
        /// 10: (Windows only) Invalid ID of the local audio playback device. Prompt the user to check the audio playback device ID.
        /// </summary>
        ///
        LOCAL_AUDIO_STREAM_REASON_PLAYOUT_INVALID_ID = 10,

    }

    ///
    /// <summary>
    /// Local video state.
    /// </summary>
    ///
    public enum LOCAL_VIDEO_STREAM_STATE
    {
        ///
        /// <summary>
        /// 0: Default initial state of local video.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_STATE_STOPPED = 0,

        ///
        /// <summary>
        /// 1: Local video capture device started successfully. This state is also reported when sharing a maximized window via StartScreenCaptureByWindowId.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_STATE_CAPTURING = 1,

        ///
        /// <summary>
        /// 2: First frame of local video encoded successfully.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_STATE_ENCODING = 2,

        ///
        /// <summary>
        /// 3: Failed to start local video.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_STATE_FAILED = 3,

    }

    ///
    /// <summary>
    /// Local video event types.
    /// 
    /// Since Available since v4.6.1.
    /// </summary>
    ///
    public enum LOCAL_VIDEO_EVENT_TYPE
    {
        ///
        /// <summary>
        /// (1): The screen capture window is hidden (Android only).
        /// </summary>
        ///
        LOCAL_VIDEO_EVENT_TYPE_SCREEN_CAPTURE_WINDOW_HIDDEN = 1,

        ///
        /// <summary>
        /// (2): The screen capture window recovers from hidden state (Android only).
        /// </summary>
        ///
        LOCAL_VIDEO_EVENT_TYPE_SCREEN_CAPTURE_WINDOW_RECOVER_FROM_HIDDEN = 2,

        ///
        /// <summary>
        /// (3): Screen capture is stopped by the user (Android only).
        /// </summary>
        ///
        LOCAL_VIDEO_EVENT_TYPE_SCREEN_CAPTURE_STOPPED_BY_USER = 3,

        ///
        /// <summary>
        /// (4): A system internal error occurred during screen capture (Android only).
        /// </summary>
        ///
        LOCAL_VIDEO_EVENT_TYPE_SCREEN_CAPTURE_SYSTEM_INTERNAL_ERROR = 4,

    }

    ///
    /// <summary>
    /// Reason for local video state change.
    /// </summary>
    ///
    public enum LOCAL_VIDEO_STREAM_REASON
    {
        ///
        /// <summary>
        /// 0: Local video is in normal state.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_OK = 0,

        ///
        /// <summary>
        /// 1: Unknown error.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_FAILURE = 1,

        ///
        /// <summary>
        /// 2: No permission to start the local video capture device. Prompt the user to enable device permissions and rejoin the channel. Deprecated: This enumeration is deprecated. Use OnPermissionError callback with CAMERA instead.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_DEVICE_NO_PERMISSION = 2,

        ///
        /// <summary>
        /// 3: The local video capture device is in use. Prompt the user to check if the camera is occupied by another app or try rejoining the channel.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_DEVICE_BUSY = 3,

        ///
        /// <summary>
        /// 4: Local video capture failed. Prompt the user to check if the video capture device is working properly, whether the camera is occupied by another app, or try rejoining the channel.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_CAPTURE_FAILURE = 4,

        ///
        /// <summary>
        /// 5: Local video encoding failed.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_CODEC_NOT_SUPPORT = 5,

        ///
        /// <summary>
        /// 6: (iOS only) The app is in the background. Prompt the user that video capture cannot proceed when the app is in the background.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_CAPTURE_INBACKGROUND = 6,

        ///
        /// <summary>
        /// 7: (iOS only) The app window is in Slide Over, Split View, or Picture-in-Picture mode, and another app is using the camera. Prompt the user that video capture cannot proceed in this case.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_CAPTURE_MULTIPLE_FOREGROUND_APPS = 7,

        ///
        /// <summary>
        /// 8: Local video capture device not found. Check whether the camera is properly connected and functioning, or try rejoining the channel.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_DEVICE_NOT_FOUND = 8,

        ///
        /// <summary>
        /// 9: (macOS and Windows only) The video capture device in use has been disconnected (e.g., unplugged).
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_DEVICE_DISCONNECTED = 9,

        ///
        /// <summary>
        /// 10: (macOS and Windows only) The SDK cannot find the video device in the list. Check whether the video device ID is valid.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_DEVICE_INVALID_ID = 10,

        ///
        /// <summary>
        /// 14: (Android only) Video capture interrupted. Possible reasons:
        ///  The camera is occupied by another app. Prompt the user to check if the camera is in use.
        ///  The app has been switched to the background. Use a foreground service notification to ensure video capture continues in the background. See [Why does audio/video capture fail when the app is locked or in the background on some Android versions?](https://doc.shengwang.cn/faq/quality-issues/android-background).
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_DEVICE_INTERRUPT = 14,

        ///
        /// <summary>
        /// 15: (Android only) Video capture device error. Prompt the user to turn off and restart the camera to restore functionality. If the issue persists, check for hardware faults.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_DEVICE_FATAL_ERROR = 15,

        ///
        /// <summary>
        /// 101: The video capture device is unavailable due to excessive system pressure.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_DEVICE_SYSTEM_PRESSURE = 101,

        ///
        /// <summary>
        /// 11: (macOS and Windows only) The window shared via StartScreenCaptureByWindowId is minimized. The SDK cannot share minimized windows. Prompt the user to restore the window.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_WINDOW_MINIMIZED = 11,

        ///
        /// <summary>
        /// 12: (macOS and Windows only) The window shared via window ID is closed, or a fullscreen window shared via window ID has exited fullscreen. After exiting fullscreen, remote users cannot see the shared window. To avoid showing a black screen, it is recommended to end the sharing immediately.
        /// Common scenarios for this error code:
        ///  The local user closes the shared window.
        ///  The local user plays a slideshow and shares it; when the slideshow ends, the SDK reports this error.
        ///  The local user watches a web video or document in fullscreen, then shares it; when exiting fullscreen, the SDK reports this error.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_WINDOW_CLOSED = 12,

        ///
        /// <summary>
        /// 13: (Windows only) The window to be shared is blocked by another window. The blocked area will be blacked out during sharing.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_WINDOW_OCCLUDED = 13,

        ///
        /// @ignore
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_WINDOW_NOT_SUPPORTED = 20,

        ///
        /// <summary>
        /// 21: (Windows and Android only) The current captured window has no data.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_FAILURE = 21,

        ///
        /// <summary>
        /// 22: (Windows and macOS only) No permission to capture the screen.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_NO_PERMISSION = 22,

        ///
        /// <summary>
        /// 24: (Windows only) An unexpected error occurred during screen sharing (possibly due to window exclusion failure), causing the screen sharing strategy to downgrade, but the sharing itself is not affected. During screen sharing, if window exclusion fails due to device driver issues, the SDK reports this event and automatically falls back to sharing the entire screen. If your scenario requires excluding specific windows for privacy, listen for this event and add additional privacy protection mechanisms when triggered.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_AUTO_FALLBACK = 24,

        ///
        /// <summary>
        /// 25: (Windows only) The window being captured is hidden and not visible on the current screen.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_WINDOW_HIDDEN = 25,

        ///
        /// <summary>
        /// 26: (Windows only) The window being captured has recovered from a hidden state.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_WINDOW_RECOVER_FROM_HIDDEN = 26,

        ///
        /// <summary>
        /// 27: (macOS and Windows only) The window being captured has recovered from a minimized state.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_WINDOW_RECOVER_FROM_MINIMIZED = 27,

        ///
        /// <summary>
        /// 28: (Windows only) Screen capture is paused. Common scenario: the current screen may have switched to a secure desktop, such as a UAC dialog or Winlogon desktop.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_PAUSED = 28,

        ///
        /// <summary>
        /// 29: (Windows only) Screen capture has resumed from a paused state.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_RESUMED = 29,

        ///
        /// <summary>
        /// 30: (Windows and macOS only) The display being captured has been disconnected. Prompt the user that screen sharing is paused and restart sharing.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_DISPLAY_DISCONNECTED = 30,

        ///
        /// @ignore
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_STOPPED_BY_USER = 31,

        ///
        /// @ignore
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_INTERRUPTED_BY_OTHER = 32,

        ///
        /// @ignore
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_STOPPED_BY_CALL = 33,

        ///
        /// <summary>
        /// 34: (Windows only) Some windows in the exclusion list failed to be excluded during screen sharing.
        /// </summary>
        ///
        LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_EXCLUDE_WINDOW_FAILED = 34,

    }

    ///
    /// <summary>
    /// Remote audio stream state.
    /// </summary>
    ///
    public enum REMOTE_AUDIO_STATE
    {
        ///
        /// <summary>
        /// 0: Default initial state of the remote audio. This state is reported under REMOTE_AUDIO_REASON_LOCAL_MUTED, REMOTE_AUDIO_REASON_REMOTE_MUTED, or REMOTE_AUDIO_REASON_REMOTE_OFFLINE.
        /// </summary>
        ///
        REMOTE_AUDIO_STATE_STOPPED = 0,

        ///
        /// <summary>
        /// 1: The local user has received the first packet of the remote audio.
        /// </summary>
        ///
        REMOTE_AUDIO_STATE_STARTING = 1,

        ///
        /// <summary>
        /// 2: Remote audio stream is decoding and playing normally. This state is reported under REMOTE_AUDIO_REASON_NETWORK_RECOVERY, REMOTE_AUDIO_REASON_LOCAL_UNMUTED, or REMOTE_AUDIO_REASON_REMOTE_UNMUTED.
        /// </summary>
        ///
        REMOTE_AUDIO_STATE_DECODING = 2,

        ///
        /// <summary>
        /// 3: Remote audio stream is frozen. This state is reported under REMOTE_AUDIO_REASON_NETWORK_CONGESTION.
        /// </summary>
        ///
        REMOTE_AUDIO_STATE_FROZEN = 3,

        ///
        /// <summary>
        /// 4: Remote audio stream playback failed. This state is reported under REMOTE_AUDIO_REASON_INTERNAL.
        /// </summary>
        ///
        REMOTE_AUDIO_STATE_FAILED = 4,

    }

    ///
    /// <summary>
    /// Reason for remote audio stream state change.
    /// </summary>
    ///
    public enum REMOTE_AUDIO_STATE_REASON
    {
        ///
        /// <summary>
        /// 0: This reason is reported when the audio state changes.
        /// </summary>
        ///
        REMOTE_AUDIO_REASON_INTERNAL = 0,

        ///
        /// <summary>
        /// 1: Network congestion.
        /// </summary>
        ///
        REMOTE_AUDIO_REASON_NETWORK_CONGESTION = 1,

        ///
        /// <summary>
        /// 2: Network recovered.
        /// </summary>
        ///
        REMOTE_AUDIO_REASON_NETWORK_RECOVERY = 2,

        ///
        /// <summary>
        /// 3: The local user stops receiving the remote audio stream or disables the audio module.
        /// </summary>
        ///
        REMOTE_AUDIO_REASON_LOCAL_MUTED = 3,

        ///
        /// <summary>
        /// 4: The local user resumes receiving the remote audio stream or enables the audio module.
        /// </summary>
        ///
        REMOTE_AUDIO_REASON_LOCAL_UNMUTED = 4,

        ///
        /// <summary>
        /// 5: The remote user stops sending the audio stream or disables the audio module.
        /// </summary>
        ///
        REMOTE_AUDIO_REASON_REMOTE_MUTED = 5,

        ///
        /// <summary>
        /// 6: The remote user resumes sending the audio stream or enables the audio module.
        /// </summary>
        ///
        REMOTE_AUDIO_REASON_REMOTE_UNMUTED = 6,

        ///
        /// <summary>
        /// 7: The remote user leaves the channel.
        /// </summary>
        ///
        REMOTE_AUDIO_REASON_REMOTE_OFFLINE = 7,

        ///
        /// @ignore
        ///
        REMOTE_AUDIO_REASON_NO_PACKET_RECEIVE = 8,

        ///
        /// @ignore
        ///
        REMOTE_AUDIO_REASON_LOCAL_PLAY_FAILED = 9,

    }

    ///
    /// <summary>
    /// Remote video stream state.
    /// </summary>
    ///
    public enum REMOTE_VIDEO_STATE
    {
        ///
        /// <summary>
        /// 0: Default initial state of the remote video. This state is reported under REMOTE_VIDEO_STATE_REASON_LOCAL_MUTED, REMOTE_VIDEO_STATE_REASON_REMOTE_MUTED, or REMOTE_VIDEO_STATE_REASON_REMOTE_OFFLINE.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_STOPPED = 0,

        ///
        /// <summary>
        /// 1: The local user has received the first packet of the remote video.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_STARTING = 1,

        ///
        /// <summary>
        /// 2: Remote video stream is decoding and playing normally. This state is reported under REMOTE_VIDEO_STATE_REASON_NETWORK_RECOVERY, REMOTE_VIDEO_STATE_REASON_LOCAL_UNMUTED, REMOTE_VIDEO_STATE_REASON_REMOTE_UNMUTED, or REMOTE_VIDEO_STATE_REASON_AUDIO_FALLBACK_RECOVERY.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_DECODING = 2,

        ///
        /// <summary>
        /// 3: Remote video stream is frozen. This state is reported under REMOTE_VIDEO_STATE_REASON_NETWORK_CONGESTION or REMOTE_VIDEO_STATE_REASON_AUDIO_FALLBACK.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_FROZEN = 3,

        ///
        /// <summary>
        /// 4: Remote video stream playback failed. This state is reported under REMOTE_VIDEO_STATE_REASON_INTERNAL.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_FAILED = 4,

    }

    ///
    /// <summary>
    /// Reason for remote video stream state change.
    /// </summary>
    ///
    public enum REMOTE_VIDEO_STATE_REASON
    {
        ///
        /// <summary>
        /// 0: This reason is reported when the video state changes.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_REASON_INTERNAL = 0,

        ///
        /// <summary>
        /// 1: Network congestion.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_REASON_NETWORK_CONGESTION = 1,

        ///
        /// <summary>
        /// 2: Network recovered.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_REASON_NETWORK_RECOVERY = 2,

        ///
        /// <summary>
        /// 3: The local user stops receiving the remote video stream or disables the video module.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_REASON_LOCAL_MUTED = 3,

        ///
        /// <summary>
        /// 4: The local user resumes receiving the remote video stream or enables the video module.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_REASON_LOCAL_UNMUTED = 4,

        ///
        /// <summary>
        /// 5: The remote user stops sending the video stream or disables the video module.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_REASON_REMOTE_MUTED = 5,

        ///
        /// <summary>
        /// 6: The remote user resumes sending the video stream or enables the video module.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_REASON_REMOTE_UNMUTED = 6,

        ///
        /// <summary>
        /// 7: The remote user leaves the channel.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_REASON_REMOTE_OFFLINE = 7,

        ///
        /// <summary>
        /// 8: Under poor network conditions, the remote audio/video stream falls back to audio only.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_REASON_AUDIO_FALLBACK = 8,

        ///
        /// <summary>
        /// 9: When the network improves, the remote audio stream recovers to audio/video stream.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_REASON_AUDIO_FALLBACK_RECOVERY = 9,

        ///
        /// @ignore
        ///
        REMOTE_VIDEO_STATE_REASON_VIDEO_STREAM_TYPE_CHANGE_TO_LOW = 10,

        ///
        /// @ignore
        ///
        REMOTE_VIDEO_STATE_REASON_VIDEO_STREAM_TYPE_CHANGE_TO_HIGH = 11,

        ///
        /// <summary>
        /// 12: (iOS only) The remote user's app has switched to the background.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_REASON_SDK_IN_BACKGROUND = 12,

        ///
        /// <summary>
        /// 13: The local video decoder does not support decoding the received remote video stream.
        /// </summary>
        ///
        REMOTE_VIDEO_STATE_REASON_CODEC_NOT_SUPPORT = 13,

    }

    ///
    /// @ignore
    ///
    public enum REMOTE_USER_STATE
    {
        ///
        /// @ignore
        ///
        USER_STATE_MUTE_AUDIO = (1 << 0),

        ///
        /// @ignore
        ///
        USER_STATE_MUTE_VIDEO = (1 << 1),

        ///
        /// @ignore
        ///
        USER_STATE_ENABLE_VIDEO = (1 << 4),

        ///
        /// @ignore
        ///
        USER_STATE_ENABLE_LOCAL_VIDEO = (1 << 8),

    }

    ///
    /// @ignore
    ///
    public class VideoTrackInfo
    {
        ///
        /// @ignore
        ///
        public bool isLocal;

        ///
        /// @ignore
        ///
        public uint ownerUid;

        ///
        /// @ignore
        ///
        public uint trackId;

        ///
        /// @ignore
        ///
        public string channelId;

        ///
        /// @ignore
        ///
        public VIDEO_CODEC_TYPE codecType;

        ///
        /// @ignore
        ///
        public bool encodedFrameOnly;

        ///
        /// @ignore
        ///
        public VIDEO_SOURCE_TYPE sourceType;

        ///
        /// @ignore
        ///
        public uint observationPosition;

        public VideoTrackInfo()
        {
            this.isLocal = false;
            this.ownerUid = 0;
            this.trackId = 0;
            this.channelId = "";
            this.codecType = VIDEO_CODEC_TYPE.VIDEO_CODEC_H265;
            this.encodedFrameOnly = false;
            this.sourceType = VIDEO_SOURCE_TYPE.VIDEO_SOURCE_CAMERA_PRIMARY;
            this.observationPosition = (uint)VIDEO_MODULE_POSITION.POSITION_POST_CAPTURER;
        }

        public VideoTrackInfo(bool isLocal, uint ownerUid, uint trackId, string channelId, VIDEO_CODEC_TYPE codecType, bool encodedFrameOnly, VIDEO_SOURCE_TYPE sourceType, uint observationPosition)
        {
            this.isLocal = isLocal;
            this.ownerUid = ownerUid;
            this.trackId = trackId;
            this.channelId = channelId;
            this.codecType = codecType;
            this.encodedFrameOnly = encodedFrameOnly;
            this.sourceType = sourceType;
            this.observationPosition = observationPosition;
        }
    }

    ///
    /// @ignore
    ///
    public enum REMOTE_VIDEO_DOWNSCALE_LEVEL
    {
        ///
        /// @ignore
        ///
        REMOTE_VIDEO_DOWNSCALE_LEVEL_NONE,

        ///
        /// @ignore
        ///
        REMOTE_VIDEO_DOWNSCALE_LEVEL_1,

        ///
        /// @ignore
        ///
        REMOTE_VIDEO_DOWNSCALE_LEVEL_2,

        ///
        /// @ignore
        ///
        REMOTE_VIDEO_DOWNSCALE_LEVEL_3,

        ///
        /// @ignore
        ///
        REMOTE_VIDEO_DOWNSCALE_LEVEL_4,

    }

    ///
    /// <summary>
    /// User volume information.
    /// </summary>
    ///
    public class AudioVolumeInfo
    {
        ///
        /// <summary>
        /// User ID.
        ///  In the callback for the local user, uid is 0.
        ///  In the callback for remote users, uid is the ID of the remote user with the highest instantaneous volume (up to 3 users).
        /// </summary>
        ///
        public uint uid;

        ///
        /// <summary>
        /// User's volume, ranging from [0,255]. If the user mutes themselves (sets MuteLocalAudioStream to true) but audio capture is still enabled, the volume value indicates the volume of the locally captured signal.
        /// </summary>
        ///
        public uint volume;

        ///
        /// <summary>
        /// vad cannot report the voice activity status of remote users. For remote users, the value of vad is always 1.
        ///  To use this parameter, set reportVad to true when calling EnableAudioVolumeIndication. Voice activity status of the local user.
        ///  0: No voice detected locally.
        ///  1: Voice detected locally.
        /// </summary>
        ///
        public uint vad;

        ///
        /// <summary>
        /// Voice pitch of the local user (Hz). Value range: [0.0, 4000.0]. voicePitch cannot report the voice pitch of remote users. For remote users, the value of voicePitch is always 0.0.
        /// </summary>
        ///
        public double voicePitch;

        public AudioVolumeInfo()
        {
            this.uid = 0;
            this.volume = 0;
            this.vad = 0;
            this.voicePitch = 0.0f;
        }

        public AudioVolumeInfo(uint uid, uint volume, uint vad, double voicePitch)
        {
            this.uid = uid;
            this.volume = volume;
            this.vad = vad;
            this.voicePitch = voicePitch;
        }
    }

    ///
    /// <summary>
    /// Sampling rate of audio for stream output.
    /// </summary>
    ///
    public enum AUDIO_SAMPLE_RATE_TYPE
    {
        ///
        /// <summary>
        /// 32000: 32 kHz
        /// </summary>
        ///
        AUDIO_SAMPLE_RATE_32000 = 32000,

        ///
        /// <summary>
        /// 44100: 44.1 kHz
        /// </summary>
        ///
        AUDIO_SAMPLE_RATE_44100 = 44100,

        ///
        /// <summary>
        /// 48000: (Default) 48 kHz
        /// </summary>
        ///
        AUDIO_SAMPLE_RATE_48000 = 48000,

    }

    ///
    /// <summary>
    /// Codec type for transcoded video stream output.
    /// </summary>
    ///
    public enum VIDEO_CODEC_TYPE_FOR_STREAM
    {
        ///
        /// <summary>
        /// 1: (Default) H.264.
        /// </summary>
        ///
        VIDEO_CODEC_H264_FOR_STREAM = 1,

        ///
        /// <summary>
        /// 2: H.265.
        /// </summary>
        ///
        VIDEO_CODEC_H265_FOR_STREAM = 2,

    }

    ///
    /// <summary>
    /// Codec profile type for video output in relayed streaming.
    /// </summary>
    ///
    public enum VIDEO_CODEC_PROFILE_TYPE
    {
        ///
        /// <summary>
        /// 66: Baseline profile, typically used for lower-end or error-resilient applications such as video calls and mobile videos.
        /// </summary>
        ///
        VIDEO_CODEC_PROFILE_BASELINE = 66,

        ///
        /// <summary>
        /// 77: Main profile, typically used in mainstream consumer electronics such as MP4, portable video players, PSP, iPad, etc.
        /// </summary>
        ///
        VIDEO_CODEC_PROFILE_MAIN = 77,

        ///
        /// <summary>
        /// 100: (Default) High profile, typically used in broadcasting, video disc storage, and HDTV.
        /// </summary>
        ///
        VIDEO_CODEC_PROFILE_HIGH = 100,

    }

    ///
    /// <summary>
    /// Audio codec profile for streaming output. Defaults to LC-AAC.
    /// </summary>
    ///
    public enum AUDIO_CODEC_PROFILE_TYPE
    {
        ///
        /// <summary>
        /// 0: (Default) LC-AAC profile.
        /// </summary>
        ///
        AUDIO_CODEC_PROFILE_LC_AAC = 0,

        ///
        /// <summary>
        /// 1: HE-AAC profile.
        /// </summary>
        ///
        AUDIO_CODEC_PROFILE_HE_AAC = 1,

        ///
        /// <summary>
        /// 2: HE-AAC v2 profile.
        /// </summary>
        ///
        AUDIO_CODEC_PROFILE_HE_AAC_V2 = 2,

    }

    ///
    /// <summary>
    /// Local audio statistics.
    /// </summary>
    ///
    public class LocalAudioStats
    {
        ///
        /// <summary>
        /// Number of audio channels.
        /// </summary>
        ///
        public int numChannels;

        ///
        /// <summary>
        /// Sampling rate of the sent local audio, in Hz.
        /// </summary>
        ///
        public int sentSampleRate;

        ///
        /// <summary>
        /// Average bitrate of the sent local audio, in Kbps.
        /// </summary>
        ///
        public int sentBitrate;

        ///
        /// <summary>
        /// Internal payload type.
        /// </summary>
        ///
        public int internalCodec;

        ///
        /// <summary>
        /// Packet loss rate (%) from the local end to the Agora edge server before network resilience.
        /// </summary>
        ///
        public ushort txPacketLossRate;

        ///
        /// <summary>
        /// Delay of the audio device module during audio playback or recording (ms).
        /// </summary>
        ///
        public int audioDeviceDelay;

        ///
        /// @ignore
        ///
        public int audioPlayoutDelay;

        ///
        /// <summary>
        /// Ear monitoring delay (ms), i.e., the delay from microphone input to headphone output.
        /// </summary>
        ///
        public int earMonitorDelay;

        ///
        /// <summary>
        /// Acoustic Echo Cancellation (AEC) delay (ms), i.e., the delay between the audio played locally and the signal captured again locally as estimated by the AEC module.
        /// </summary>
        ///
        public int aecEstimatedDelay;

        public LocalAudioStats(int numChannels, int sentSampleRate, int sentBitrate, int internalCodec, ushort txPacketLossRate, int audioDeviceDelay, int audioPlayoutDelay, int earMonitorDelay, int aecEstimatedDelay)
        {
            this.numChannels = numChannels;
            this.sentSampleRate = sentSampleRate;
            this.sentBitrate = sentBitrate;
            this.internalCodec = internalCodec;
            this.txPacketLossRate = txPacketLossRate;
            this.audioDeviceDelay = audioDeviceDelay;
            this.audioPlayoutDelay = audioPlayoutDelay;
            this.earMonitorDelay = earMonitorDelay;
            this.aecEstimatedDelay = aecEstimatedDelay;
        }
        public LocalAudioStats()
        {
        }

    }

    ///
    /// <summary>
    /// Streaming state.
    /// </summary>
    ///
    public enum RTMP_STREAM_PUBLISH_STATE
    {
        ///
        /// <summary>
        /// 0: Streaming has not started or has ended.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_STATE_IDLE = 0,

        ///
        /// <summary>
        /// 1: Connecting to the streaming server and CDN server.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_STATE_CONNECTING = 1,

        ///
        /// <summary>
        /// 2: Streaming is in progress. This state is returned after successful streaming.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_STATE_RUNNING = 2,

        ///
        /// <summary>
        /// 3: Recovering the stream. When a CDN exception occurs or the stream is briefly interrupted, the SDK automatically attempts to recover the stream and returns this state.
        ///  If the stream is successfully recovered, it enters the RTMP_STREAM_PUBLISH_STATE_RUNNING(2) state.
        ///  If the server encounters an error or recovery fails within 60 seconds, it enters the RTMP_STREAM_PUBLISH_STATE_FAILURE(4) state. If you think 60 seconds is too long, you can also try reconnecting manually.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_STATE_RECOVERING = 3,

        ///
        /// <summary>
        /// 4: Streaming failed. After failure, you can troubleshoot the cause using the returned error code.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_STATE_FAILURE = 4,

        ///
        /// <summary>
        /// 5: The SDK is disconnecting from the streaming server and CDN server. When you call the StopRtmpStream method to end streaming normally, the SDK reports the streaming states in sequence: RTMP_STREAM_PUBLISH_STATE_DISCONNECTING, RTMP_STREAM_PUBLISH_STATE_IDLE.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_STATE_DISCONNECTING = 5,

    }

    ///
    /// <summary>
    /// Reason for stream state change.
    /// </summary>
    ///
    public enum RTMP_STREAM_PUBLISH_REASON
    {
        ///
        /// <summary>
        /// 0: Stream published successfully.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_OK = 0,

        ///
        /// <summary>
        /// 1: Invalid parameters. Please check whether the input parameters are correct.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_INVALID_ARGUMENT = 1,

        ///
        /// <summary>
        /// 2: The stream is encrypted and cannot be published.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_ENCRYPTED_STREAM_NOT_ALLOWED = 2,

        ///
        /// <summary>
        /// 3: Stream publishing timed out and failed.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_CONNECTION_TIMEOUT = 3,

        ///
        /// <summary>
        /// 4: An error occurred on the streaming server.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_INTERNAL_SERVER_ERROR = 4,

        ///
        /// <summary>
        /// 5: An error occurred on the CDN server.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_RTMP_SERVER_ERROR = 5,

        ///
        /// <summary>
        /// 6: Streaming requests are too frequent.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_TOO_OFTEN = 6,

        ///
        /// <summary>
        /// 7: The number of stream URLs for a single host has reached the limit of 10. Please delete some unused stream URLs before adding new ones.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_REACH_LIMIT = 7,

        ///
        /// <summary>
        /// 8: The host is operating on a stream that does not belong to them. For example, updating another host's stream parameters or stopping another host's stream. Please check your app logic.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_NOT_AUTHORIZED = 8,

        ///
        /// <summary>
        /// 9: The server could not find the stream.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_STREAM_NOT_FOUND = 9,

        ///
        /// <summary>
        /// 10: The stream URL format is incorrect. Please check whether the stream URL format is correct.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_FORMAT_NOT_SUPPORTED = 10,

        ///
        /// <summary>
        /// 11: The user role is not a broadcaster and cannot use the streaming function. Please check your application code logic.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_NOT_BROADCASTER = 11,

        ///
        /// <summary>
        /// 13: Called the UpdateRtmpTranscoding method to update transcoding properties in a non-transcoding stream scenario. Please check your application code logic.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_TRANSCODING_NO_MIX_STREAM = 13,

        ///
        /// <summary>
        /// 14: The host's network encountered an error.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_NET_DOWN = 14,

        ///
        /// @ignore
        ///
        RTMP_STREAM_PUBLISH_REASON_INVALID_APPID = 15,

        ///
        /// <summary>
        /// 16: Your project does not have permission to use the streaming service.
        /// </summary>
        ///
        RTMP_STREAM_PUBLISH_REASON_INVALID_PRIVILEGE = 16,

        ///
        /// <summary>
        /// 100: Streaming ended normally. After you stop streaming, the SDK returns this value.
        /// </summary>
        ///
        RTMP_STREAM_UNPUBLISH_REASON_OK = 100,

    }

    ///
    /// <summary>
    /// Events that occur during relayed streaming.
    /// </summary>
    ///
    public enum RTMP_STREAMING_EVENT
    {
        ///
        /// <summary>
        /// 1: Error adding background image or watermark during relayed streaming.
        /// </summary>
        ///
        RTMP_STREAMING_EVENT_FAILED_LOAD_IMAGE = 1,

        ///
        /// <summary>
        /// 2: The stream URL is already in use. If you want to start a new stream, please use a new stream URL.
        /// </summary>
        ///
        RTMP_STREAMING_EVENT_URL_ALREADY_IN_USE = 2,

        ///
        /// <summary>
        /// 3: Feature not supported.
        /// </summary>
        ///
        RTMP_STREAMING_EVENT_ADVANCED_FEATURE_NOT_SUPPORT = 3,

        ///
        /// <summary>
        /// 4: Reserved parameter.
        /// </summary>
        ///
        RTMP_STREAMING_EVENT_REQUEST_TOO_OFTEN = 4,

    }

    ///
    /// <summary>
    /// Image properties.
    /// 
    /// Used to set the watermark and background image properties for live video.
    /// </summary>
    ///
    public class RtcImage
    {
        ///
        /// <summary>
        /// HTTP/HTTPS address of the image on the live video. The character length must not exceed 1024 bytes.
        /// </summary>
        ///
        public string url;

        ///
        /// <summary>
        /// The x-coordinate (in px) of the image on the video frame, with the top-left corner of the output video frame as the origin.
        /// </summary>
        ///
        public int x;

        ///
        /// <summary>
        /// The y-coordinate (in px) of the image on the video frame, with the top-left corner of the output video frame as the origin.
        /// </summary>
        ///
        public int y;

        ///
        /// <summary>
        /// The width (in px) of the image on the video frame.
        /// </summary>
        ///
        public int width;

        ///
        /// <summary>
        /// The height (in px) of the image on the video frame.
        /// </summary>
        ///
        public int height;

        ///
        /// <summary>
        /// Z-order of the watermark or background image. When using an array of watermarks to add one or more watermarks, you must assign a value to zOrder, with a valid range of [1,255], otherwise the SDK will report an error. In other cases, zOrder is optional, with a valid range of [0,255]. 0 is the default value. 0 represents the bottom layer, and 255 represents the top layer.
        /// </summary>
        ///
        public int zOrder;

        ///
        /// <summary>
        /// Transparency of the watermark or background image. Value range is [0.0,1.0]:
        ///  0.0: Fully transparent.
        ///  1.0: (Default) Fully opaque.
        /// </summary>
        ///
        public double alpha;

        public RtcImage()
        {
            this.url = "";
            this.x = 0;
            this.y = 0;
            this.width = 0;
            this.height = 0;
            this.zOrder = 0;
            this.alpha = 1.0;
        }

        public RtcImage(string url, int x, int y, int width, int height, int zOrder, double alpha)
        {
            this.url = url;
            this.x = x;
            this.y = y;
            this.width = width;
            this.height = height;
            this.zOrder = zOrder;
            this.alpha = alpha;
        }
    }

    ///
    /// <summary>
    /// Advanced configuration for transcoding live streaming.
    /// 
    /// To use advanced features for transcoding live streaming, please [contact sales](https://www.shengwang.cn/contact-sales/).
    /// </summary>
    ///
    public class LiveStreamAdvancedFeature
    {
        ///
        /// <summary>
        /// The name of the advanced transcoding live streaming feature, including LBHQ (low-bitrate high-quality video) and VEO (optimized video encoder).
        /// </summary>
        ///
        public string featureName;

        ///
        /// <summary>
        /// Whether to enable the advanced transcoding live streaming feature: true : Enables the advanced transcoding live streaming feature. false : (Default) Disables the advanced transcoding live streaming feature.
        /// </summary>
        ///
        public bool opened;

        public LiveStreamAdvancedFeature()
        {
            this.featureName = "";
            this.opened = false;
        }

        public LiveStreamAdvancedFeature(string feat_name, bool open)
        {
            this.featureName = feat_name;
            this.opened = open;
        }

    }

    ///
    /// <summary>
    /// Network connection state.
    /// </summary>
    ///
    public enum CONNECTION_STATE_TYPE
    {
        ///
        /// <summary>
        /// 1: Network disconnected. This state indicates the SDK is:
        ///  In the initialization phase before calling JoinChannel [2/2].
        ///  Or in the leave phase after calling LeaveChannel [2/2].
        /// </summary>
        ///
        CONNECTION_STATE_DISCONNECTED = 1,

        ///
        /// <summary>
        /// 2: Connecting to the network. This state indicates the SDK is establishing a connection to the specified channel after calling JoinChannel [2/2].
        ///  If the channel is joined successfully, the app receives the OnConnectionStateChanged callback indicating the state changes to CONNECTION_STATE_CONNECTED.
        ///  After the connection is established, the SDK initializes media and then triggers OnJoinChannelSuccess when ready.
        /// </summary>
        ///
        CONNECTION_STATE_CONNECTING = 2,

        ///
        /// <summary>
        /// 3: Network connected. This state indicates the user has joined the channel and can publish or subscribe to media streams. If the connection is interrupted due to network issues or switching, the SDK automatically reconnects. The app receives the OnConnectionStateChanged callback indicating the state changes to CONNECTION_STATE_RECONNECTING.
        /// </summary>
        ///
        CONNECTION_STATE_CONNECTED = 3,

        ///
        /// <summary>
        /// 4: Reconnecting to the network. This state indicates the SDK had previously joined the channel but got disconnected due to network issues. The SDK automatically attempts to rejoin the channel.
        ///  If the SDK fails to rejoin within 10 seconds, OnConnectionLost is triggered. The SDK remains in CONNECTION_STATE_RECONNECTING and keeps trying to rejoin.
        ///  If the SDK fails to rejoin within 20 minutes, the app receives the OnConnectionStateChanged callback indicating the state changes to CONNECTION_STATE_FAILED, and the SDK stops trying to reconnect.
        /// </summary>
        ///
        CONNECTION_STATE_RECONNECTING = 4,

        ///
        /// <summary>
        /// 5: Network connection failed. This state indicates the SDK has stopped trying to rejoin the channel. You need to call LeaveChannel [1/2] to leave the channel.
        ///  If the user wants to rejoin, call JoinChannel [2/2] again.
        ///  If the SDK is prevented from joining by the server using RESTful API, the app receives OnConnectionStateChanged.
        /// </summary>
        ///
        CONNECTION_STATE_FAILED = 5,

    }

    ///
    /// <summary>
    /// Settings for each host participating in the transcoding mix.
    /// </summary>
    ///
    public class TranscodingUser
    {
        ///
        /// <summary>
        /// User ID of the host.
        /// </summary>
        ///
        public uint uid;

        ///
        /// <summary>
        /// The x-coordinate (px) of the host video in the output video, with the top-left corner of the output video as the origin. Value range: [0,width], where width is set in LiveTranscoding.
        /// </summary>
        ///
        public int x;

        ///
        /// <summary>
        /// The y-coordinate (px) of the host video in the output video, with the top-left corner of the output video as the origin. Value range: [0,height], where height is set in LiveTranscoding.
        /// </summary>
        ///
        public int y;

        ///
        /// <summary>
        /// Width (px) of the host video.
        /// </summary>
        ///
        public int width;

        ///
        /// <summary>
        /// Height (px) of the host video.
        /// </summary>
        ///
        public int height;

        ///
        /// <summary>
        /// If the value is less than 0 or greater than 100, the error ERR_INVALID_ARGUMENT is returned.
        ///  Setting zOrder to 0 is supported. Layer number of the host video. Value range: [0,100].
        ///  0: (Default) Video is at the bottom layer.
        ///  100: Video is at the top layer.
        /// </summary>
        ///
        public int zOrder;

        ///
        /// <summary>
        /// Transparency of the host video. Value range: [0.0,1.0].
        ///  0.0: Fully transparent.
        ///  1.0: (Default) Fully opaque.
        /// </summary>
        ///
        public double alpha;

        ///
        /// <summary>
        /// When the value is not 0, a special player is required. The audio channel occupied by the host's audio in the output audio. Default is 0. Value range: [0,5]: 0 : (Recommended) Default audio mixing setting, supports up to stereo, related to the host's upstream audio. 1 : Host audio is in the FL channel of the output audio. If the upstream audio is multi-channel, the Agora server mixes it into mono first. 2 : Host audio is in the FC channel of the output audio. If the upstream audio is multi-channel, the Agora server mixes it into mono first. 3 : Host audio is in the FR channel of the output audio. If the upstream audio is multi-channel, the Agora server mixes it into mono first. 4 : Host audio is in the BL channel of the output audio. If the upstream audio is multi-channel, the Agora server mixes it into mono first. 5 : Host audio is in the BR channel of the output audio. If the upstream audio is multi-channel, the Agora server mixes it into mono first. 0xFF or values greater than 5 : This host's audio is muted, and the Agora server removes the host's audio.
        /// </summary>
        ///
        public int audioChannel;

        public TranscodingUser()
        {
            this.uid = 0;
            this.x = 0;
            this.y = 0;
            this.width = 0;
            this.height = 0;
            this.zOrder = 0;
            this.alpha = 1.0;
            this.audioChannel = 0;
        }

        public TranscodingUser(uint uid, int x, int y, int width, int height, int zOrder, double alpha, int audioChannel)
        {
            this.uid = uid;
            this.x = x;
            this.y = y;
            this.width = width;
            this.height = height;
            this.zOrder = zOrder;
            this.alpha = alpha;
            this.audioChannel = audioChannel;
        }
    }

    ///
    /// <summary>
    /// Transcoding properties for RTMP streaming.
    /// </summary>
    ///
    public class LiveTranscoding
    {
        ///
        /// <summary>
        /// The total width of the video stream, in pixels. Default is 360.
        ///  For video streams, the value range is [64,1920]. If the value is less than 64, the Agora server adjusts it to 64; if greater than 1920, it adjusts to 1920.
        ///  For audio-only streams, set both width and height to 0.
        /// </summary>
        ///
        public int width;

        ///
        /// <summary>
        /// The total height of the video stream, in pixels. Default is 640.
        ///  For video streams, the value range is [64,1080]. If the value is less than 64, the Agora server adjusts it to 64; if greater than 1080, it adjusts to 1080.
        ///  For audio-only streams, set both width and height to 0.
        /// </summary>
        ///
        public int height;

        ///
        /// <summary>
        /// Video encoding bitrate in Kbps. See BITRATE. You don't need to set this parameter; keep the default STANDARD_BITRATE. The SDK automatically matches the optimal bitrate based on your video resolution and frame rate. For more on resolution and frame rate, see [Video Profile](https://doc.shengwang.cn/doc/rtc/unity/basic-features/video-profile#%E8%A7%86%E9%A2%91%E5%B1%9E%E6%80%A7%E5%8F%82%E8%80%83).
        /// </summary>
        ///
        public int videoBitrate;

        ///
        /// <summary>
        /// Frame rate of the output video for RTMP streaming. Range is (0,30], in fps. Default is 15 fps. The Agora server adjusts any frame rate above 30 fps to 30 fps.
        /// </summary>
        ///
        public int videoFramerate;

        ///
        /// <summary>
        /// Deprecated. Not recommended for use. Low-latency mode true : Low latency, lower video quality. false : (Default) Higher latency, better video quality.
        /// </summary>
        ///
        public bool lowLatency;

        ///
        /// <summary>
        /// GOP (Group of Pictures) of the output video for RTMP streaming, in frames. Default is 30.
        /// </summary>
        ///
        public int videoGop;

        ///
        /// <summary>
        /// Codec profile of the output video for RTMP streaming. Can be set to 66, 77, or 100. See VIDEO_CODEC_PROFILE_TYPE. If you set this to another value, the Agora server resets it to the default.
        /// </summary>
        ///
        public VIDEO_CODEC_PROFILE_TYPE videoCodecProfile;

        ///
        /// <summary>
        /// Background color of the output video for RTMP streaming, represented as a hexadecimal RGB integer without the # symbol. For example, 0xFFB6C1 is light pink. Default is 0x000000 (black).
        /// </summary>
        ///
        public uint backgroundColor;

        ///
        /// <summary>
        /// Codec type of the output video for RTMP streaming. See VIDEO_CODEC_TYPE_FOR_STREAM.
        /// </summary>
        ///
        public VIDEO_CODEC_TYPE_FOR_STREAM videoCodecType;

        ///
        /// <summary>
        /// Number of users participating in the video mixing. Default is 0. Range: [0,17].
        /// </summary>
        ///
        public uint userCount;

        ///
        /// <summary>
        /// Manages the users participating in the video mixing for RTMP streaming. Supports up to 17 users. See TranscodingUser.
        /// </summary>
        ///
        public TranscodingUser[] transcodingUsers;

        ///
        /// <summary>
        /// Reserved parameter: Custom information sent to the RTMP client, used to populate SEI frames in H264/H265 video. Max length: 4096 bytes. For more details on SEI, see [SEI Frame Issues](https://doc.shengwang.cn/faq/quality-issues/sei).
        /// </summary>
        ///
        public string transcodingExtraInfo;

        ///
        /// <summary>
        /// Metadata sent to CDN clients. Deprecated. Not recommended for use.
        /// </summary>
        ///
        public string metadata;

        ///
        /// <summary>
        /// Watermark(s) on the live video. PNG format is required. See RtcImage.
        /// You can add one watermark or use an array to add multiple. Use this parameter together with watermarkCount.
        /// </summary>
        ///
        public RtcImage[] watermark;

        ///
        /// <summary>
        /// Number of watermarks on the live video. The total number of watermarks and background images must be between 0 and 10. Use this parameter together with watermark.
        /// </summary>
        ///
        public uint watermarkCount;

        ///
        /// <summary>
        /// Background image(s) on the live video. PNG format is required. See RtcImage.
        /// You can add one background image or use an array to add multiple. Use this parameter together with backgroundImageCount.
        /// </summary>
        ///
        public RtcImage[] backgroundImage;

        ///
        /// <summary>
        /// Number of background images on the live video. The total number of watermarks and background images must be between 0 and 10. Use this parameter together with backgroundImage.
        /// </summary>
        ///
        public uint backgroundImageCount;

        ///
        /// <summary>
        /// Audio sample rate (Hz) of the output media stream for RTMP streaming. See AUDIO_SAMPLE_RATE_TYPE.
        /// </summary>
        ///
        public AUDIO_SAMPLE_RATE_TYPE audioSampleRate;

        ///
        /// <summary>
        /// Bitrate of the output audio for RTMP streaming, in Kbps. Default is 48, maximum is 128.
        /// </summary>
        ///
        public int audioBitrate;

        ///
        /// <summary>
        /// Number of audio channels in the output audio for RTMP streaming. Default is 1. Valid values are integers in [1,5]. Recommended values are 1 or 2. Values 3, 4, and 5 require special player support:
        ///  1: (Default) Mono
        ///  2: Stereo
        ///  3: Three channels
        ///  4: Four channels
        ///  5: Five channels
        /// </summary>
        ///
        public int audioChannels;

        ///
        /// <summary>
        /// Codec profile of the output audio for RTMP streaming. See AUDIO_CODEC_PROFILE_TYPE.
        /// </summary>
        ///
        public AUDIO_CODEC_PROFILE_TYPE audioCodecProfile;

        ///
        /// <summary>
        /// Advanced features for transcoding and streaming. See LiveStreamAdvancedFeature.
        /// </summary>
        ///
        public LiveStreamAdvancedFeature[] advancedFeatures;

        ///
        /// <summary>
        /// Number of enabled advanced features. Default is 0.
        /// </summary>
        ///
        public uint advancedFeatureCount;

        public LiveTranscoding()
        {
            this.width = 360;
            this.height = 640;
            this.videoBitrate = 400;
            this.videoFramerate = 15;
            this.lowLatency = false;
            this.videoGop = 30;
            this.videoCodecProfile = VIDEO_CODEC_PROFILE_TYPE.VIDEO_CODEC_PROFILE_HIGH;
            this.backgroundColor = 0x000000;
            this.videoCodecType = VIDEO_CODEC_TYPE_FOR_STREAM.VIDEO_CODEC_H264_FOR_STREAM;
            this.userCount = 0;
            this.transcodingUsers = new TranscodingUser[0];
            this.transcodingExtraInfo = "";
            this.metadata = "";
            this.watermark = new RtcImage[0];
            this.watermarkCount = 0;
            this.backgroundImage = new RtcImage[0];
            this.backgroundImageCount = 0;
            this.audioSampleRate = AUDIO_SAMPLE_RATE_TYPE.AUDIO_SAMPLE_RATE_48000;
            this.audioBitrate = 48;
            this.audioChannels = 1;
            this.audioCodecProfile = AUDIO_CODEC_PROFILE_TYPE.AUDIO_CODEC_PROFILE_LC_AAC;
            this.advancedFeatures = new LiveStreamAdvancedFeature[0];
            this.advancedFeatureCount = 0;
        }

        public LiveTranscoding(int width, int height, int videoBitrate, int videoFramerate, bool lowLatency, int videoGop, VIDEO_CODEC_PROFILE_TYPE videoCodecProfile, uint backgroundColor, VIDEO_CODEC_TYPE_FOR_STREAM videoCodecType, uint userCount, TranscodingUser[] transcodingUsers, string transcodingExtraInfo, string metadata, RtcImage[] watermark, uint watermarkCount, RtcImage[] backgroundImage, uint backgroundImageCount, AUDIO_SAMPLE_RATE_TYPE audioSampleRate, int audioBitrate, int audioChannels, AUDIO_CODEC_PROFILE_TYPE audioCodecProfile, LiveStreamAdvancedFeature[] advancedFeatures, uint advancedFeatureCount)
        {
            this.width = width;
            this.height = height;
            this.videoBitrate = videoBitrate;
            this.videoFramerate = videoFramerate;
            this.lowLatency = lowLatency;
            this.videoGop = videoGop;
            this.videoCodecProfile = videoCodecProfile;
            this.backgroundColor = backgroundColor;
            this.videoCodecType = videoCodecType;
            this.userCount = userCount;
            this.transcodingUsers = transcodingUsers;
            this.transcodingExtraInfo = transcodingExtraInfo;
            this.metadata = metadata;
            this.watermark = watermark;
            this.watermarkCount = watermarkCount;
            this.backgroundImage = backgroundImage;
            this.backgroundImageCount = backgroundImageCount;
            this.audioSampleRate = audioSampleRate;
            this.audioBitrate = audioBitrate;
            this.audioChannels = audioChannels;
            this.audioCodecProfile = audioCodecProfile;
            this.advancedFeatures = advancedFeatures;
            this.advancedFeatureCount = advancedFeatureCount;
        }
    }

    ///
    /// <summary>
    /// Video streams participating in local composition.
    /// </summary>
    ///
    public class TranscodingVideoStream
    {
        ///
        /// <summary>
        /// The video source type participating in local composition. See VIDEO_SOURCE_TYPE.
        /// </summary>
        ///
        public VIDEO_SOURCE_TYPE sourceType;

        ///
        /// <summary>
        /// Remote user ID. Use this parameter only when the video source type for local composition is VIDEO_SOURCE_REMOTE.
        /// </summary>
        ///
        public uint remoteUserUid;

        ///
        /// <summary>
        /// Use this parameter only when the video source type for local composition is an image. Path to the local image. Example paths:
        ///  Android: /storage/emulated/0/Pictures/image.png
        ///  iOS: /var/mobile/Containers/Data/Application/<APP-UUID>/Documents/image.png
        ///  macOS: ~/Pictures/image.png
        ///  Windows: C:\\Users\\{username}\\Pictures\\image.png
        /// </summary>
        ///
        public string imageUrl;

        ///
        /// <summary>
        /// (Optional) Media player ID. You need to set this parameter when sourceType is VIDEO_SOURCE_MEDIA_PLAYER.
        /// </summary>
        ///
        public int mediaPlayerId;

        ///
        /// <summary>
        /// Horizontal offset of the top-left corner of the video participating in local composition relative to the top-left corner (origin) of the composition canvas.
        /// </summary>
        ///
        public int x;

        ///
        /// <summary>
        /// Vertical offset of the top-left corner of the video participating in local composition relative to the top-left corner (origin) of the composition canvas.
        /// </summary>
        ///
        public int y;

        ///
        /// <summary>
        /// Width (px) of the video participating in local composition.
        /// </summary>
        ///
        public int width;

        ///
        /// <summary>
        /// Height (px) of the video participating in local composition.
        /// </summary>
        ///
        public int height;

        ///
        /// <summary>
        /// Layer number of the video participating in local composition. Value range: [0,100].
        ///  0: (Default) Layer is at the bottom.
        ///  100: Layer is at the top.
        /// </summary>
        ///
        public int zOrder;

        ///
        /// <summary>
        /// Transparency of the video participating in local composition. Value range: [0.0,1.0]. 0.0 means fully transparent, 1.0 means fully opaque.
        /// </summary>
        ///
        public double alpha;

        ///
        /// <summary>
        /// This parameter only takes effect for camera video sources. Whether to mirror the video participating in local composition: true : Mirror the video. false : (Default) Do not mirror the video.
        /// </summary>
        ///
        public bool mirror;

        public TranscodingVideoStream()
        {
            this.sourceType = VIDEO_SOURCE_TYPE.VIDEO_SOURCE_CAMERA_PRIMARY;
            this.remoteUserUid = 0;
            this.imageUrl = "";
            this.x = 0;
            this.y = 0;
            this.width = 0;
            this.height = 0;
            this.zOrder = 0;
            this.alpha = 1.0;
            this.mirror = false;
        }

        public TranscodingVideoStream(VIDEO_SOURCE_TYPE sourceType, uint remoteUserUid, string imageUrl, int mediaPlayerId, int x, int y, int width, int height, int zOrder, double alpha, bool mirror)
        {
            this.sourceType = sourceType;
            this.remoteUserUid = remoteUserUid;
            this.imageUrl = imageUrl;
            this.mediaPlayerId = mediaPlayerId;
            this.x = x;
            this.y = y;
            this.width = width;
            this.height = height;
            this.zOrder = zOrder;
            this.alpha = alpha;
            this.mirror = mirror;
        }
    }

    ///
    /// <summary>
    /// Configuration for local video mixing.
    /// </summary>
    ///
    public class LocalTranscoderConfiguration
    {
        ///
        /// <summary>
        /// Number of video streams participating in local video mixing.
        /// </summary>
        ///
        public uint streamCount;

        ///
        /// <summary>
        /// Video streams participating in local video mixing. See TranscodingVideoStream.
        /// </summary>
        ///
        public TranscodingVideoStream[] videoInputStreams;

        ///
        /// <summary>
        /// Encoding configuration for the mixed video after local mixing. See VideoEncoderConfiguration.
        /// </summary>
        ///
        public VideoEncoderConfiguration videoOutputConfiguration;

        ///
        /// @ignore
        ///
        public bool syncWithPrimaryCamera;

        public LocalTranscoderConfiguration()
        {
            this.streamCount = 0;
            this.videoInputStreams = new TranscodingVideoStream[0];
            this.videoOutputConfiguration = new VideoEncoderConfiguration();
            this.syncWithPrimaryCamera = true;
        }

        public LocalTranscoderConfiguration(uint streamCount, TranscodingVideoStream[] videoInputStreams, VideoEncoderConfiguration videoOutputConfiguration, bool syncWithPrimaryCamera)
        {
            this.streamCount = streamCount;
            this.videoInputStreams = videoInputStreams;
            this.videoOutputConfiguration = videoOutputConfiguration;
            this.syncWithPrimaryCamera = syncWithPrimaryCamera;
        }
    }

    ///
    /// <summary>
    /// Local video composition error codes.
    /// </summary>
    ///
    public enum VIDEO_TRANSCODER_ERROR
    {
        ///
        /// <summary>
        /// 1: The specified video source has not started video capture. You need to create a video track for it and start video capture.
        /// </summary>
        ///
        VT_ERR_VIDEO_SOURCE_NOT_READY = 1,

        ///
        /// <summary>
        /// 2: Invalid video source type. You need to specify a supported video source type again.
        /// </summary>
        ///
        VT_ERR_INVALID_VIDEO_SOURCE_TYPE = 2,

        ///
        /// <summary>
        /// 3: Invalid image path. You need to specify the correct image path again.
        /// </summary>
        ///
        VT_ERR_INVALID_IMAGE_PATH = 3,

        ///
        /// <summary>
        /// 4: Unsupported image format. Make sure the image format is one of PNG, JPEG, or GIF.
        /// </summary>
        ///
        VT_ERR_UNSUPPORT_IMAGE_FORMAT = 4,

        ///
        /// <summary>
        /// 5: Invalid video encoding resolution after composition.
        /// </summary>
        ///
        VT_ERR_INVALID_LAYOUT = 5,

        ///
        /// <summary>
        /// 20: Internal unknown error.
        /// </summary>
        ///
        VT_ERR_INTERNAL = 20,

    }

    ///
    /// @ignore
    ///
    public class MixedAudioStream
    {
        ///
        /// @ignore
        ///
        public AUDIO_SOURCE_TYPE sourceType;

        ///
        /// @ignore
        ///
        public uint remoteUserUid;

        ///
        /// @ignore
        ///
        public string channelId;

        ///
        /// @ignore
        ///
        public uint trackId;

        public MixedAudioStream(AUDIO_SOURCE_TYPE source)
        {
            this.sourceType = source;
            this.remoteUserUid = 0;
            this.channelId = "";
            this.trackId = 0xffffffff;
        }

        public MixedAudioStream(AUDIO_SOURCE_TYPE source, uint track)
        {
            this.sourceType = source;
            this.trackId = track;
        }

        public MixedAudioStream(AUDIO_SOURCE_TYPE source, uint uid, string channel)
        {
            this.sourceType = source;
            this.remoteUserUid = uid;
            this.channelId = channel;
        }

        public MixedAudioStream(AUDIO_SOURCE_TYPE source, uint uid, string channel, uint track)
        {
            this.sourceType = source;
            this.remoteUserUid = uid;
            this.channelId = channel;
            this.trackId = track;
        }

        public MixedAudioStream()
        {
        }

    }

    ///
    /// <summary>
    /// Local audio mixing configuration.
    /// </summary>
    ///
    public class LocalAudioMixerConfiguration
    {
        ///
        /// <summary>
        /// Number of audio streams to be mixed locally.
        /// </summary>
        ///
        public uint streamCount;

        ///
        /// <summary>
        /// Audio sources to be mixed locally. See MixedAudioStream.
        /// </summary>
        ///
        public MixedAudioStream[] audioInputStreams;

        ///
        /// <summary>
        /// Whether the mixed audio stream uses the timestamp of audio frames captured by the local microphone: true : (default) Uses the timestamp of audio frames captured by the local microphone. Set this value if you want all locally captured audio streams to stay synchronized. false : Does not use the timestamp of audio frames captured by the local microphone. The SDK uses the timestamp when the mixed audio frame is constructed.
        /// </summary>
        ///
        public bool syncWithLocalMic;

        public LocalAudioMixerConfiguration()
        {
            this.streamCount = 0;
            this.syncWithLocalMic = true;
        }

        public LocalAudioMixerConfiguration(uint streamCount, MixedAudioStream[] audioInputStreams, bool syncWithLocalMic)
        {
            this.streamCount = streamCount;
            this.audioInputStreams = audioInputStreams;
            this.syncWithLocalMic = syncWithLocalMic;
        }
    }

    ///
    /// <summary>
    /// Last mile network probe configuration.
    /// </summary>
    ///
    public class LastmileProbeConfig
    {
        ///
        /// <summary>
        /// Whether to probe the uplink network. Some users, such as audience members in a live broadcast channel, do not need network probing: true : Probe the uplink network. false : Do not probe the uplink network.
        /// </summary>
        ///
        public bool probeUplink;

        ///
        /// <summary>
        /// Whether to probe the downlink network: true : Probe the downlink network. false : Do not probe the downlink network.
        /// </summary>
        ///
        public bool probeDownlink;

        ///
        /// <summary>
        /// The expected maximum uplink bitrate in bps, ranging from [100000, 5000000]. It is recommended to refer to the bitrate values in SetVideoEncoderConfiguration when setting this parameter.
        /// </summary>
        ///
        public uint expectedUplinkBitrate;

        ///
        /// <summary>
        /// The expected maximum downlink bitrate in bps, ranging from [100000, 5000000].
        /// </summary>
        ///
        public uint expectedDownlinkBitrate;

        public LastmileProbeConfig(bool probeUplink, bool probeDownlink, uint expectedUplinkBitrate, uint expectedDownlinkBitrate)
        {
            this.probeUplink = probeUplink;
            this.probeDownlink = probeDownlink;
            this.expectedUplinkBitrate = expectedUplinkBitrate;
            this.expectedDownlinkBitrate = expectedDownlinkBitrate;
        }
        public LastmileProbeConfig()
        {
        }

    }

    ///
    /// <summary>
    /// Status of the last mile probe result.
    /// </summary>
    ///
    public enum LASTMILE_PROBE_RESULT_STATE
    {
        ///
        /// <summary>
        /// 1: Indicates the result of the last mile probe is complete.
        /// </summary>
        ///
        LASTMILE_PROBE_RESULT_COMPLETE = 1,

        ///
        /// <summary>
        /// 2: Indicates the last mile probe did not perform bandwidth estimation, so the result is incomplete. One possible reason is temporarily limited testing resources.
        /// </summary>
        ///
        LASTMILE_PROBE_RESULT_INCOMPLETE_NO_BWE = 2,

        ///
        /// <summary>
        /// 3: Last mile probe was not performed. One possible reason is network disconnection.
        /// </summary>
        ///
        LASTMILE_PROBE_RESULT_UNAVAILABLE = 3,

    }

    ///
    /// <summary>
    /// One-way (uplink or downlink) last mile network quality probe result.
    /// </summary>
    ///
    public class LastmileProbeOneWayResult
    {
        ///
        /// <summary>
        /// Packet loss rate.
        /// </summary>
        ///
        public uint packetLossRate;

        ///
        /// <summary>
        /// Network jitter (ms).
        /// </summary>
        ///
        public uint jitter;

        ///
        /// <summary>
        /// Estimated available network bandwidth (bps).
        /// </summary>
        ///
        public uint availableBandwidth;

        public LastmileProbeOneWayResult()
        {
            this.packetLossRate = 0;
            this.jitter = 0;
            this.availableBandwidth = 0;
        }

        public LastmileProbeOneWayResult(uint packetLossRate, uint jitter, uint availableBandwidth)
        {
            this.packetLossRate = packetLossRate;
            this.jitter = jitter;
            this.availableBandwidth = availableBandwidth;
        }
    }

    ///
    /// <summary>
    /// Uplink and downlink last mile network quality probe result.
    /// </summary>
    ///
    public class LastmileProbeResult
    {
        ///
        /// <summary>
        /// The state of the last mile probe result. See: LASTMILE_PROBE_RESULT_STATE.
        /// </summary>
        ///
        public LASTMILE_PROBE_RESULT_STATE state;

        ///
        /// <summary>
        /// Uplink network quality report. See LastmileProbeOneWayResult.
        /// </summary>
        ///
        public LastmileProbeOneWayResult uplinkReport;

        ///
        /// <summary>
        /// Downlink network quality report. See LastmileProbeOneWayResult.
        /// </summary>
        ///
        public LastmileProbeOneWayResult downlinkReport;

        ///
        /// <summary>
        /// Round-trip time (ms).
        /// </summary>
        ///
        public uint rtt;

        public LastmileProbeResult()
        {
            this.state = LASTMILE_PROBE_RESULT_STATE.LASTMILE_PROBE_RESULT_UNAVAILABLE;
            this.rtt = 0;
        }

        public LastmileProbeResult(LASTMILE_PROBE_RESULT_STATE state, LastmileProbeOneWayResult uplinkReport, LastmileProbeOneWayResult downlinkReport, uint rtt)
        {
            this.state = state;
            this.uplinkReport = uplinkReport;
            this.downlinkReport = downlinkReport;
            this.rtt = rtt;
        }
    }

    ///
    /// <summary>
    /// Reason for network connection state change.
    /// </summary>
    ///
    public enum CONNECTION_CHANGED_REASON_TYPE
    {
        ///
        /// <summary>
        /// 0: Establishing network connection.
        /// </summary>
        ///
        CONNECTION_CHANGED_CONNECTING = 0,

        ///
        /// <summary>
        /// 1: Successfully joined the channel.
        /// </summary>
        ///
        CONNECTION_CHANGED_JOIN_SUCCESS = 1,

        ///
        /// <summary>
        /// 2: Network connection interrupted.
        /// </summary>
        ///
        CONNECTION_CHANGED_INTERRUPTED = 2,

        ///
        /// <summary>
        /// 3: Network connection is banned by the server. For example, this status is returned when the user is kicked out of the channel.
        /// </summary>
        ///
        CONNECTION_CHANGED_BANNED_BY_SERVER = 3,

        ///
        /// <summary>
        /// 4: Failed to join the channel. If the SDK fails to join the channel after trying for 20 minutes, this status is returned and it stops attempting to reconnect. Prompt the user to switch networks and try joining the channel again.
        /// </summary>
        ///
        CONNECTION_CHANGED_JOIN_FAILED = 4,

        ///
        /// <summary>
        /// 5: Left the channel.
        /// </summary>
        ///
        CONNECTION_CHANGED_LEAVE_CHANNEL = 5,

        ///
        /// <summary>
        /// 6: Invalid App ID. Use a valid App ID to rejoin the channel and ensure the App ID matches the one generated in the Agora Console.
        /// </summary>
        ///
        CONNECTION_CHANGED_INVALID_APP_ID = 6,

        ///
        /// <summary>
        /// 7: Invalid channel name. Use a valid channel name to rejoin the channel. A valid channel name is a string within 64 bytes. The supported character set includes 89 characters:
        /// </summary>
        ///
        CONNECTION_CHANGED_INVALID_CHANNEL_NAME = 7,

        ///
        /// <summary>
        /// 8: Invalid Token. Possible reasons:
        ///  Your project has App Certificate enabled but you joined the channel without using a Token.
        ///  The user ID specified in JoinChannel [2/2] does not match the one used to generate the Token.
        ///  The generated Token does not match the one used to join the channel. Ensure that:
        ///  When App Certificate is enabled, use Token to join the channel.
        ///  The user ID used to generate the Token matches the one used to join the channel.
        ///  The generated Token matches the one used to join the channel.
        /// </summary>
        ///
        CONNECTION_CHANGED_INVALID_TOKEN = 8,

        ///
        /// <summary>
        /// 9: The current Token has expired. Generate a new Token on your server and use it to rejoin the channel.
        /// </summary>
        ///
        CONNECTION_CHANGED_TOKEN_EXPIRED = 9,

        ///
        /// <summary>
        /// 10: This user is banned by the server. Possible reasons:
        ///  The user already joined the channel and calls the join channel API again, such as JoinChannel [2/2]. Stop calling this method.
        ///  The user tries to join the channel during a call test. Wait until the test ends before joining the channel.
        /// </summary>
        ///
        CONNECTION_CHANGED_REJECTED_BY_SERVER = 10,

        ///
        /// <summary>
        /// 11: SDK attempts to reconnect due to proxy server settings.
        /// </summary>
        ///
        CONNECTION_CHANGED_SETTING_PROXY_SERVER = 11,

        ///
        /// <summary>
        /// 12: Network connection status changed due to Token renewal.
        /// </summary>
        ///
        CONNECTION_CHANGED_RENEW_TOKEN = 12,

        ///
        /// <summary>
        /// 13: Client IP address changed. If this status is received multiple times, prompt the user to switch networks and try joining the channel again.
        /// </summary>
        ///
        CONNECTION_CHANGED_CLIENT_IP_ADDRESS_CHANGED = 13,

        ///
        /// <summary>
        /// 14: Keep-alive timeout between SDK and server, entering auto-reconnect state.
        /// </summary>
        ///
        CONNECTION_CHANGED_KEEP_ALIVE_TIMEOUT = 14,

        ///
        /// <summary>
        /// 15: Successfully rejoined the channel.
        /// </summary>
        ///
        CONNECTION_CHANGED_REJOIN_SUCCESS = 15,

        ///
        /// <summary>
        /// 16: SDK lost connection with the server.
        /// </summary>
        ///
        CONNECTION_CHANGED_LOST = 16,

        ///
        /// <summary>
        /// 17: Connection state changed due to echo test.
        /// </summary>
        ///
        CONNECTION_CHANGED_ECHO_TEST = 17,

        ///
        /// <summary>
        /// 18: Local IP address changed by user.
        /// </summary>
        ///
        CONNECTION_CHANGED_CLIENT_IP_ADDRESS_CHANGED_BY_USER = 18,

        ///
        /// <summary>
        /// 19: The same UID joined the same channel from a different device.
        /// </summary>
        ///
        CONNECTION_CHANGED_SAME_UID_LOGIN = 19,

        ///
        /// <summary>
        /// 20: The number of broadcasters in the channel has reached the limit.
        /// </summary>
        ///
        CONNECTION_CHANGED_TOO_MANY_BROADCASTERS = 20,

        ///
        /// @ignore
        ///
        CONNECTION_CHANGED_LICENSE_VALIDATION_FAILURE = 21,

        ///
        /// @ignore
        ///
        CONNECTION_CHANGED_CERTIFICATION_VERYFY_FAILURE = 22,

        ///
        /// @ignore
        ///
        CONNECTION_CHANGED_STREAM_CHANNEL_NOT_AVAILABLE = 23,

        ///
        /// @ignore
        ///
        CONNECTION_CHANGED_INCONSISTENT_APPID = 24,

    }

    ///
    /// <summary>
    /// Reasons for user role switch failure.
    /// </summary>
    ///
    public enum CLIENT_ROLE_CHANGE_FAILED_REASON
    {
        ///
        /// <summary>
        /// 1: The number of broadcasters in the channel has reached the limit. This enum is only reported when the 128-user feature is enabled. The limit depends on the actual configuration when enabling the feature.
        /// </summary>
        ///
        CLIENT_ROLE_CHANGE_FAILED_TOO_MANY_BROADCASTERS = 1,

        ///
        /// <summary>
        /// 2: Request was rejected by the server. It is recommended to prompt the user to try switching roles again.
        /// </summary>
        ///
        CLIENT_ROLE_CHANGE_FAILED_NOT_AUTHORIZED = 2,

        ///
        /// <summary>
        /// 3: Request timed out. It is recommended to prompt the user to check the network connection and try switching roles again. Deprecated: This enum value is deprecated since v4.4.0 and is not recommended for use.
        /// </summary>
        ///
        [Obsolete("This reason is deprecated.")]
        CLIENT_ROLE_CHANGE_FAILED_REQUEST_TIME_OUT = 3,

        ///
        /// <summary>
        /// 4: Network connection lost. You can troubleshoot the specific reason based on the reason reported by OnConnectionStateChanged. Deprecated: This enum value is deprecated since v4.4.0 and is not recommended for use.
        /// </summary>
        ///
        [Obsolete("This reason is deprecated.")]
        CLIENT_ROLE_CHANGE_FAILED_CONNECTION_FAILED = 4,

    }

    ///
    /// <summary>
    /// Network connection type.
    /// </summary>
    ///
    public enum NETWORK_TYPE
    {
        ///
        /// <summary>
        /// -1: Unknown network connection type.
        /// </summary>
        ///
        NETWORK_TYPE_UNKNOWN = -1,

        ///
        /// <summary>
        /// 0: Network connection is disconnected.
        /// </summary>
        ///
        NETWORK_TYPE_DISCONNECTED = 0,

        ///
        /// <summary>
        /// 1: Network type is LAN.
        /// </summary>
        ///
        NETWORK_TYPE_LAN = 1,

        ///
        /// <summary>
        /// 2: Network type is Wi-Fi (including hotspot).
        /// </summary>
        ///
        NETWORK_TYPE_WIFI = 2,

        ///
        /// <summary>
        /// 3: Network type is 2G mobile network.
        /// </summary>
        ///
        NETWORK_TYPE_MOBILE_2G = 3,

        ///
        /// <summary>
        /// 4: Network type is 3G mobile network.
        /// </summary>
        ///
        NETWORK_TYPE_MOBILE_3G = 4,

        ///
        /// <summary>
        /// 5: Network type is 4G mobile network.
        /// </summary>
        ///
        NETWORK_TYPE_MOBILE_4G = 5,

        ///
        /// <summary>
        /// 6: Network type is 5G mobile network.
        /// </summary>
        ///
        NETWORK_TYPE_MOBILE_5G = 6,

    }

    ///
    /// <summary>
    /// View setup mode.
    /// </summary>
    ///
    public enum VIDEO_VIEW_SETUP_MODE
    {
        ///
        /// <summary>
        /// 0: (Default) Clears all added views and replaces them with a new view.
        /// </summary>
        ///
        VIDEO_VIEW_SETUP_REPLACE = 0,

        ///
        /// <summary>
        /// 1: Adds a view.
        /// </summary>
        ///
        VIDEO_VIEW_SETUP_ADD = 1,

        ///
        /// <summary>
        /// 2: Removes a view. When you no longer need a view, it is recommended to set setupMode to VIDEO_VIEW_SETUP_REMOVE in time to remove the view, otherwise it may cause rendering resource leaks.
        /// </summary>
        ///
        VIDEO_VIEW_SETUP_REMOVE = 2,

    }

    ///
    /// <summary>
    /// Properties of the video canvas object.
    /// </summary>
    ///
    public class VideoCanvas
    {
        ///
        /// <summary>
        /// For Android and iOS platforms, when the video source is a composite video stream (VIDEO_SOURCE_TRANSCODED), this parameter represents the user ID that publishes the composite video stream.
        /// </summary>
        ///
        public uint uid;

        ///
        /// <summary>
        /// The user ID that publishes a specific sub-video stream of the composite stream.
        /// </summary>
        ///
        public uint subviewUid;

        ///
        /// <summary>
        /// The video display window. In a VideoCanvas, you can only set either view or surfaceTexture. If both are set, only the configuration in view takes effect.
        /// </summary>
        ///
        public view_t view;

        ///
        /// <summary>
        /// Background color of the video canvas in RGBA format. The default value is 0x00000000, which represents black.
        /// </summary>
        ///
        public uint backgroundColor;

        ///
        /// <summary>
        /// Video rendering mode. See RENDER_MODE_TYPE.
        /// </summary>
        ///
        public RENDER_MODE_TYPE renderMode;

        ///
        /// <summary>
        /// View mirroring mode. See VIDEO_MIRROR_MODE_TYPE.
        ///  Local view mirroring mode: If you use the front camera, local view mirroring is enabled by default; if you use the rear camera, it is disabled by default.
        ///  Remote user view mirroring mode: Mirroring is disabled by default for remote users.
        /// </summary>
        ///
        public VIDEO_MIRROR_MODE_TYPE mirrorMode;

        ///
        /// <summary>
        /// View setup mode. See VIDEO_VIEW_SETUP_MODE.
        /// </summary>
        ///
        public VIDEO_VIEW_SETUP_MODE setupMode;

        ///
        /// <summary>
        /// Type of video source. See VIDEO_SOURCE_TYPE.
        /// </summary>
        ///
        public VIDEO_SOURCE_TYPE sourceType;

        ///
        /// <summary>
        /// Media player ID. You can obtain it via GetId.
        /// </summary>
        ///
        public int mediaPlayerId;

        ///
        /// <summary>
        /// (Optional) Display area of the video frame. See Rectangle. width and height indicate the pixel width and height of the area. The default is null (width or height is 0), which means the actual resolution of the video frame is displayed.
        /// </summary>
        ///
        public Rectangle cropArea;

        ///
        /// <summary>
        /// The receiver can render alpha channel information only when the sender enables the alpha transmission feature.
        ///  To enable the alpha transmission feature, please [contact technical support](https://ticket.shengwang.cn/). (Optional) Whether to enable alpha mask rendering: true : Enable alpha mask rendering. false : (Default) Disable alpha mask rendering. Alpha mask rendering can create transparent images and extract portraits from videos. When used with other methods, it enables effects like portrait picture-in-picture and watermarking.
        /// </summary>
        ///
        public bool enableAlphaMask;

        ///
        /// <summary>
        /// The position of the video frame in the video pipeline. See VIDEO_MODULE_POSITION.
        /// </summary>
        ///
        public VIDEO_MODULE_POSITION position;

        public VideoCanvas()
        {
            this.uid = 0;
            this.subviewUid = 0;
            this.view = 0;
            this.backgroundColor = 0x00000000;
            this.renderMode = RENDER_MODE_TYPE.RENDER_MODE_HIDDEN;
            this.mirrorMode = VIDEO_MIRROR_MODE_TYPE.VIDEO_MIRROR_MODE_AUTO;
            this.setupMode = VIDEO_VIEW_SETUP_MODE.VIDEO_VIEW_SETUP_REPLACE;
            this.sourceType = VIDEO_SOURCE_TYPE.VIDEO_SOURCE_CAMERA_PRIMARY;
            this.mediaPlayerId = -(int)ERROR_CODE_TYPE.ERR_NOT_READY;
            this.cropArea = new Rectangle(0, 0, 0, 0);
            this.enableAlphaMask = false;
            this.position = VIDEO_MODULE_POSITION.POSITION_POST_CAPTURER;
        }

        public VideoCanvas(view_t v, RENDER_MODE_TYPE m, VIDEO_MIRROR_MODE_TYPE mt)
        {
            this.uid = 0;
            this.subviewUid = 0;
            this.view = v;
            this.backgroundColor = 0x00000000;
            this.renderMode = m;
            this.mirrorMode = mt;
            this.setupMode = VIDEO_VIEW_SETUP_MODE.VIDEO_VIEW_SETUP_REPLACE;
            this.sourceType = VIDEO_SOURCE_TYPE.VIDEO_SOURCE_CAMERA_PRIMARY;
            this.mediaPlayerId = -(int)ERROR_CODE_TYPE.ERR_NOT_READY;
            this.cropArea = new Rectangle(0, 0, 0, 0);
            this.enableAlphaMask = false;
            this.position = VIDEO_MODULE_POSITION.POSITION_POST_CAPTURER;
        }

        public VideoCanvas(view_t v, RENDER_MODE_TYPE m, VIDEO_MIRROR_MODE_TYPE mt, uint u)
        {
            this.uid = u;
            this.subviewUid = 0;
            this.view = v;
            this.backgroundColor = 0x00000000;
            this.renderMode = m;
            this.mirrorMode = mt;
            this.setupMode = VIDEO_VIEW_SETUP_MODE.VIDEO_VIEW_SETUP_REPLACE;
            this.sourceType = VIDEO_SOURCE_TYPE.VIDEO_SOURCE_CAMERA_PRIMARY;
            this.mediaPlayerId = -(int)ERROR_CODE_TYPE.ERR_NOT_READY;
            this.cropArea = new Rectangle(0, 0, 0, 0);
            this.enableAlphaMask = false;
            this.position = VIDEO_MODULE_POSITION.POSITION_POST_CAPTURER;
        }

        public VideoCanvas(view_t v, RENDER_MODE_TYPE m, VIDEO_MIRROR_MODE_TYPE mt, uint u, uint subu)
        {
            this.uid = u;
            this.subviewUid = subu;
            this.view = v;
            this.backgroundColor = 0x00000000;
            this.renderMode = m;
            this.mirrorMode = mt;
            this.setupMode = VIDEO_VIEW_SETUP_MODE.VIDEO_VIEW_SETUP_REPLACE;
            this.sourceType = VIDEO_SOURCE_TYPE.VIDEO_SOURCE_CAMERA_PRIMARY;
            this.mediaPlayerId = -(int)ERROR_CODE_TYPE.ERR_NOT_READY;
            this.cropArea = new Rectangle(0, 0, 0, 0);
            this.enableAlphaMask = false;
            this.position = VIDEO_MODULE_POSITION.POSITION_POST_CAPTURER;
        }

        public VideoCanvas(uint uid, uint subviewUid, view_t view, uint backgroundColor, RENDER_MODE_TYPE renderMode, VIDEO_MIRROR_MODE_TYPE mirrorMode, VIDEO_VIEW_SETUP_MODE setupMode, VIDEO_SOURCE_TYPE sourceType, int mediaPlayerId, Rectangle cropArea, bool enableAlphaMask, VIDEO_MODULE_POSITION position)
        {
            this.uid = uid;
            this.subviewUid = subviewUid;
            this.view = view;
            this.backgroundColor = backgroundColor;
            this.renderMode = renderMode;
            this.mirrorMode = mirrorMode;
            this.setupMode = setupMode;
            this.sourceType = sourceType;
            this.mediaPlayerId = mediaPlayerId;
            this.cropArea = cropArea;
            this.enableAlphaMask = enableAlphaMask;
            this.position = position;
        }
    }

    ///
    /// <summary>
    /// Beauty options.
    /// </summary>
    ///
    public class BeautyOptions
    {
        ///
        /// <summary>
        /// Contrast level, usually used with lighteningLevel. The larger the value, the greater the contrast between light and dark. See LIGHTENING_CONTRAST_LEVEL.
        /// </summary>
        ///
        public LIGHTENING_CONTRAST_LEVEL lighteningContrastLevel;

        ///
        /// <summary>
        /// Whitening level, ranging from [0.0, 1.0], where 0.0 means original brightness. Default is 0.0. The larger the value, the greater the whitening effect.
        /// </summary>
        ///
        public float lighteningLevel;

        ///
        /// <summary>
        /// Smoothing level, ranging from [0.0, 1.0], where 0.0 means original smoothness. Default is 0.0. The larger the value, the greater the smoothing effect.
        /// </summary>
        ///
        public float smoothnessLevel;

        ///
        /// <summary>
        /// Redness level, ranging from [0.0, 1.0], where 0.0 means original redness. Default is 0.0. The larger the value, the more rosy the effect.
        /// </summary>
        ///
        public float rednessLevel;

        ///
        /// <summary>
        /// Sharpening level, ranging from [0.0, 1.0], where 0.0 means original sharpness. Default is 0.0. The larger the value, the sharper the image.
        /// </summary>
        ///
        public float sharpnessLevel;

        public BeautyOptions(LIGHTENING_CONTRAST_LEVEL contrastLevel, float lightening, float smoothness, float redness, float sharpness)
        {
            this.lighteningContrastLevel = contrastLevel;
            this.lighteningLevel = lightening;
            this.smoothnessLevel = smoothness;
            this.rednessLevel = redness;
            this.sharpnessLevel = sharpness;
        }

        public BeautyOptions()
        {
            this.lighteningContrastLevel = LIGHTENING_CONTRAST_LEVEL.LIGHTENING_CONTRAST_NORMAL;
            this.lighteningLevel = 0;
            this.smoothnessLevel = 0;
            this.rednessLevel = 0;
            this.sharpnessLevel = 0;
        }

    }

    ///
    /// <summary>
    /// Brightness contrast level.
    /// </summary>
    ///
    public enum LIGHTENING_CONTRAST_LEVEL
    {
        ///
        /// <summary>
        /// 0: Low contrast.
        /// </summary>
        ///
        LIGHTENING_CONTRAST_LOW = 0,

        ///
        /// <summary>
        /// 1: Normal contrast.
        /// </summary>
        ///
        LIGHTENING_CONTRAST_NORMAL = 1,

        ///
        /// <summary>
        /// 2: High contrast.
        /// </summary>
        ///
        LIGHTENING_CONTRAST_HIGH = 2,

    }

    ///
    /// <summary>
    /// Filter effect options.
    /// </summary>
    ///
    public class FaceShapeAreaOptions
    {
        ///
        /// <summary>
        /// Facial area. See FACE_SHAPE_AREA.
        /// </summary>
        ///
        public FACE_SHAPE_AREA shapeArea;

        ///
        /// <summary>
        /// Intensity of the modification. Definitions vary by area (including direction, range, preset values, etc.). See FACE_SHAPE_AREA.
        /// </summary>
        ///
        public int shapeIntensity;

        public FaceShapeAreaOptions(FACE_SHAPE_AREA shapeArea, int areaIntensity)
        {
            this.shapeArea = shapeArea;
            this.shapeIntensity = areaIntensity;
        }

        public FaceShapeAreaOptions()
        {
            this.shapeArea = FACE_SHAPE_AREA.FACE_SHAPE_AREA_NONE;
            this.shapeIntensity = 0;
        }

    }

    ///
    /// <summary>
    /// Selects the specific facial area to be adjusted.
    /// 
    /// Since Available since v4.4.0.
    /// </summary>
    ///
    public enum FACE_SHAPE_AREA
    {
        ///
        /// <summary>
        /// (-1): Default value, indicates an invalid area, facial shaping effect is not applied.
        /// </summary>
        ///
        FACE_SHAPE_AREA_NONE = -1,

        ///
        /// <summary>
        /// (100): Head area, used to achieve a smaller head effect. Value range is [0, 100], default is 50. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_HEADSCALE = 100,

        ///
        /// <summary>
        /// (101): Forehead area, used to adjust the hairline height. Value range is [0, 100], default is 0. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_FOREHEAD = 101,

        ///
        /// <summary>
        /// (102): Face contour area, used to achieve a slimming face effect. Value range is [0, 100], default is 0. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_FACECONTOUR = 102,

        ///
        /// <summary>
        /// (103): Face length area, used to elongate the face. Value range is [-100, 100], default is 0. The greater the absolute value, the more obvious the adjustment. Negative values indicate the opposite direction.
        /// </summary>
        ///
        FACE_SHAPE_AREA_FACELENGTH = 103,

        ///
        /// <summary>
        /// (104): Face width area, used to achieve a narrower face effect. Value range is [0, 100], default is 0. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_FACEWIDTH = 104,

        ///
        /// <summary>
        /// (105): Cheekbone area, used to adjust cheekbone width. Value range is [0, 100], default is 0. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_CHEEKBONE = 105,

        ///
        /// <summary>
        /// (106): Cheek area, used to adjust cheek width. Value range is [0, 100], default is 0. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_CHEEK = 106,

        ///
        /// <summary>
        /// (107): Mandible area, used to adjust mandible width. Value range is [0, 100], default is 0. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_MANDIBLE = 107,

        ///
        /// <summary>
        /// (108): Chin area, used to adjust chin length. Value range is [-100, 100], default is 0. The greater the absolute value, the more obvious the adjustment. Negative values indicate the opposite direction.
        /// </summary>
        ///
        FACE_SHAPE_AREA_CHIN = 108,

        ///
        /// <summary>
        /// (200): Eye area, used to achieve a big eye effect. Value range is [0, 100], default is 50. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_EYESCALE = 200,

        ///
        /// <summary>
        /// (201): Eye distance area, used to adjust the distance between the eyes. Value range is [-100, 100], default is 0. The greater the absolute value, the more obvious the adjustment. Negative values indicate the opposite direction.
        /// </summary>
        ///
        FACE_SHAPE_AREA_EYEDISTANCE = 201,

        ///
        /// <summary>
        /// (202): Eye position area, used to adjust the overall eye position. Value range is [-100, 100], default is 0. The greater the absolute value, the more obvious the adjustment. Negative values indicate the opposite direction.
        /// </summary>
        ///
        FACE_SHAPE_AREA_EYEPOSITION = 202,

        ///
        /// <summary>
        /// (203): Lower eyelid area, used to adjust the shape of the lower eyelid. Value range is [0, 100], default is 0. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_LOWEREYELID = 203,

        ///
        /// <summary>
        /// (204): Pupil area, used to adjust pupil size. Value range is [0, 100], default is 0. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_EYEPUPILS = 204,

        ///
        /// <summary>
        /// (205): Inner eye corner area, used to adjust the shape of the inner eye corner. Value range is [-100, 100], default is 0. The greater the absolute value, the more obvious the adjustment. Negative values indicate the opposite direction.
        /// </summary>
        ///
        FACE_SHAPE_AREA_EYEINNERCORNER = 205,

        ///
        /// <summary>
        /// (206): Outer eye corner area, used to adjust the shape of the outer eye corner. Value range is [-100, 100], default is 0. The greater the absolute value, the more obvious the adjustment. Negative values indicate the opposite direction.
        /// </summary>
        ///
        FACE_SHAPE_AREA_EYEOUTERCORNER = 206,

        ///
        /// <summary>
        /// (300): Nose length area, used to elongate the nose. Value range is [-100, 100], default is 0. The greater the absolute value, the more obvious the adjustment. Negative values indicate the opposite direction.
        /// </summary>
        ///
        FACE_SHAPE_AREA_NOSELENGTH = 300,

        ///
        /// <summary>
        /// (301): Nose width area, used to achieve a slimmer nose effect. Value range is [0, 100], default is 0. The larger the value, the more obvious the slimming effect.
        /// </summary>
        ///
        FACE_SHAPE_AREA_NOSEWIDTH = 301,

        ///
        /// <summary>
        /// (302): Nose wing area, used to adjust the width of the nose wings. Value range is [0, 100], default is 10. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_NOSEWING = 302,

        ///
        /// <summary>
        /// (303): Nose root area, used to adjust the height of the nose root. Value range is [0, 100], default is 0. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_NOSEROOT = 303,

        ///
        /// <summary>
        /// (304): Nose bridge area, used to adjust the height of the nose bridge. Value range is [0, 100], default is 50. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_NOSEBRIDGE = 304,

        ///
        /// <summary>
        /// (305): Nose tip area, used to adjust the shape of the nose tip. Value range is [0, 100], default is 50. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_NOSETIP = 305,

        ///
        /// <summary>
        /// (306): Overall nose area, used to uniformly adjust nose shape. Value range is [-100, 100], default is 50. The greater the absolute value, the more obvious the adjustment. Negative values indicate the opposite direction.
        /// </summary>
        ///
        FACE_SHAPE_AREA_NOSEGENERAL = 306,

        ///
        /// <summary>
        /// (400): Mouth area, used to achieve a larger mouth effect. Value range is [-100, 100], default is 20. The greater the absolute value, the more obvious the adjustment. Negative values indicate the opposite direction.
        /// </summary>
        ///
        FACE_SHAPE_AREA_MOUTHSCALE = 400,

        ///
        /// <summary>
        /// (401): Mouth position area, used to adjust the overall mouth position. Value range is [0, 100], default is 0. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_MOUTHPOSITION = 401,

        ///
        /// <summary>
        /// (402): Mouth smile area, used to adjust the upward curve of the mouth corners. Value range is [0, 1], default is 0. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_MOUTHSMILE = 402,

        ///
        /// <summary>
        /// (403): Lip shape area, used to adjust the shape of the lips. Value range is [0, 100], default is 0. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_MOUTHLIP = 403,

        ///
        /// <summary>
        /// (500): Eyebrow position area, used to adjust the overall position of the eyebrows. Value range is [-100, 100], default is 0. The greater the absolute value, the more obvious the adjustment. Negative values indicate the opposite direction.
        /// </summary>
        ///
        FACE_SHAPE_AREA_EYEBROWPOSITION = 500,

        ///
        /// <summary>
        /// (501): Eyebrow thickness area, used to adjust the thickness of the eyebrows. Value range is [-100, 100], default is 0. The larger the value, the more obvious the adjustment.
        /// </summary>
        ///
        FACE_SHAPE_AREA_EYEBROWTHICKNESS = 501,

    }

    ///
    /// <summary>
    /// Beauty style options.
    /// </summary>
    ///
    public class FaceShapeBeautyOptions
    {
        ///
        /// <summary>
        /// Beauty style. See FACE_SHAPE_BEAUTY_STYLE.
        /// </summary>
        ///
        public FACE_SHAPE_BEAUTY_STYLE shapeStyle;

        ///
        /// <summary>
        /// Beauty style intensity. Range: [0,100]. Default is 0.0, meaning no effect. Higher values result in more noticeable changes.
        /// </summary>
        ///
        public int styleIntensity;

        public FaceShapeBeautyOptions(FACE_SHAPE_BEAUTY_STYLE shapeStyle, int styleIntensity)
        {
            this.shapeStyle = shapeStyle;
            this.styleIntensity = styleIntensity;
        }

        public FaceShapeBeautyOptions()
        {
            this.shapeStyle = FACE_SHAPE_BEAUTY_STYLE.FACE_SHAPE_BEAUTY_STYLE_FEMALE;
            this.styleIntensity = 50;
        }

    }

    ///
    /// <summary>
    /// Face shape beauty style effect options.
    /// 
    /// Since Available since v4.4.0.
    /// </summary>
    ///
    public enum FACE_SHAPE_BEAUTY_STYLE
    {
        ///
        /// <summary>
        /// (0): (Default) Female style beauty effect.
        /// </summary>
        ///
        FACE_SHAPE_BEAUTY_STYLE_FEMALE = 0,

        ///
        /// <summary>
        /// (1): Male style beauty effect.
        /// </summary>
        ///
        FACE_SHAPE_BEAUTY_STYLE_MALE = 1,

        ///
        /// <summary>
        /// (2): Natural style beauty effect, only minimal adjustments to facial features.
        /// </summary>
        ///
        FACE_SHAPE_BEAUTY_STYLE_NATURAL = 2,

    }

    ///
    /// <summary>
    /// Filter effect options.
    /// </summary>
    ///
    public class FilterEffectOptions
    {
        ///
        /// <summary>
        /// The local absolute path to the 3D LUT (Lookup Table) file used to implement custom filter effects. The referenced.cube file must strictly follow the Cube LUT specification; otherwise, the filter effect will not work. Below is an example of a.cube file: LUT_3D_SIZE 32
        /// 0.0039215689 0 0.0039215682
        /// 0.0086021447 0.0037950677 0
        /// ...
        /// 0.0728652592 0.0039215689 0
        ///  The first line of the LUT file must contain the identifier LUT_3D_SIZE, which indicates the size of the 3D lookup table. Currently, only a LUT size of 32 is supported.
        ///  The SDK provides a built-in built_in_whiten_filter.cube file. Passing the absolute path of this file applies a whitening filter effect.
        /// </summary>
        ///
        public string path;

        ///
        /// <summary>
        /// The strength of the filter effect, ranging from [0.0, 1.0], where 0.0 means no filter effect. The default value is 0.5. A higher value results in a stronger filter effect.
        /// </summary>
        ///
        public float strength;

        public FilterEffectOptions(string lut3dPath, float filterStrength)
        {
            this.path = lut3dPath;
            this.strength = filterStrength;
        }

        public FilterEffectOptions()
        {
            this.path = "";
            this.strength = 0.5f;
        }

    }

    ///
    /// <summary>
    /// Low-light enhancement options.
    /// </summary>
    ///
    public class LowlightEnhanceOptions
    {
        ///
        /// <summary>
        /// Low-light enhancement mode. See LOW_LIGHT_ENHANCE_MODE.
        /// </summary>
        ///
        public LOW_LIGHT_ENHANCE_MODE mode;

        ///
        /// <summary>
        /// Low-light enhancement level. See LOW_LIGHT_ENHANCE_LEVEL.
        /// </summary>
        ///
        public LOW_LIGHT_ENHANCE_LEVEL level;

        public LowlightEnhanceOptions(LOW_LIGHT_ENHANCE_MODE lowlightMode, LOW_LIGHT_ENHANCE_LEVEL lowlightLevel)
        {
            this.mode = lowlightMode;
            this.level = lowlightLevel;
        }

        public LowlightEnhanceOptions()
        {
            this.mode = LOW_LIGHT_ENHANCE_MODE.LOW_LIGHT_ENHANCE_AUTO;
            this.level = LOW_LIGHT_ENHANCE_LEVEL.LOW_LIGHT_ENHANCE_LEVEL_HIGH_QUALITY;
        }

    }

    ///
    /// <summary>
    /// Low-light enhancement mode.
    /// </summary>
    ///
    public enum LOW_LIGHT_ENHANCE_MODE
    {
        ///
        /// <summary>
        /// 0: (Default) Auto mode. The SDK automatically enables or disables the low-light enhancement feature based on ambient brightness to provide appropriate lighting and prevent overexposure.
        /// </summary>
        ///
        LOW_LIGHT_ENHANCE_AUTO = 0,

        ///
        /// <summary>
        /// 1: Manual mode. Users need to manually enable or disable the low-light enhancement feature.
        /// </summary>
        ///
        LOW_LIGHT_ENHANCE_MANUAL = 1,

    }

    ///
    /// <summary>
    /// Low-light enhancement level.
    /// </summary>
    ///
    public enum LOW_LIGHT_ENHANCE_LEVEL
    {
        ///
        /// <summary>
        /// 0: (Default) Low-light enhancement prioritizing image quality. It processes brightness, details, and noise in the video image. It has moderate performance consumption and processing speed, offering optimal overall quality.
        /// </summary>
        ///
        LOW_LIGHT_ENHANCE_LEVEL_HIGH_QUALITY = 0,

        ///
        /// <summary>
        /// 1: Low-light enhancement prioritizing performance. It processes brightness and details in the video image with lower performance consumption and faster processing speed.
        /// </summary>
        ///
        LOW_LIGHT_ENHANCE_LEVEL_FAST = 1,

    }

    ///
    /// <summary>
    /// Video denoising options.
    /// </summary>
    ///
    public class VideoDenoiserOptions
    {
        ///
        /// <summary>
        /// Video denoising mode.
        /// </summary>
        ///
        public VIDEO_DENOISER_MODE mode;

        ///
        /// <summary>
        /// Video denoising level.
        /// </summary>
        ///
        public VIDEO_DENOISER_LEVEL level;

        public VideoDenoiserOptions(VIDEO_DENOISER_MODE denoiserMode, VIDEO_DENOISER_LEVEL denoiserLevel)
        {
            this.mode = denoiserMode;
            this.level = denoiserLevel;
        }

        public VideoDenoiserOptions()
        {
            this.mode = VIDEO_DENOISER_MODE.VIDEO_DENOISER_AUTO;
            this.level = VIDEO_DENOISER_LEVEL.VIDEO_DENOISER_LEVEL_HIGH_QUALITY;
        }

    }

    ///
    /// <summary>
    /// Video denoising mode.
    /// </summary>
    ///
    public enum VIDEO_DENOISER_MODE
    {
        ///
        /// <summary>
        /// 0: (Default) Auto mode. The SDK automatically enables or disables the video denoising feature based on ambient brightness.
        /// </summary>
        ///
        VIDEO_DENOISER_AUTO = 0,

        ///
        /// <summary>
        /// 1: Manual mode. Users need to manually enable or disable the video denoising feature.
        /// </summary>
        ///
        VIDEO_DENOISER_MANUAL = 1,

    }

    ///
    /// <summary>
    /// Video denoising level.
    /// </summary>
    ///
    public enum VIDEO_DENOISER_LEVEL
    {
        ///
        /// <summary>
        /// 0: (Default) Video denoising prioritizing image quality. This level balances performance consumption and denoising effect. It has moderate performance consumption and denoising speed, providing optimal overall quality.
        /// </summary>
        ///
        VIDEO_DENOISER_LEVEL_HIGH_QUALITY = 0,

        ///
        /// <summary>
        /// 1: Video denoising prioritizing performance. This level focuses on saving performance in the balance between performance consumption and denoising effect. It consumes less performance and has faster denoising speed. To avoid noticeable ghosting in the processed video, it is recommended to use this setting when the camera is stationary.
        /// </summary>
        ///
        VIDEO_DENOISER_LEVEL_FAST = 1,

    }

    ///
    /// <summary>
    /// Color enhancement options.
    /// </summary>
    ///
    public class ColorEnhanceOptions
    {
        ///
        /// <summary>
        /// Degree of color enhancement. Value range is [0.0,1.0]. 0.0 means no color enhancement is applied to the video. The higher the value, the stronger the enhancement. Default is 0.5.
        /// </summary>
        ///
        public float strengthLevel;

        ///
        /// <summary>
        /// Degree of skin tone protection. Value range is [0.0,1.0]. 0.0 means no skin tone protection. The higher the value, the greater the protection. Default is 1.0.
        ///  When the color enhancement level is high, facial skin tones may appear distorted, so you need to set the skin protection level.
        ///  A higher skin protection level may slightly reduce the color enhancement effect. Therefore, to achieve the best color enhancement result, it is recommended that you dynamically adjust strengthLevel and skinProtectLevel to achieve the optimal effect.
        /// </summary>
        ///
        public float skinProtectLevel;

        public ColorEnhanceOptions(float stength, float skinProtect)
        {
            this.strengthLevel = stength;
            this.skinProtectLevel = skinProtect;
        }

        public ColorEnhanceOptions()
        {
            this.strengthLevel = 0;
            this.skinProtectLevel = 1;
        }

    }

    ///
    /// <summary>
    /// Custom background.
    /// </summary>
    ///
    public class VirtualBackgroundSource
    {
        ///
        /// <summary>
        /// Custom background. See BACKGROUND_SOURCE_TYPE.
        /// </summary>
        ///
        public BACKGROUND_SOURCE_TYPE background_source_type;

        ///
        /// <summary>
        /// Color of the custom background image. Format is a hexadecimal integer under RGB definition, without the # symbol, e.g., 0xFFB6C1 represents light pink. Default value is 0xFFFFFF, which represents white. Value range is [0x000000, 0xffffff]. If the value is invalid, the SDK replaces the original background with a white background. This parameter only takes effect when the custom background is one of the following types:
        ///  BACKGROUND_COLOR: The background image is a solid color image of the specified color.
        ///  BACKGROUND_IMG: If the image in source has a transparent background, the transparent area is filled with the specified color.
        /// </summary>
        ///
        public uint color;

        ///
        /// <summary>
        /// Absolute local path to the custom background. Supports PNG, JPG, MP4, AVI, MKV, and FLV formats. If the path is invalid, the SDK uses the original background or the solid color specified by color. This parameter only takes effect when the custom background type is BACKGROUND_IMG or BACKGROUND_VIDEO.
        /// </summary>
        ///
        public string source;

        ///
        /// <summary>
        /// Blur level of the custom background. See BACKGROUND_BLUR_DEGREE. This parameter only takes effect when the custom background type is BACKGROUND_BLUR.
        /// </summary>
        ///
        public BACKGROUND_BLUR_DEGREE blur_degree;

        public VirtualBackgroundSource()
        {
            this.background_source_type = BACKGROUND_SOURCE_TYPE.BACKGROUND_COLOR;
            this.color = 0xffffff;
            this.source = "";
            this.blur_degree = BACKGROUND_BLUR_DEGREE.BLUR_DEGREE_HIGH;
        }

        public VirtualBackgroundSource(BACKGROUND_SOURCE_TYPE background_source_type, uint color, string source, BACKGROUND_BLUR_DEGREE blur_degree)
        {
            this.background_source_type = background_source_type;
            this.color = color;
            this.source = source;
            this.blur_degree = blur_degree;
        }
    }

    ///
    /// <summary>
    /// Custom background.
    /// </summary>
    ///
    public enum BACKGROUND_SOURCE_TYPE
    {
        ///
        /// <summary>
        /// 0: Process the background as Alpha data without replacement, only segmenting the portrait and background. After setting this, you can call StartLocalVideoTranscoder to achieve a picture-in-picture effect for the portrait.
        /// </summary>
        ///
        BACKGROUND_NONE = 0,

        ///
        /// <summary>
        /// 1: (Default) The background is a solid color.
        /// </summary>
        ///
        BACKGROUND_COLOR = 1,

        ///
        /// <summary>
        /// 2: The background is an image in PNG or JPG format.
        /// </summary>
        ///
        BACKGROUND_IMG = 2,

        ///
        /// <summary>
        /// 3: The background is a blurred version of the original background.
        /// </summary>
        ///
        BACKGROUND_BLUR = 3,

        ///
        /// <summary>
        /// 4: The background is a local video in MP4, AVI, MKV, FLV, or similar formats.
        /// </summary>
        ///
        BACKGROUND_VIDEO = 4,

    }

    ///
    /// <summary>
    /// Blur level of the custom background image.
    /// </summary>
    ///
    public enum BACKGROUND_BLUR_DEGREE
    {
        ///
        /// <summary>
        /// 1: Low blur level for the custom background image. The background is still somewhat visible to the user.
        /// </summary>
        ///
        BLUR_DEGREE_LOW = 1,

        ///
        /// <summary>
        /// 2: Medium blur level for the custom background image. The background is harder to distinguish.
        /// </summary>
        ///
        BLUR_DEGREE_MEDIUM = 2,

        ///
        /// <summary>
        /// 3: (Default) High blur level for the custom background image. The background is barely visible.
        /// </summary>
        ///
        BLUR_DEGREE_HIGH = 3,

    }

    ///
    /// <summary>
    /// Processing properties for background images.
    /// </summary>
    ///
    public class SegmentationProperty
    {
        ///
        /// <summary>
        /// Algorithm used for background processing. See SEG_MODEL_TYPE.
        /// </summary>
        ///
        public SEG_MODEL_TYPE modelType;

        ///
        /// <summary>
        /// Accuracy range for recognizing background colors in the image. Value range is [0,1], default is 0.5. A larger value indicates a wider range of solid colors can be recognized. If the value is too large, solid colors at the edge or within the portrait may also be recognized. It is recommended to adjust this value dynamically based on actual results. This parameter takes effect only when modelType is set to SEG_MODEL_GREEN.
        /// </summary>
        ///
        public float greenCapacity;

        ///
        /// <summary>
        /// Screen color type. See SCREEN_COLOR_TYPE.
        /// </summary>
        ///
        public SCREEN_COLOR_TYPE screenColorType;

        public SegmentationProperty()
        {
            this.modelType = SEG_MODEL_TYPE.SEG_MODEL_AI;
            this.greenCapacity = 0.5f;
            this.screenColorType = SCREEN_COLOR_TYPE.SCREEN_COLOR_AUTO;
        }

        public SegmentationProperty(SEG_MODEL_TYPE modelType, float greenCapacity, SCREEN_COLOR_TYPE screenColorType)
        {
            this.modelType = modelType;
            this.greenCapacity = greenCapacity;
            this.screenColorType = screenColorType;
        }
    }

    ///
    /// <summary>
    /// Algorithm for background processing.
    /// </summary>
    ///
    public enum SEG_MODEL_TYPE
    {
        ///
        /// <summary>
        /// 1: (Default) Background processing algorithm suitable for all scenarios.
        /// </summary>
        ///
        SEG_MODEL_AI = 1,

        ///
        /// <summary>
        /// 2: Background processing algorithm (green screen only).
        /// </summary>
        ///
        SEG_MODEL_GREEN = 2,

    }

    ///
    /// <summary>
    /// Screen color types.
    /// </summary>
    ///
    public enum SCREEN_COLOR_TYPE
    {
        ///
        /// <summary>
        /// (0): Automatically select screen color.
        /// </summary>
        ///
        SCREEN_COLOR_AUTO = 0,

        ///
        /// <summary>
        /// (1): Green screen color.
        /// </summary>
        ///
        SCREEN_COLOR_GREEN = 1,

        ///
        /// <summary>
        /// (2): Blue screen color.
        /// </summary>
        ///
        SCREEN_COLOR_BLUE = 2,

    }

    ///
    /// <summary>
    /// Type of custom audio capture track.
    /// </summary>
    ///
    public enum AUDIO_TRACK_TYPE
    {
        ///
        /// @ignore
        ///
        AUDIO_TRACK_INVALID = -1,

        ///
        /// <summary>
        /// 0: Mixable audio track. Supports mixing with other audio streams (e.g., microphone audio) before playing locally or publishing to the channel. Higher latency compared to non-mixable tracks.
        /// </summary>
        ///
        AUDIO_TRACK_MIXABLE = 0,

        ///
        /// <summary>
        /// 1: Non-mixable audio track. Replaces microphone capture and does not support mixing with other audio streams. Lower latency compared to mixable tracks. If AUDIO_TRACK_DIRECT is specified, you must set publishMicrophoneTrack to false in ChannelMediaOptions when calling JoinChannel [2/2]; otherwise, joining the channel fails and returns error code -2.
        /// </summary>
        ///
        AUDIO_TRACK_DIRECT = 1,

    }

    ///
    /// <summary>
    /// Configuration options for custom audio tracks.
    /// </summary>
    ///
    public class AudioTrackConfig
    {
        ///
        /// <summary>
        /// Whether to enable local audio playback: true : (Default) Enable local audio playback. false : Disable local audio playback.
        /// </summary>
        ///
        public bool enableLocalPlayback;

        ///
        /// <summary>
        /// This parameter only takes effect for custom audio capture tracks of type AUDIO_TRACK_DIRECT. Whether to enable the audio processing module: true : Enable the audio processing module, including Echo Cancellation (AEC), Noise Suppression (ANS), and Automatic Gain Control (AGC). false : (Default) Disable the audio processing module.
        /// </summary>
        ///
        public bool enableAudioProcessing;

        public AudioTrackConfig()
        {
            this.enableLocalPlayback = true;
            this.enableAudioProcessing = false;
        }

        public AudioTrackConfig(bool enableLocalPlayback, bool enableAudioProcessing)
        {
            this.enableLocalPlayback = enableLocalPlayback;
            this.enableAudioProcessing = enableAudioProcessing;
        }
    }

    ///
    /// <summary>
    /// Preset voice beautifier effects.
    /// </summary>
    ///
    public enum VOICE_BEAUTIFIER_PRESET
    {
        ///
        /// <summary>
        /// Original voice, i.e., disables voice beautifier effects.
        /// </summary>
        ///
        VOICE_BEAUTIFIER_OFF = 0x00000000,

        ///
        /// <summary>
        /// Magnetic (male). This setting is only effective for male voices. Do not apply it to female voices, otherwise audio distortion may occur.
        /// </summary>
        ///
        CHAT_BEAUTIFIER_MAGNETIC = 0x01010100,

        ///
        /// <summary>
        /// Fresh (female). This setting is only effective for female voices. Do not apply it to male voices, otherwise audio distortion may occur.
        /// </summary>
        ///
        CHAT_BEAUTIFIER_FRESH = 0x01010200,

        ///
        /// <summary>
        /// Energetic (female). This setting is only effective for female voices. Do not apply it to male voices, otherwise audio distortion may occur.
        /// </summary>
        ///
        CHAT_BEAUTIFIER_VITALITY = 0x01010300,

        ///
        /// <summary>
        /// Singing beautifier.
        ///  If you call SetVoiceBeautifierPreset (SINGING_BEAUTIFIER), you can beautify male voices and add a small room reverb effect. Do not apply it to female voices, otherwise audio distortion may occur.
        ///  If you call SetVoiceBeautifierParameters (SINGING_BEAUTIFIER, param1, param2), you can beautify male or female voices and add reverb effects.
        /// </summary>
        ///
        SINGING_BEAUTIFIER = 0x01020100,

        ///
        /// <summary>
        /// Vigorous.
        /// </summary>
        ///
        TIMBRE_TRANSFORMATION_VIGOROUS = 0x01030100,

        ///
        /// <summary>
        /// Deep.
        /// </summary>
        ///
        TIMBRE_TRANSFORMATION_DEEP = 0x01030200,

        ///
        /// <summary>
        /// Mellow.
        /// </summary>
        ///
        TIMBRE_TRANSFORMATION_MELLOW = 0x01030300,

        ///
        /// <summary>
        /// Falsetto.
        /// </summary>
        ///
        TIMBRE_TRANSFORMATION_FALSETTO = 0x01030400,

        ///
        /// <summary>
        /// Full.
        /// </summary>
        ///
        TIMBRE_TRANSFORMATION_FULL = 0x01030500,

        ///
        /// <summary>
        /// Clear.
        /// </summary>
        ///
        TIMBRE_TRANSFORMATION_CLEAR = 0x01030600,

        ///
        /// <summary>
        /// Resounding.
        /// </summary>
        ///
        TIMBRE_TRANSFORMATION_RESOUNDING = 0x01030700,

        ///
        /// <summary>
        /// Ringing.
        /// </summary>
        ///
        TIMBRE_TRANSFORMATION_RINGING = 0x01030800,

        ///
        /// <summary>
        /// Ultra-high-quality voice, which makes the audio clearer and more detailed.
        ///  For better results, it is recommended to set the profile parameter of SetAudioProfile [2/2] to AUDIO_PROFILE_MUSIC_HIGH_QUALITY (4) or AUDIO_PROFILE_MUSIC_HIGH_QUALITY_STEREO (5), and the scenario parameter to AUDIO_SCENARIO_GAME_STREAMING (3) before calling SetVoiceBeautifierPreset.
        ///  If the user's audio capture device can highly restore audio details, it is recommended not to enable ultra-high-quality voice, otherwise the SDK may over-restore audio details and fail to achieve the expected effect.
        /// </summary>
        ///
        ULTRA_HIGH_QUALITY_VOICE = 0x01040100,

    }

    ///
    /// <summary>
    /// Preset audio effects.
    /// 
    /// SetAudioProfile [1/2] profile
    /// Preset audio effects profile
    ///  ROOM_ACOUSTICS_VIRTUAL_STEREO
    ///  ROOM_ACOUSTICS_3D_VOICE
    ///  ROOM_ACOUSTICS_VIRTUAL_SURROUND_SOUND AUDIO_PROFILE_MUSIC_HIGH_QUALITY_STEREO or AUDIO_PROFILE_MUSIC_STANDARD_STEREO Other preset audio effects (excluding AUDIO_EFFECT_OFF) AUDIO_PROFILE_MUSIC_HIGH_QUALITY or AUDIO_PROFILE_MUSIC_HIGH_QUALITY_STEREO
    /// </summary>
    ///
    public enum AUDIO_EFFECT_PRESET
    {
        ///
        /// <summary>
        /// Original sound, disables voice effects.
        /// </summary>
        ///
        AUDIO_EFFECT_OFF = 0x00000000,

        ///
        /// <summary>
        /// KTV.
        /// </summary>
        ///
        ROOM_ACOUSTICS_KTV = 0x02010100,

        ///
        /// <summary>
        /// Concert.
        /// </summary>
        ///
        ROOM_ACOUSTICS_VOCAL_CONCERT = 0x02010200,

        ///
        /// <summary>
        /// Studio.
        /// </summary>
        ///
        ROOM_ACOUSTICS_STUDIO = 0x02010300,

        ///
        /// <summary>
        /// Phonograph.
        /// </summary>
        ///
        ROOM_ACOUSTICS_PHONOGRAPH = 0x02010400,

        ///
        /// <summary>
        /// Virtual stereo, where the SDK renders mono audio into stereo effects.
        /// </summary>
        ///
        ROOM_ACOUSTICS_VIRTUAL_STEREO = 0x02010500,

        ///
        /// <summary>
        /// Spacious.
        /// </summary>
        ///
        ROOM_ACOUSTICS_SPACIAL = 0x02010600,

        ///
        /// <summary>
        /// Ethereal.
        /// </summary>
        ///
        ROOM_ACOUSTICS_ETHEREAL = 0x02010700,

        ///
        /// <summary>
        /// 3D voice, where the SDK renders audio to surround the user. The default surround cycle is 10 seconds. After setting this effect, you can also call SetAudioEffectParameters to modify the surround cycle. To hear the expected effect when 3D voice is enabled, users must use audio playback devices that support stereo.
        /// </summary>
        ///
        ROOM_ACOUSTICS_3D_VOICE = 0x02010800,

        ///
        /// <summary>
        /// Virtual surround sound, where the SDK simulates a surround sound field based on stereo audio to create a surround effect. To hear the expected effect when virtual surround sound is enabled, users must use audio playback devices that support stereo.
        /// </summary>
        ///
        ROOM_ACOUSTICS_VIRTUAL_SURROUND_SOUND = 0x02010900,

        ///
        /// <summary>
        /// Chorus. Agora recommends using this in chorus scenarios to make vocals sound more spatial and three-dimensional.
        /// </summary>
        ///
        ROOM_ACOUSTICS_CHORUS = 0x02010D00,

        ///
        /// <summary>
        /// Deep male voice. Recommended for male voice processing; otherwise, the expected effect may not be achieved.
        /// </summary>
        ///
        VOICE_CHANGER_EFFECT_UNCLE = 0x02020100,

        ///
        /// <summary>
        /// Elderly male. Recommended for male voice processing; otherwise, the expected effect may not be achieved.
        /// </summary>
        ///
        VOICE_CHANGER_EFFECT_OLDMAN = 0x02020200,

        ///
        /// <summary>
        /// Boy. Recommended for male voice processing; otherwise, the expected effect may not be achieved.
        /// </summary>
        ///
        VOICE_CHANGER_EFFECT_BOY = 0x02020300,

        ///
        /// <summary>
        /// Young woman. Recommended for female voice processing; otherwise, the expected effect may not be achieved.
        /// </summary>
        ///
        VOICE_CHANGER_EFFECT_SISTER = 0x02020400,

        ///
        /// <summary>
        /// Girl. Recommended for female voice processing; otherwise, the expected effect may not be achieved.
        /// </summary>
        ///
        VOICE_CHANGER_EFFECT_GIRL = 0x02020500,

        ///
        /// <summary>
        /// Pig King.
        /// </summary>
        ///
        VOICE_CHANGER_EFFECT_PIGKING = 0x02020600,

        ///
        /// <summary>
        /// Hulk.
        /// </summary>
        ///
        VOICE_CHANGER_EFFECT_HULK = 0x02020700,

        ///
        /// <summary>
        /// R&B.
        /// </summary>
        ///
        STYLE_TRANSFORMATION_RNB = 0x02030100,

        ///
        /// <summary>
        /// Pop.
        /// </summary>
        ///
        STYLE_TRANSFORMATION_POPULAR = 0x02030200,

        ///
        /// <summary>
        /// Auto-tune, where the SDK corrects the actual pitch based on the C major scale. After setting this effect, you can also call SetAudioEffectParameters to adjust the base scale and tonic pitch.
        /// </summary>
        ///
        PITCH_CORRECTION = 0x02040100,

    }

    ///
    /// <summary>
    /// Preset voice conversion effects.
    /// </summary>
    ///
    public enum VOICE_CONVERSION_PRESET
    {
        ///
        /// <summary>
        /// Original voice, i.e., disables voice conversion effects.
        /// </summary>
        ///
        VOICE_CONVERSION_OFF = 0x00000000,

        ///
        /// <summary>
        /// Neutral. To avoid audio distortion, make sure to apply this effect only to female voices.
        /// </summary>
        ///
        VOICE_CHANGER_NEUTRAL = 0x03010100,

        ///
        /// <summary>
        /// Sweet. To avoid audio distortion, make sure to apply this effect only to female voices.
        /// </summary>
        ///
        VOICE_CHANGER_SWEET = 0x03010200,

        ///
        /// <summary>
        /// Solid. To avoid audio distortion, make sure to apply this effect only to male voices.
        /// </summary>
        ///
        VOICE_CHANGER_SOLID = 0x03010300,

        ///
        /// <summary>
        /// Bass. To avoid audio distortion, make sure to apply this effect only to male voices.
        /// </summary>
        ///
        VOICE_CHANGER_BASS = 0x03010400,

        ///
        /// @ignore
        ///
        VOICE_CHANGER_CARTOON = 0x03010500,

        ///
        /// @ignore
        ///
        VOICE_CHANGER_CHILDLIKE = 0x03010600,

        ///
        /// @ignore
        ///
        VOICE_CHANGER_PHONE_OPERATOR = 0x03010700,

        ///
        /// @ignore
        ///
        VOICE_CHANGER_MONSTER = 0x03010800,

        ///
        /// @ignore
        ///
        VOICE_CHANGER_TRANSFORMERS = 0x03010900,

        ///
        /// @ignore
        ///
        VOICE_CHANGER_GROOT = 0x03010A00,

        ///
        /// @ignore
        ///
        VOICE_CHANGER_DARTH_VADER = 0x03010B00,

        ///
        /// @ignore
        ///
        VOICE_CHANGER_IRON_LADY = 0x03010C00,

        ///
        /// @ignore
        ///
        VOICE_CHANGER_SHIN_CHAN = 0x03010D00,

        ///
        /// @ignore
        ///
        VOICE_CHANGER_GIRLISH_MAN = 0x03010E00,

        ///
        /// @ignore
        ///
        VOICE_CHANGER_CHIPMUNK = 0x03010F00,

    }

    ///
    /// <summary>
    /// Preset headphone equalizer types.
    /// </summary>
    ///
    public enum HEADPHONE_EQUALIZER_PRESET
    {
        ///
        /// <summary>
        /// Turn off headphone equalizer and listen to original audio.
        /// </summary>
        ///
        HEADPHONE_EQUALIZER_OFF = 0x00000000,

        ///
        /// <summary>
        /// Use equalizer for over-ear headphones.
        /// </summary>
        ///
        HEADPHONE_EQUALIZER_OVEREAR = 0x04000001,

        ///
        /// <summary>
        /// Use equalizer for in-ear headphones.
        /// </summary>
        ///
        HEADPHONE_EQUALIZER_INEAR = 0x04000002,

    }

    ///
    /// <summary>
    /// AI tuner voice effect types.
    /// </summary>
    ///
    public enum VOICE_AI_TUNER_TYPE
    {
        ///
        /// <summary>
        /// 0: Mature male voice. A deep and magnetic male voice.
        /// </summary>
        ///
        VOICE_AI_TUNER_MATURE_MALE,

        ///
        /// <summary>
        /// 1: Fresh male voice. A fresh and slightly sweet male voice.
        /// </summary>
        ///
        VOICE_AI_TUNER_FRESH_MALE,

        ///
        /// <summary>
        /// 2: Elegant female voice. A deep and charming female voice.
        /// </summary>
        ///
        VOICE_AI_TUNER_ELEGANT_FEMALE,

        ///
        /// <summary>
        /// 3: Sweet girl voice. A high-pitched and cute female voice.
        /// </summary>
        ///
        VOICE_AI_TUNER_SWEET_FEMALE,

        ///
        /// <summary>
        /// 4: Warm male singing voice. A warm and melodious male voice.
        /// </summary>
        ///
        VOICE_AI_TUNER_WARM_MALE_SINGING,

        ///
        /// <summary>
        /// 5: Gentle female singing voice. A soft and delicate female voice.
        /// </summary>
        ///
        VOICE_AI_TUNER_GENTLE_FEMALE_SINGING,

        ///
        /// <summary>
        /// 6: Husky mature male singing voice. A unique hoarse male voice.
        /// </summary>
        ///
        VOICE_AI_TUNER_HUSKY_MALE_SINGING,

        ///
        /// <summary>
        /// 7: Warm elegant female singing voice. A warm and mature female voice.
        /// </summary>
        ///
        VOICE_AI_TUNER_WARM_ELEGANT_FEMALE_SINGING,

        ///
        /// <summary>
        /// 8: Powerful male singing voice. A strong and forceful male voice.
        /// </summary>
        ///
        VOICE_AI_TUNER_POWERFUL_MALE_SINGING,

        ///
        /// <summary>
        /// 9: Dreamy female singing voice. A dreamy and soft female voice.
        /// </summary>
        ///
        VOICE_AI_TUNER_DREAMY_FEMALE_SINGING,

    }

    ///
    /// <summary>
    /// Audio configuration for the shared screen stream.
    /// 
    /// Only applicable when captureAudio is set to true.
    /// </summary>
    ///
    public class ScreenAudioParameters
    {
        ///
        /// <summary>
        /// Audio sampling rate (Hz). Default is 16000.
        /// </summary>
        ///
        public int sampleRate;

        ///
        /// <summary>
        /// Number of audio channels. Default is 2, indicating stereo.
        /// </summary>
        ///
        public int channels;

        ///
        /// <summary>
        /// Captured system volume. Value range is [0,100]. Default is 100.
        /// </summary>
        ///
        public int captureSignalVolume;

        ///
        /// @ignore
        ///
        public bool excludeCurrentProcessAudio;

        public ScreenAudioParameters()
        {
            this.sampleRate = 48000;
            this.channels = 2;
            this.captureSignalVolume = 100;
        }

        public ScreenAudioParameters(int sampleRate, int channels, int captureSignalVolume, bool excludeCurrentProcessAudio)
        {
            this.sampleRate = sampleRate;
            this.channels = channels;
            this.captureSignalVolume = captureSignalVolume;
            this.excludeCurrentProcessAudio = excludeCurrentProcessAudio;
        }
    }

    ///
    /// <summary>
    /// Configuration parameters for screen sharing.
    /// </summary>
    ///
    public class ScreenCaptureParameters
    {
        ///
        /// @ignore
        ///
        public bool captureAudio;

        ///
        /// @ignore
        ///
        public ScreenAudioParameters audioParams;

        ///
        /// <summary>
        /// When setting the encoding resolution in a document sharing scenario (SCREEN_SCENARIO_DOCUMENT), choose one of the following options:
        ///  If you want the best image quality, it is recommended to set the encoding resolution equal to the capture resolution.
        ///  If you want a balance between image quality, bandwidth, and system performance:
        ///  When the capture resolution is greater than 1920  1080, it is recommended that the encoding resolution is no less than 1920  1080.
        ///  When the capture resolution is less than 1920  1080, it is recommended that the encoding resolution is no less than 1280  720. The video encoding resolution of the screen sharing stream. See VideoDimensions. The default value is 1920  1080, i.e., 2073600 pixels. This pixel value is used for billing. When the aspect ratio of the shared screen resolution does not match this setting, the SDK encodes using the following strategy. Suppose dimensions is set to 1920  1080:
        ///  If the screen resolution is smaller than dimensions, e.g., 1000  1000, the SDK encodes directly at 1000  1000.
        ///  If the screen resolution is larger than dimensions, e.g., 2000  1500, the SDK uses the screen resolution's aspect ratio (4:3) to select the maximum resolution within dimensions, i.e., 1440  1080.
        /// </summary>
        ///
        public VideoDimensions dimensions;

        ///
        /// <summary>
        /// On Windows and macOS platforms, specifies the video encoding frame rate of the screen sharing stream. Unit: fps; default is 5. It is recommended not to exceed 15.
        /// </summary>
        ///
        public int frameRate;

        ///
        /// <summary>
        /// Bitrate of the shared video. On Windows and macOS platforms, specifies the video encoding bitrate of the screen sharing stream. Unit: Kbps; default is 0, which means the SDK calculates a reasonable value based on the current shared screen resolution.
        /// </summary>
        ///
        public int bitrate;

        ///
        /// <summary>
        /// Due to macOS system limitations, setting this parameter to false has no effect when sharing the screen (no effect when sharing a window). Whether to capture the mouse for screen sharing: true : (default) capture the mouse. false : do not capture the mouse.
        /// </summary>
        ///
        public bool captureMouseCursor;

        ///
        /// <summary>
        /// Due to macOS system limitations, when setting this member to bring the window to the front, only the main window is brought to the front if the current application has multiple windows. When calling the StartScreenCaptureByWindowId method to share a window, whether to bring the window to the front: true : bring the window to the front. false : (default) do not bring the window to the front.
        /// </summary>
        ///
        public bool windowFocus;

        ///
        /// <summary>
        /// List of IDs of windows to be excluded. When calling StartScreenCaptureByDisplayId to start screen sharing, you can use this parameter to exclude specific windows. You can also dynamically exclude specific windows by using this parameter when calling UpdateScreenCaptureParameters to update the screen sharing configuration.
        /// </summary>
        ///
        public view_t[] excludeWindowList;

        ///
        /// <summary>
        /// Number of windows to be excluded. On Windows, the maximum value for this parameter is 24. If it exceeds this value, the window exclusion feature becomes ineffective.
        /// </summary>
        ///
        public int excludeWindowCount;

        ///
        /// <summary>
        /// (Applicable only to macOS and Windows) Width of the highlight border (px). Default is 5. Value range is (0,50]. This parameter only takes effect when highLighted is set to true.
        /// </summary>
        ///
        public int highLightWidth;

        ///
        /// <summary>
        /// (Applicable only to macOS and Windows)
        ///  On Windows, specifies the highlight ARGB color. Default is 0xFF8CBF26.
        ///  On macOS, COLOR_CLASS refers to NSColor.
        /// </summary>
        ///
        public uint highLightColor;

        ///
        /// <summary>
        /// When sharing a partial area of a window or screen, if this parameter is set to true, the SDK highlights the entire window or screen. (Applicable only to macOS and Windows) Whether to highlight the shared window or screen: true : highlight. false : (default) do not highlight.
        /// </summary>
        ///
        public bool enableHighLight;

        public ScreenCaptureParameters()
        {
            this.captureAudio = false;
            this.dimensions = new VideoDimensions(1920, 1080);
            this.frameRate = 5;
            this.bitrate = (int)BITRATE.STANDARD_BITRATE;
            this.captureMouseCursor = true;
            this.windowFocus = false;
            this.excludeWindowList = new view_t[0];
            this.excludeWindowCount = 0;
            this.highLightWidth = 0;
            this.highLightColor = 0;
            this.enableHighLight = false;
        }

        public ScreenCaptureParameters(VideoDimensions d, int f, int b)
        {
            this.captureAudio = false;
            this.dimensions = d;
            this.frameRate = f;
            this.bitrate = b;
            this.captureMouseCursor = true;
            this.windowFocus = false;
            this.excludeWindowList = new view_t[0];
            this.excludeWindowCount = 0;
            this.highLightWidth = 0;
            this.highLightColor = 0;
            this.enableHighLight = false;
        }

        public ScreenCaptureParameters(int width, int height, int f, int b)
        {
            this.captureAudio = false;
            this.dimensions = new VideoDimensions(width, height);
            this.frameRate = f;
            this.bitrate = b;
            this.captureMouseCursor = true;
            this.windowFocus = false;
            this.excludeWindowList = new view_t[0];
            this.excludeWindowCount = 0;
            this.highLightWidth = 0;
            this.highLightColor = 0;
            this.enableHighLight = false;
        }

        public ScreenCaptureParameters(int width, int height, int f, int b, bool cur, bool fcs)
        {
            this.captureAudio = false;
            this.dimensions = new VideoDimensions(width, height);
            this.frameRate = f;
            this.bitrate = b;
            this.captureMouseCursor = cur;
            this.windowFocus = fcs;
            this.excludeWindowList = new view_t[0];
            this.excludeWindowCount = 0;
            this.highLightWidth = 0;
            this.highLightColor = 0;
            this.enableHighLight = false;
        }

        public ScreenCaptureParameters(int width, int height, int f, int b, view_t[] ex, int cnt)
        {
            this.captureAudio = false;
            this.dimensions = new VideoDimensions(width, height);
            this.frameRate = f;
            this.bitrate = b;
            this.captureMouseCursor = true;
            this.windowFocus = false;
            this.excludeWindowList = ex;
            this.excludeWindowCount = cnt;
            this.highLightWidth = 0;
            this.highLightColor = 0;
            this.enableHighLight = false;
        }

        public ScreenCaptureParameters(int width, int height, int f, int b, bool cur, bool fcs, view_t[] ex, int cnt)
        {
            this.captureAudio = false;
            this.dimensions = new VideoDimensions(width, height);
            this.frameRate = f;
            this.bitrate = b;
            this.captureMouseCursor = cur;
            this.windowFocus = fcs;
            this.excludeWindowList = ex;
            this.excludeWindowCount = cnt;
            this.highLightWidth = 0;
            this.highLightColor = 0;
            this.enableHighLight = false;
        }

        public ScreenCaptureParameters(bool captureAudio, ScreenAudioParameters audioParams, VideoDimensions dimensions, int frameRate, int bitrate, bool captureMouseCursor, bool windowFocus, view_t[] excludeWindowList, int excludeWindowCount, int highLightWidth, uint highLightColor, bool enableHighLight)
        {
            this.captureAudio = captureAudio;
            this.audioParams = audioParams;
            this.dimensions = dimensions;
            this.frameRate = frameRate;
            this.bitrate = bitrate;
            this.captureMouseCursor = captureMouseCursor;
            this.windowFocus = windowFocus;
            this.excludeWindowList = excludeWindowList;
            this.excludeWindowCount = excludeWindowCount;
            this.highLightWidth = highLightWidth;
            this.highLightColor = highLightColor;
            this.enableHighLight = enableHighLight;
        }
    }

    ///
    /// <summary>
    /// Audio recording quality.
    /// </summary>
    ///
    public enum AUDIO_RECORDING_QUALITY_TYPE
    {
        ///
        /// <summary>
        /// 0: Low quality. 32 kHz sampling rate, file size for 10 minutes is about 1.2 MB.
        /// </summary>
        ///
        AUDIO_RECORDING_QUALITY_LOW = 0,

        ///
        /// <summary>
        /// 1: Medium quality. 32 kHz sampling rate, file size for 10 minutes is about 2 MB.
        /// </summary>
        ///
        AUDIO_RECORDING_QUALITY_MEDIUM = 1,

        ///
        /// <summary>
        /// 2: High quality. 32 kHz sampling rate, file size for 10 minutes is about 3.75 MB.
        /// </summary>
        ///
        AUDIO_RECORDING_QUALITY_HIGH = 2,

        ///
        /// <summary>
        /// 3: Ultra high quality. 32 kHz sampling rate, file size for 10 minutes is about 7.5 MB.
        /// </summary>
        ///
        AUDIO_RECORDING_QUALITY_ULTRA_HIGH = 3,

    }

    ///
    /// <summary>
    /// Recording content. Set in StartAudioRecording [3/3].
    /// </summary>
    ///
    public enum AUDIO_FILE_RECORDING_TYPE
    {
        ///
        /// <summary>
        /// 1: Record only the local user's audio.
        /// </summary>
        ///
        AUDIO_FILE_RECORDING_MIC = 1,

        ///
        /// <summary>
        /// 2: Record only the audio of all remote users.
        /// </summary>
        ///
        AUDIO_FILE_RECORDING_PLAYBACK = 2,

        ///
        /// <summary>
        /// 3: Record the mixed audio of the local and all remote users.
        /// </summary>
        ///
        AUDIO_FILE_RECORDING_MIXED = 3,

    }

    ///
    /// <summary>
    /// Audio encoding content.
    /// </summary>
    ///
    public enum AUDIO_ENCODED_FRAME_OBSERVER_POSITION
    {
        ///
        /// <summary>
        /// 1: Encode only the local user's audio.
        /// </summary>
        ///
        AUDIO_ENCODED_FRAME_OBSERVER_POSITION_RECORD = 1,

        ///
        /// <summary>
        /// 2: Encode only the audio of all remote users.
        /// </summary>
        ///
        AUDIO_ENCODED_FRAME_OBSERVER_POSITION_PLAYBACK = 2,

        ///
        /// <summary>
        /// 3: Encode the mixed audio of the local and all remote users.
        /// </summary>
        ///
        AUDIO_ENCODED_FRAME_OBSERVER_POSITION_MIXED = 3,

    }

    ///
    /// <summary>
    /// Recording configuration.
    /// </summary>
    ///
    public class AudioRecordingConfiguration
    {
        ///
        /// <summary>
        /// The absolute path where the recording file is saved locally. Must include the file name and extension. For example: C:\music\audio.aac. Make sure the path you specify exists and is writable.
        /// </summary>
        ///
        public string filePath;

        ///
        /// <summary>
        /// Specifies whether to encode the audio data: true : Encode the audio data using AAC. false : (Default) Do not encode the audio data. Save the raw recorded audio data directly.
        /// </summary>
        ///
        public bool encode;

        ///
        /// <summary>
        /// If you set this parameter to 44100 or 48000, to ensure recording quality, we recommend recording a WAV file or an AAC file with quality set to AUDIO_RECORDING_QUALITY_MEDIUM or AUDIO_RECORDING_QUALITY_HIGH. Recording sample rate (Hz).
        ///  16000
        ///  32000 (Default)
        ///  44100
        ///  48000
        /// </summary>
        ///
        public int sampleRate;

        ///
        /// <summary>
        /// Recording content. See AUDIO_FILE_RECORDING_TYPE.
        /// </summary>
        ///
        public AUDIO_FILE_RECORDING_TYPE fileRecordingType;

        ///
        /// <summary>
        /// Recording quality. See AUDIO_RECORDING_QUALITY_TYPE. This parameter applies to AAC files only.
        /// </summary>
        ///
        public AUDIO_RECORDING_QUALITY_TYPE quality;

        ///
        /// <summary>
        /// The actual recorded audio channel depends on the captured audio channel:
        ///  If the captured audio is mono and recordingChannel is set to 2, the recorded audio is stereo created by duplicating the mono data, not true stereo.
        ///  If the captured audio is stereo and recordingChannel is set to 1, the recorded audio is mono created by mixing the stereo data. In addition, the integration solution may affect the final recorded audio channel. If you want to record true stereo, please [contact technical support](https://ticket.shengwang.cn/) for assistance. Audio channel for recording. The following values are supported:
        ///  1: (Default) Mono.
        ///  2: Stereo.
        /// </summary>
        ///
        public int recordingChannel;

        public AudioRecordingConfiguration()
        {
            this.filePath = "";
            this.encode = false;
            this.sampleRate = 32000;
            this.fileRecordingType = AUDIO_FILE_RECORDING_TYPE.AUDIO_FILE_RECORDING_MIXED;
            this.quality = AUDIO_RECORDING_QUALITY_TYPE.AUDIO_RECORDING_QUALITY_LOW;
            this.recordingChannel = 1;
        }

        public AudioRecordingConfiguration(string file_path, int sample_rate, AUDIO_RECORDING_QUALITY_TYPE quality_type, int channel)
        {
            this.filePath = file_path;
            this.encode = false;
            this.sampleRate = sample_rate;
            this.fileRecordingType = AUDIO_FILE_RECORDING_TYPE.AUDIO_FILE_RECORDING_MIXED;
            this.quality = quality_type;
            this.recordingChannel = channel;
        }

        public AudioRecordingConfiguration(string file_path, bool enc, int sample_rate, AUDIO_FILE_RECORDING_TYPE type, AUDIO_RECORDING_QUALITY_TYPE quality_type, int channel)
        {
            this.filePath = file_path;
            this.encode = enc;
            this.sampleRate = sample_rate;
            this.fileRecordingType = type;
            this.quality = quality_type;
            this.recordingChannel = channel;
        }

        public AudioRecordingConfiguration(AudioRecordingConfiguration rhs)
        {
            this.filePath = rhs.filePath;
            this.encode = rhs.encode;
            this.sampleRate = rhs.sampleRate;
            this.fileRecordingType = rhs.fileRecordingType;
            this.quality = rhs.quality;
            this.recordingChannel = rhs.recordingChannel;
        }

    }

    ///
    /// <summary>
    /// Configuration for encoded audio frame observer.
    /// </summary>
    ///
    public class AudioEncodedFrameObserverConfig
    {
        ///
        /// <summary>
        /// Audio encoding content. See AUDIO_ENCODED_FRAME_OBSERVER_POSITION.
        /// </summary>
        ///
        public AUDIO_ENCODED_FRAME_OBSERVER_POSITION postionType;

        ///
        /// <summary>
        /// Audio encoding type. See AUDIO_ENCODING_TYPE.
        /// </summary>
        ///
        public AUDIO_ENCODING_TYPE encodingType;

        public AudioEncodedFrameObserverConfig()
        {
            this.postionType = AUDIO_ENCODED_FRAME_OBSERVER_POSITION.AUDIO_ENCODED_FRAME_OBSERVER_POSITION_PLAYBACK;
            this.encodingType = AUDIO_ENCODING_TYPE.AUDIO_ENCODING_TYPE_OPUS_48000_MEDIUM;
        }

        public AudioEncodedFrameObserverConfig(AUDIO_ENCODED_FRAME_OBSERVER_POSITION postionType, AUDIO_ENCODING_TYPE encodingType)
        {
            this.postionType = postionType;
            this.encodingType = encodingType;
        }
    }

    ///
    /// <summary>
    /// Access region, i.e., the region of the server the SDK connects to.
    /// </summary>
    ///
    public enum AREA_CODE : uint
    {
        ///
        /// <summary>
        /// Mainland China.
        /// </summary>
        ///
        AREA_CODE_CN = 0x00000001,

        ///
        /// <summary>
        /// North America.
        /// </summary>
        ///
        AREA_CODE_NA = 0x00000002,

        ///
        /// <summary>
        /// Europe.
        /// </summary>
        ///
        AREA_CODE_EU = 0x00000004,

        ///
        /// <summary>
        /// Asia excluding China.
        /// </summary>
        ///
        AREA_CODE_AS = 0x00000008,

        ///
        /// <summary>
        /// Japan.
        /// </summary>
        ///
        AREA_CODE_JP = 0x00000010,

        ///
        /// <summary>
        /// India.
        /// </summary>
        ///
        AREA_CODE_IN = 0x00000020,

        ///
        /// <summary>
        /// Global.
        /// </summary>
        ///
        AREA_CODE_GLOB = (0xFFFFFFFF),

    }

    ///
    /// @ignore
    ///
    public enum AREA_CODE_EX : uint
    {
        ///
        /// @ignore
        ///
        AREA_CODE_OC = 0x00000040,

        ///
        /// @ignore
        ///
        AREA_CODE_SA = 0x00000080,

        ///
        /// @ignore
        ///
        AREA_CODE_AF = 0x00000100,

        ///
        /// @ignore
        ///
        AREA_CODE_KR = 0x00000200,

        ///
        /// @ignore
        ///
        AREA_CODE_HKMC = 0x00000400,

        ///
        /// @ignore
        ///
        AREA_CODE_US = 0x00000800,

        ///
        /// @ignore
        ///
        AREA_CODE_RU = 0x00001000,

        ///
        /// @ignore
        ///
        AREA_CODE_OVS = 0xFFFFFFFE,

    }

    ///
    /// <summary>
    /// Error codes for cross-channel media stream relay failures.
    /// </summary>
    ///
    public enum CHANNEL_MEDIA_RELAY_ERROR
    {
        ///
        /// <summary>
        /// 0: Everything works fine.
        /// </summary>
        ///
        RELAY_OK = 0,

        ///
        /// <summary>
        /// 1: Server returned an error.
        /// </summary>
        ///
        RELAY_ERROR_SERVER_ERROR_RESPONSE = 1,

        ///
        /// <summary>
        /// 2: No response from server.
        /// This error may be caused by poor network conditions. If this error is reported when initiating cross-channel media relay, you can try again later; if it occurs during the relay, you can call the LeaveChannel [2/2] method to leave the channel.
        /// It may also be caused by the current App ID not having cross-channel relay enabled. You can [contact technical support](https://ticket.shengwang.cn/) to request enabling it.
        /// </summary>
        ///
        RELAY_ERROR_SERVER_NO_RESPONSE = 2,

        ///
        /// <summary>
        /// 3: SDK cannot acquire the service, possibly due to limited server resources.
        /// </summary>
        ///
        RELAY_ERROR_NO_RESOURCE_AVAILABLE = 3,

        ///
        /// <summary>
        /// 4: Failed to initiate cross-channel media stream relay request.
        /// </summary>
        ///
        RELAY_ERROR_FAILED_JOIN_SRC = 4,

        ///
        /// <summary>
        /// 5: Failed to accept cross-channel media stream relay request.
        /// </summary>
        ///
        RELAY_ERROR_FAILED_JOIN_DEST = 5,

        ///
        /// <summary>
        /// 6: Server failed to receive cross-channel media stream.
        /// </summary>
        ///
        RELAY_ERROR_FAILED_PACKET_RECEIVED_FROM_SRC = 6,

        ///
        /// <summary>
        /// 7: Server failed to send cross-channel media stream.
        /// </summary>
        ///
        RELAY_ERROR_FAILED_PACKET_SENT_TO_DEST = 7,

        ///
        /// <summary>
        /// 8: SDK disconnected from the server due to poor network quality. You can call the LeaveChannel [2/2] method to leave the current channel.
        /// </summary>
        ///
        RELAY_ERROR_SERVER_CONNECTION_LOST = 8,

        ///
        /// <summary>
        /// 9: Internal server error.
        /// </summary>
        ///
        RELAY_ERROR_INTERNAL_ERROR = 9,

        ///
        /// <summary>
        /// 10: The token for the source channel has expired.
        /// </summary>
        ///
        RELAY_ERROR_SRC_TOKEN_EXPIRED = 10,

        ///
        /// <summary>
        /// 11: The token for the destination channel has expired.
        /// </summary>
        ///
        RELAY_ERROR_DEST_TOKEN_EXPIRED = 11,

    }

    ///
    /// <summary>
    /// Status codes for cross-channel media stream relay.
    /// </summary>
    ///
    public enum CHANNEL_MEDIA_RELAY_STATE
    {
        ///
        /// <summary>
        /// 0: Initial state. After successfully calling StopChannelMediaRelay to stop the relay, OnChannelMediaRelayStateChanged will callback this state.
        /// </summary>
        ///
        RELAY_STATE_IDLE = 0,

        ///
        /// <summary>
        /// 1: SDK is attempting cross-channel connection.
        /// </summary>
        ///
        RELAY_STATE_CONNECTING = 1,

        ///
        /// <summary>
        /// 2: The broadcaster in the source channel has successfully joined the destination channel.
        /// </summary>
        ///
        RELAY_STATE_RUNNING = 2,

        ///
        /// <summary>
        /// 3: An error occurred. See the code parameter in OnChannelMediaRelayStateChanged for error details.
        /// </summary>
        ///
        RELAY_STATE_FAILURE = 3,

    }

    ///
    /// <summary>
    /// Channel media information.
    /// </summary>
    ///
    public class ChannelMediaInfo
    {
        ///
        /// <summary>
        /// User ID.
        /// </summary>
        ///
        public uint uid;

        ///
        /// <summary>
        /// Channel name.
        /// </summary>
        ///
        public string channelName;

        ///
        /// <summary>
        /// Token used to join the channel.
        /// </summary>
        ///
        public string token;

        public ChannelMediaInfo()
        {
            this.uid = 0;
            this.channelName = "";
            this.token = "";
        }

        public ChannelMediaInfo(string c, string t, uint u)
        {
            this.uid = u;
            this.channelName = c;
            this.token = t;
        }

    }

    ///
    /// <summary>
    /// Configuration information for media stream relay across channels.
    /// </summary>
    ///
    public class ChannelMediaRelayConfiguration
    {
        ///
        /// <summary>
        /// Source channel information ChannelMediaInfo, includes the following members: channelName : Name of the source channel. Default is NULL, which means the SDK fills in the current channel name. token : The token used to join the source channel. It is generated based on the channelName and uid you set in srcInfo.
        ///  If App Certificate is not enabled, you can set this parameter to the default value NULL, which means the SDK fills in the App ID.
        ///  If App Certificate is enabled, you must provide a token generated using channelName and uid, and the uid must be 0. uid : UID identifying the media stream being relayed in the source channel. Default is 0. Do not modify.
        /// </summary>
        ///
        public ChannelMediaInfo srcInfo;

        ///
        /// <summary>
        /// Because the expiration of the token in any target channel will cause all cross-channel streaming to stop, it is recommended that you set the same expiration duration for the tokens in all target channels. Target channel information ChannelMediaInfo, includes the following members: channelName : Name of the target channel. token : The token used to join the target channel. It is generated based on the channelName and uid you set in destInfos.
        ///  If App Certificate is not enabled, you can set this parameter to the default value NULL, which means the SDK fills in the App ID.
        ///  If App Certificate is enabled, you must provide a token generated using channelName and uid. uid : UID identifying the media stream being relayed in the target channel. The value range is [0, 2^32 - 1]. Make sure it is different from all UIDs in the target channel. Default is 0, which means the SDK assigns a UID randomly.
        /// </summary>
        ///
        public ChannelMediaInfo[] destInfos;

        ///
        /// <summary>
        /// Number of target channels. Default is 0. Value range is [0,6]. This parameter should match the number of ChannelMediaInfo objects you define in destInfos.
        /// </summary>
        ///
        public int destCount;

        public ChannelMediaRelayConfiguration()
        {
            this.srcInfo = new ChannelMediaInfo();
            this.destInfos = new ChannelMediaInfo[0];
            this.destCount = 0;
        }

        public ChannelMediaRelayConfiguration(ChannelMediaInfo srcInfo, ChannelMediaInfo[] destInfos, int destCount)
        {
            this.srcInfo = srcInfo;
            this.destInfos = destInfos;
            this.destCount = destCount;
        }
    }

    ///
    /// <summary>
    /// Uplink network information.
    /// </summary>
    ///
    public class UplinkNetworkInfo
    {
        ///
        /// <summary>
        /// Target bitrate (bps) of the video encoder.
        /// </summary>
        ///
        public int video_encoder_target_bitrate_bps;

        public UplinkNetworkInfo()
        {
            this.video_encoder_target_bitrate_bps = 0;
        }

        public UplinkNetworkInfo(int video_encoder_target_bitrate_bps)
        {
            this.video_encoder_target_bitrate_bps = video_encoder_target_bitrate_bps;
        }
    }

    ///
    /// @ignore
    ///
    public class PeerDownlinkInfo
    {
        ///
        /// @ignore
        ///
        public string userId;

        ///
        /// @ignore
        ///
        public VIDEO_STREAM_TYPE stream_type;

        ///
        /// @ignore
        ///
        public REMOTE_VIDEO_DOWNSCALE_LEVEL current_downscale_level;

        ///
        /// @ignore
        ///
        public int expected_bitrate_bps;

        public PeerDownlinkInfo()
        {
            this.userId = "";
            this.stream_type = VIDEO_STREAM_TYPE.VIDEO_STREAM_HIGH;
            this.current_downscale_level = REMOTE_VIDEO_DOWNSCALE_LEVEL.REMOTE_VIDEO_DOWNSCALE_LEVEL_NONE;
            this.expected_bitrate_bps = -1;
        }

        public PeerDownlinkInfo(PeerDownlinkInfo rhs)
        {
            this.stream_type = rhs.stream_type;
            this.current_downscale_level = rhs.current_downscale_level;
            this.expected_bitrate_bps = rhs.expected_bitrate_bps;
        }

        public PeerDownlinkInfo(string userId, VIDEO_STREAM_TYPE stream_type, REMOTE_VIDEO_DOWNSCALE_LEVEL current_downscale_level, int expected_bitrate_bps)
        {
            this.userId = userId;
            this.stream_type = stream_type;
            this.current_downscale_level = current_downscale_level;
            this.expected_bitrate_bps = expected_bitrate_bps;
        }
    }

    ///
    /// <summary>
    /// Built-in encryption modes.
    /// 
    /// It is recommended to use AES_128_GCM2 or AES_256_GCM2 encryption modes. These modes support salt, providing higher security.
    /// </summary>
    ///
    public enum ENCRYPTION_MODE
    {
        ///
        /// <summary>
        /// 1: 128-bit AES encryption, XTS mode.
        /// </summary>
        ///
        AES_128_XTS = 1,

        ///
        /// <summary>
        /// 2: 128-bit AES encryption, ECB mode.
        /// </summary>
        ///
        AES_128_ECB = 2,

        ///
        /// <summary>
        /// 3: 256-bit AES encryption, XTS mode.
        /// </summary>
        ///
        AES_256_XTS = 3,

        ///
        /// <summary>
        /// 4: 128-bit SM4 encryption, ECB mode.
        /// </summary>
        ///
        SM4_128_ECB = 4,

        ///
        /// <summary>
        /// 5: 128-bit AES encryption, GCM mode.
        /// </summary>
        ///
        AES_128_GCM = 5,

        ///
        /// <summary>
        /// 6: 256-bit AES encryption, GCM mode.
        /// </summary>
        ///
        AES_256_GCM = 6,

        ///
        /// <summary>
        /// 7: (Default) 128-bit AES encryption, GCM mode. This encryption mode requires setting a salt (encryptionKdfSalt).
        /// </summary>
        ///
        AES_128_GCM2 = 7,

        ///
        /// <summary>
        /// 8: 256-bit AES encryption, GCM mode. This encryption mode requires setting a salt (encryptionKdfSalt).
        /// </summary>
        ///
        AES_256_GCM2 = 8,

        ///
        /// <summary>
        /// Enum boundary value.
        /// </summary>
        ///
        MODE_END,

    }

    ///
    /// <summary>
    /// Error types for built-in encryption.
    /// </summary>
    ///
    public enum ENCRYPTION_ERROR_TYPE
    {
        ///
        /// <summary>
        /// 0: Internal error.
        /// </summary>
        ///
        ENCRYPTION_ERROR_INTERNAL_FAILURE = 0,

        ///
        /// <summary>
        /// 1: Media stream decryption error. Make sure the encryption mode or key used by the receiver and sender are the same.
        /// </summary>
        ///
        ENCRYPTION_ERROR_DECRYPTION_FAILURE = 1,

        ///
        /// <summary>
        /// 2: Media stream encryption error.
        /// </summary>
        ///
        ENCRYPTION_ERROR_ENCRYPTION_FAILURE = 2,

        ///
        /// <summary>
        /// 3: Data stream decryption error. Make sure the encryption mode or key used by the receiver and sender are the same.
        /// </summary>
        ///
        ENCRYPTION_ERROR_DATASTREAM_DECRYPTION_FAILURE = 3,

        ///
        /// <summary>
        /// 4: Data stream encryption error.
        /// </summary>
        ///
        ENCRYPTION_ERROR_DATASTREAM_ENCRYPTION_FAILURE = 4,

    }

    ///
    /// @ignore
    ///
    public enum UPLOAD_ERROR_REASON
    {
        ///
        /// @ignore
        ///
        UPLOAD_SUCCESS = 0,

        ///
        /// @ignore
        ///
        UPLOAD_NET_ERROR = 1,

        ///
        /// @ignore
        ///
        UPLOAD_SERVER_ERROR = 2,

    }

    ///
    /// <summary>
    /// Error codes after calling renewToken.
    /// 
    /// Since Available since 4.6.0.
    /// </summary>
    ///
    public enum RENEW_TOKEN_ERROR_CODE
    {
        ///
        /// <summary>
        /// (0): Token updated successfully.
        /// </summary>
        ///
        RENEW_TOKEN_SUCCESS = 0,

        ///
        /// <summary>
        /// (1): Token update failed due to an unknown server error. It is recommended to check the parameters used to generate the Token, regenerate the Token, and retry renewToken.
        /// </summary>
        ///
        RENEW_TOKEN_FAILURE = 1,

        ///
        /// <summary>
        /// (2): Token update failed because the provided Token has expired. It is recommended to generate a new Token with a longer expiration time and retry renewToken.
        /// </summary>
        ///
        RENEW_TOKEN_TOKEN_EXPIRED = 2,

        ///
        /// <summary>
        /// (3): Token update failed because the provided Token is invalid. Common causes include: the project has enabled App Certificate in the Agora Console but Token is not used when joining the channel; the uid specified in joinChannel is inconsistent with the one used to generate the Token; the channel name specified in joinChannel is inconsistent with the one used to generate the Token. It is recommended to check the Token generation process, regenerate the Token, and retry renewToken.
        /// </summary>
        ///
        RENEW_TOKEN_INVALID_TOKEN = 3,

        ///
        /// <summary>
        /// (4): Token update failed because the channel name in the Token is inconsistent with the current channel. It is recommended to check the channel name, regenerate the Token, and retry renewToken.
        /// </summary>
        ///
        RENEW_TOKEN_INVALID_CHANNEL_NAME = 4,

        ///
        /// <summary>
        /// (5): Token update failed because the App ID in the Token is inconsistent with the current App ID. It is recommended to check the App ID, regenerate the Token, and retry renewToken.
        /// </summary>
        ///
        RENEW_TOKEN_INCONSISTENT_APPID = 5,

        ///
        /// <summary>
        /// (6): The previous Token update request was canceled due to a new request being initiated.
        /// </summary>
        ///
        RENEW_TOKEN_CANCELED_BY_NEW_REQUEST = 6,

    }

    ///
    /// <summary>
    /// Device permission types.
    /// </summary>
    ///
    public enum PERMISSION_TYPE
    {
        ///
        /// <summary>
        /// 0: Permission for audio capture device.
        /// </summary>
        ///
        RECORD_AUDIO = 0,

        ///
        /// <summary>
        /// 1: Camera permission.
        /// </summary>
        ///
        CAMERA = 1,

        ///
        /// <summary>
        /// (Android only) 2: Screen sharing permission.
        /// </summary>
        ///
        SCREEN_CAPTURE = 2,

    }

    ///
    /// <summary>
    /// Subscribe state.
    /// </summary>
    ///
    public enum STREAM_SUBSCRIBE_STATE
    {
        ///
        /// <summary>
        /// 0: Initial subscribe state after joining the channel.
        /// </summary>
        ///
        SUB_STATE_IDLE = 0,

        ///
        /// <summary>
        /// 1: Subscription failed. Possible reasons:
        ///  Remote user:
        ///  Called MuteLocalAudioStream (true) or MuteLocalVideoStream (true) to stop sending local media streams.
        ///  Called DisableAudio or DisableVideo to disable the local audio or video module.
        ///  Called EnableLocalAudio (false) or EnableLocalVideo (false) to disable local audio or video capture.
        ///  User role is audience.
        ///  Local user called the following methods to stop receiving remote media streams:
        ///  Called MuteRemoteAudioStream (true), MuteAllRemoteAudioStreams (true) to stop receiving remote audio streams.
        ///  Called MuteRemoteVideoStream (true), MuteAllRemoteVideoStreams (true) to stop receiving remote video streams.
        /// </summary>
        ///
        SUB_STATE_NO_SUBSCRIBED = 1,

        ///
        /// <summary>
        /// 2: Subscribing.
        /// </summary>
        ///
        SUB_STATE_SUBSCRIBING = 2,

        ///
        /// <summary>
        /// 3: Remote stream received, subscription succeeded.
        /// </summary>
        ///
        SUB_STATE_SUBSCRIBED = 3,

    }

    ///
    /// <summary>
    /// Publish state.
    /// </summary>
    ///
    public enum STREAM_PUBLISH_STATE
    {
        ///
        /// <summary>
        /// 0: Initial publish state after joining the channel.
        /// </summary>
        ///
        PUB_STATE_IDLE = 0,

        ///
        /// <summary>
        /// 1: Publish failed. Possible reasons:
        ///  The local user called MuteLocalAudioStream (true) or MuteLocalVideoStream (true) to stop sending local media streams.
        ///  The local user called DisableAudio or DisableVideo to disable the local audio or video module.
        ///  The local user called EnableLocalAudio (false) or EnableLocalVideo (false) to disable local audio or video capture.
        ///  The local user's role is audience.
        /// </summary>
        ///
        PUB_STATE_NO_PUBLISHED = 1,

        ///
        /// <summary>
        /// 2: Publishing.
        /// </summary>
        ///
        PUB_STATE_PUBLISHING = 2,

        ///
        /// <summary>
        /// 3: Publish succeeded.
        /// </summary>
        ///
        PUB_STATE_PUBLISHED = 3,

    }

    ///
    /// <summary>
    /// Configuration for audio and video loopback test.
    /// </summary>
    ///
    public class EchoTestConfiguration
    {
        ///
        /// <summary>
        /// The view used to render the local user's video. This parameter applies only to scenarios where video device testing is needed. Make sure enableVideo is set to true.
        /// </summary>
        ///
        public view_t view;

        ///
        /// <summary>
        /// Whether to enable audio device: true : (default) Enables the audio device. Set to true to test audio devices. false : Disables the audio device.
        /// </summary>
        ///
        public bool enableAudio;

        ///
        /// <summary>
        /// Whether to enable video device. Video device detection is not supported yet. Set this parameter to false.
        /// </summary>
        ///
        public bool enableVideo;

        ///
        /// <summary>
        /// The token used to ensure the security of the audio and video loopback test. If you have not enabled the App Certificate in the console, you do not need to provide a value for this parameter. If you have enabled the App Certificate in the console, you must provide a token for this parameter, and the uid used when generating the token must be 0xFFFFFFFF. The channel name used must uniquely identify each audio and video loopback test. For how to generate tokens on the server, see [Use Token Authentication](https://doc.shengwang.cn/doc/rtc/unity/basic-features/token-authentication).
        /// </summary>
        ///
        public string token;

        ///
        /// <summary>
        /// The channel name that identifies each audio and video loopback test. To ensure the loopback test functions correctly, the channel names passed in by different terminal users under the same project (App ID) on different devices must be unique.
        /// </summary>
        ///
        public string channelId;

        ///
        /// <summary>
        /// Sets the interval or delay for returning the audio and video loopback test results. The value range is [2,10] seconds, with a default of 2 seconds.
        ///  For audio loopback tests, the results are returned based on the interval you set.
        ///  For video loopback tests, the video will display briefly, then the delay gradually increases until it reaches the set delay.
        /// </summary>
        ///
        public int intervalInSeconds;

        public EchoTestConfiguration(view_t v, bool ea, bool ev, string t, string c, int @is)
        {
            this.view = v;
            this.enableAudio = ea;
            this.enableVideo = ev;
            this.token = t;
            this.channelId = c;
            this.intervalInSeconds = @is;
        }

        public EchoTestConfiguration()
        {
            this.view = 0;
            this.enableAudio = true;
            this.enableVideo = true;
            this.token = "";
            this.channelId = "";
            this.intervalInSeconds = 2;
        }

    }

    ///
    /// <summary>
    /// Ear monitoring audio filter type.
    /// </summary>
    ///
    public enum EAR_MONITORING_FILTER_TYPE
    {
        ///
        /// <summary>
        /// 1<<0: Do not add audio filters in ear monitoring.
        /// </summary>
        ///
        EAR_MONITORING_FILTER_NONE = (1 << 0),

        ///
        /// <summary>
        /// 1<<1: Add vocal effect audio filters in ear monitoring. If you implement features like voice beautification or sound effects, users can hear the processed sound in ear monitoring.
        /// </summary>
        ///
        EAR_MONITORING_FILTER_BUILT_IN_AUDIO_FILTERS = (1 << 1),

        ///
        /// <summary>
        /// 1<<2: Add noise suppression audio filters in ear monitoring.
        /// </summary>
        ///
        EAR_MONITORING_FILTER_NOISE_SUPPRESSION = (1 << 2),

        ///
        /// <summary>
        /// 1<<15: Reuse audio filters that have already been applied on the sending side. Reusing filters reduces CPU usage for ear monitoring but increases latency, suitable for scenarios where lower CPU usage is preferred and latency is not critical.
        /// </summary>
        ///
        EAR_MONITORING_FILTER_REUSE_POST_PROCESSING_FILTER = (1 << 15),

    }

    ///
    /// @ignore
    ///
    public enum THREAD_PRIORITY_TYPE
    {
        ///
        /// @ignore
        ///
        LOWEST = 0,

        ///
        /// @ignore
        ///
        LOW = 1,

        ///
        /// @ignore
        ///
        NORMAL = 2,

        ///
        /// @ignore
        ///
        HIGH = 3,

        ///
        /// @ignore
        ///
        HIGHEST = 4,

        ///
        /// @ignore
        ///
        CRITICAL = 5,

    }

    ///
    /// <summary>
    /// Video encoding configuration for the shared screen stream.
    /// </summary>
    ///
    public class ScreenVideoParameters
    {
        ///
        /// <summary>
        /// Video encoding resolution. Default is 1280  720.
        /// </summary>
        ///
        public VideoDimensions dimensions;

        ///
        /// <summary>
        /// Video encoding frame rate (fps). Default is 15.
        /// </summary>
        ///
        public int frameRate;

        ///
        /// <summary>
        /// Video encoding bitrate (Kbps).
        /// </summary>
        ///
        public int bitrate;

        ///
        /// <summary>
        /// Content type of the screen sharing video.
        /// </summary>
        ///
        public VIDEO_CONTENT_HINT contentHint;

        public ScreenVideoParameters()
        {
            this.dimensions = new VideoDimensions(1280, 720);
        }

        public ScreenVideoParameters(VideoDimensions dimensions, int frameRate, int bitrate, VIDEO_CONTENT_HINT contentHint)
        {
            this.dimensions = dimensions;
            this.frameRate = frameRate;
            this.bitrate = bitrate;
            this.contentHint = contentHint;
        }
    }

    ///
    /// <summary>
    /// Parameter configuration for screen sharing.
    /// </summary>
    ///
    public class ScreenCaptureParameters2
    {
        ///
        /// <summary>
        /// Due to system limitations, capturing system audio is only supported on Android API level 29 and above, i.e., Android 10 and above.
        ///  To improve the success rate of capturing system audio during screen sharing, make sure you have called the SetAudioScenario method and set the audio scenario to AUDIO_SCENARIO_GAME_STREAMING. Whether to capture system audio during screen sharing: true : Capture system audio. false : (Default) Do not capture system audio.
        /// </summary>
        ///
        public bool captureAudio;

        ///
        /// <summary>
        /// Audio configuration for the shared screen stream. See ScreenAudioParameters. This parameter takes effect only when captureAudio is set to true.
        /// </summary>
        ///
        public ScreenAudioParameters audioParams;

        ///
        /// <summary>
        /// Due to system limitations, screen capture is only supported on Android API level 21 and above, i.e., Android 5 and above. Whether to capture the screen during screen sharing: true : (Default) Capture the screen. false : Do not capture the screen.
        /// </summary>
        ///
        public bool captureVideo;

        ///
        /// <summary>
        /// Video encoding configuration for the shared screen stream. See ScreenVideoParameters. This parameter takes effect only when captureVideo is set to true.
        /// </summary>
        ///
        public ScreenVideoParameters videoParams;

        public ScreenCaptureParameters2(bool captureAudio, ScreenAudioParameters audioParams, bool captureVideo, ScreenVideoParameters videoParams)
        {
            this.captureAudio = captureAudio;
            this.audioParams = audioParams;
            this.captureVideo = captureVideo;
            this.videoParams = videoParams;
        }
        public ScreenCaptureParameters2()
        {
        }

    }

    ///
    /// <summary>
    /// Rendering status of media frames.
    /// </summary>
    ///
    public enum MEDIA_TRACE_EVENT
    {
        ///
        /// <summary>
        /// 0: Video frame has been rendered.
        /// </summary>
        ///
        MEDIA_TRACE_EVENT_VIDEO_RENDERED = 0,

        ///
        /// <summary>
        /// 1: Video frame has been decoded.
        /// </summary>
        ///
        MEDIA_TRACE_EVENT_VIDEO_DECODED,

    }

    ///
    /// <summary>
    /// Metrics during the video frame rendering process.
    /// </summary>
    ///
    public class VideoRenderingTracingInfo
    {
        ///
        /// <summary>
        /// The time interval (ms) from calling StartMediaRenderingTracing to triggering the OnVideoRenderingTracingResult callback. It is recommended to call StartMediaRenderingTracing before joining the channel.
        /// </summary>
        ///
        public int elapsedTime;

        ///
        /// <summary>
        /// The time interval (ms) from calling StartMediaRenderingTracing to calling JoinChannel [1/2] or JoinChannel [2/2]. A negative value indicates that StartMediaRenderingTracing was called after JoinChannel [2/2].
        /// </summary>
        ///
        public int start2JoinChannel;

        ///
        /// <summary>
        /// The time interval (ms) from calling JoinChannel [1/2] or JoinChannel [2/2] to successfully joining the channel.
        /// </summary>
        ///
        public int join2JoinSuccess;

        ///
        /// <summary>
        /// If the local user calls StartMediaRenderingTracing after the remote user has already joined the channel, this value is 0 and has no reference value.
        ///  To improve the rendering speed of the remote user, it is recommended that the local user join the channel after the remote user has joined, to reduce this value.
        ///  If the local user calls StartMediaRenderingTracing before successfully joining the channel, this value is the time interval (ms) from the local user successfully joining the channel to the remote user joining the channel.
        ///  If the local user calls StartMediaRenderingTracing after successfully joining the channel, this value is the time interval (ms) from calling StartMediaRenderingTracing to the remote user joining the channel.
        /// </summary>
        ///
        public int joinSuccess2RemoteJoined;

        ///
        /// <summary>
        /// If the local user calls StartMediaRenderingTracing after setting the remote view, this value is 0 and has no reference value.
        ///  To improve the rendering speed of the remote user, it is recommended to set the remote view before the remote user joins the channel, or immediately after the remote user joins the channel, to reduce this value.
        ///  If the local user calls StartMediaRenderingTracing before the remote user joins the channel, this value is the time interval (ms) from the remote user joining the channel to the local user setting the remote view.
        ///  If the local user calls StartMediaRenderingTracing after the remote user joins the channel, this value is the time interval (ms) from calling StartMediaRenderingTracing to setting the remote view.
        /// </summary>
        ///
        public int remoteJoined2SetView;

        ///
        /// <summary>
        /// If StartMediaRenderingTracing is called after subscribing to the remote video stream, this value is 0 and has no reference value.
        ///  To improve the rendering speed of the remote user, it is recommended that the local user subscribe to the remote video stream immediately after the remote user joins the channel, to reduce this value.
        ///  If the local user calls StartMediaRenderingTracing before the remote user joins the channel, this value is the time interval (ms) from the remote user joining the channel to subscribing to the remote video stream.
        ///  If the local user calls StartMediaRenderingTracing after the remote user joins the channel, this value is the time interval (ms) from calling StartMediaRenderingTracing to subscribing to the remote video stream.
        /// </summary>
        ///
        public int remoteJoined2UnmuteVideo;

        ///
        /// <summary>
        /// If StartMediaRenderingTracing is called after receiving the remote video stream, this value is 0 and has no reference value.
        ///  To improve the rendering speed of the remote user, it is recommended that the remote user publish the video stream immediately after joining the channel, and the local user subscribe to the remote video stream immediately, to reduce this value.
        ///  If the local user calls StartMediaRenderingTracing before the remote user joins the channel, this value is the time interval (ms) from the remote user joining the channel to the local user receiving the first remote data packet.
        ///  If the local user calls StartMediaRenderingTracing after the remote user joins the channel, this value is the time interval (ms) from calling StartMediaRenderingTracing to receiving the first remote data packet.
        /// </summary>
        ///
        public int remoteJoined2PacketReceived;

        public VideoRenderingTracingInfo(int elapsedTime, int start2JoinChannel, int join2JoinSuccess, int joinSuccess2RemoteJoined, int remoteJoined2SetView, int remoteJoined2UnmuteVideo, int remoteJoined2PacketReceived)
        {
            this.elapsedTime = elapsedTime;
            this.start2JoinChannel = start2JoinChannel;
            this.join2JoinSuccess = join2JoinSuccess;
            this.joinSuccess2RemoteJoined = joinSuccess2RemoteJoined;
            this.remoteJoined2SetView = remoteJoined2SetView;
            this.remoteJoined2UnmuteVideo = remoteJoined2UnmuteVideo;
            this.remoteJoined2PacketReceived = remoteJoined2PacketReceived;
        }
        public VideoRenderingTracingInfo()
        {
        }

    }

    ///
    /// @ignore
    ///
    public enum CONFIG_FETCH_TYPE
    {
        ///
        /// @ignore
        ///
        CONFIG_FETCH_TYPE_INITIALIZE = 1,

        ///
        /// @ignore
        ///
        CONFIG_FETCH_TYPE_JOIN_CHANNEL = 2,

    }

    ///
    /// @ignore
    ///
    public enum LOCAL_PROXY_MODE
    {
        ///
        /// @ignore
        ///
        ConnectivityFirst = 0,

        ///
        /// @ignore
        ///
        LocalOnly = 1,

    }

    ///
    /// <summary>
    /// Configuration information of the log server.
    /// </summary>
    ///
    public class LogUploadServerInfo
    {
        ///
        /// <summary>
        /// Domain name of the log server.
        /// </summary>
        ///
        public string serverDomain;

        ///
        /// <summary>
        /// Storage path of logs on the server.
        /// </summary>
        ///
        public string serverPath;

        ///
        /// <summary>
        /// Port of the log server.
        /// </summary>
        ///
        public int serverPort;

        ///
        /// <summary>
        /// Whether the log server uses HTTPS: true : Uses HTTPS protocol. false : Uses HTTP protocol.
        /// </summary>
        ///
        public bool serverHttps;

        public LogUploadServerInfo()
        {
            this.serverDomain = "";
            this.serverPath = "";
            this.serverPort = 0;
            this.serverHttps = true;
        }

        public LogUploadServerInfo(string domain, string path, int port, bool https)
        {
            this.serverDomain = domain;
            this.serverPath = path;
            this.serverPort = port;
            this.serverHttps = https;
        }

    }

    ///
    /// <summary>
    /// Advanced options for Local Access Point.
    /// </summary>
    ///
    public class AdvancedConfigInfo
    {
        ///
        /// <summary>
        /// Custom log upload server. By default, the SDK uploads logs to the Agora log server. You can modify the log upload server using this parameter. See LogUploadServerInfo.
        /// </summary>
        ///
        public LogUploadServerInfo logUploadServer;

        public AdvancedConfigInfo(LogUploadServerInfo logUploadServer)
        {
            this.logUploadServer = logUploadServer;
        }
        public AdvancedConfigInfo()
        {
        }

    }

    ///
    /// <summary>
    /// Local Access Point configuration.
    /// </summary>
    ///
    public class LocalAccessPointConfiguration
    {
        ///
        /// <summary>
        /// Internal IP address list of the Local Access Point. Either ipList or domainList must be provided.
        /// </summary>
        ///
        public string[] ipList;

        ///
        /// <summary>
        /// Number of internal IP addresses of the Local Access Point. This value must match the number of IP addresses you provide.
        /// </summary>
        ///
        public int ipListSize;

        ///
        /// <summary>
        /// Domain name list of the Local Access Point. The SDK resolves the IP addresses of the Local Access Point based on the provided domain names. The domain name resolution timeout is 10 seconds. Either ipList or domainList must be provided. If you specify both IP addresses and domain names, the SDK merges and deduplicates the resolved IP addresses and the specified IP addresses, then randomly connects to one IP to achieve load balancing.
        /// </summary>
        ///
        public string[] domainList;

        ///
        /// <summary>
        /// Number of domain names of the Local Access Point. This value must match the number of domain names you provide.
        /// </summary>
        ///
        public int domainListSize;

        ///
        /// <summary>
        /// Domain name for internal certificate verification. If left empty, the SDK uses the default certificate verification domain name secure-edge.local.
        /// </summary>
        ///
        public string verifyDomainName;

        ///
        /// <summary>
        /// Connection mode. See LOCAL_PROXY_MODE.
        /// </summary>
        ///
        public LOCAL_PROXY_MODE mode;

        ///
        /// <summary>
        /// Advanced options for the Local Access Point. See AdvancedConfigInfo.
        /// </summary>
        ///
        public AdvancedConfigInfo advancedConfig;

        ///
        /// @ignore
        ///
        public bool disableAut;

        public LocalAccessPointConfiguration()
        {
            this.ipList = new string[0];
            this.ipListSize = 0;
            this.domainList = new string[0];
            this.domainListSize = 0;
            this.verifyDomainName = "";
            this.mode = LOCAL_PROXY_MODE.ConnectivityFirst;
            this.disableAut = true;
        }

        public LocalAccessPointConfiguration(string[] ipList, int ipListSize, string[] domainList, int domainListSize, string verifyDomainName, LOCAL_PROXY_MODE mode, AdvancedConfigInfo advancedConfig, bool disableAut)
        {
            this.ipList = ipList;
            this.ipListSize = ipListSize;
            this.domainList = domainList;
            this.domainListSize = domainListSize;
            this.verifyDomainName = verifyDomainName;
            this.mode = mode;
            this.advancedConfig = advancedConfig;
            this.disableAut = disableAut;
        }
    }

    ///
    /// @ignore
    ///
    public enum RecorderStreamType
    {
        ///
        /// @ignore
        ///
        RTC,

        ///
        /// @ignore
        ///
        PREVIEW,

    }

    ///
    /// @ignore
    ///
    public class RecorderStreamInfo
    {
        ///
        /// @ignore
        ///
        public string channelId;

        ///
        /// @ignore
        ///
        public uint uid;

        ///
        /// @ignore
        ///
        public RecorderStreamType type;

        public RecorderStreamInfo()
        {
            this.channelId = "";
            this.uid = 0;
            this.type = RecorderStreamType.RTC;
        }

        public RecorderStreamInfo(string channelId, uint uid)
        {
            this.channelId = channelId;
            this.uid = uid;
            this.type = RecorderStreamType.RTC;
        }

        public RecorderStreamInfo(string channelId, uint uid, RecorderStreamType type)
        {
            this.channelId = channelId;
            this.uid = uid;
            this.type = type;
        }

    }

    ///
    /// @ignore
    ///
    public enum RdtStreamType
    {
        ///
        /// @ignore
        ///
        RDT_STREAM_CMD,

        ///
        /// @ignore
        ///
        RDT_STREAM_DATA,

        ///
        /// @ignore
        ///
        RDT_STREAM_COUNT,

    }

    ///
    /// @ignore
    ///
    public enum RdtState
    {
        ///
        /// @ignore
        ///
        RDT_STATE_CLOSED,

        ///
        /// @ignore
        ///
        RDT_STATE_OPENED,

        ///
        /// @ignore
        ///
        RDT_STATE_BLOCKED,

        ///
        /// @ignore
        ///
        RDT_STATE_PENDING,

        ///
        /// @ignore
        ///
        RDT_STATE_BROKEN,

    }

    ///
    /// <summary>
    /// Spatial audio parameters.
    /// </summary>
    ///
    public class SpatialAudioParams : IOptionalJsonParse
    {
        ///
        /// <summary>
        /// The horizontal angle of the remote user or media player relative to the local user. Range: [0,360], in degrees:
        ///  0: (default) 0 degrees, directly in front.
        ///  90: 90 degrees, to the left.
        ///  180: 180 degrees, behind.
        ///  270: 270 degrees, to the right.
        ///  360: 360 degrees, directly in front.
        /// </summary>
        ///
        public Optional<double> speaker_azimuth = new Optional<double>();

        ///
        /// <summary>
        /// The elevation angle of the remote user or media player relative to the local user. Range: [-90,90], in degrees:
        ///  0: (default) 0 degrees, no vertical rotation.
        ///  -90: -90 degrees, rotated downward.
        ///  90: 90 degrees, rotated upward.
        /// </summary>
        ///
        public Optional<double> speaker_elevation = new Optional<double>();

        ///
        /// <summary>
        /// The distance between the remote user or media player and the local user. Range: [1,50], in meters. Default is 1 meter.
        /// </summary>
        ///
        public Optional<double> speaker_distance = new Optional<double>();

        ///
        /// <summary>
        /// The orientation of the remote user or media player relative to the local user. Range: [0,180], in degrees:
        ///  0: (default) 0 degrees, facing the same direction.
        ///  180: 180 degrees, facing each other.
        /// </summary>
        ///
        public Optional<int> speaker_orientation = new Optional<int>();

        ///
        /// <summary>
        /// Whether to enable sound blur processing: true : Enable blur. false : (default) Disable blur.
        /// </summary>
        ///
        public Optional<bool> enable_blur = new Optional<bool>();

        ///
        /// <summary>
        /// Whether to enable air absorption, simulating timbre attenuation during sound propagation in air: high frequencies attenuate faster than low frequencies over distance. true : (default) Enable air absorption. Make sure speaker_attenuation is not 0, otherwise this setting has no effect. false : Disable air absorption.
        /// </summary>
        ///
        public Optional<bool> enable_air_absorb = new Optional<bool>();

        ///
        /// <summary>
        /// The sound attenuation coefficient of the remote user or media player. Range: [0,1]:
        ///  0: Broadcast mode, no volume or timbre attenuation with distance.
        ///  (0,0.5): Weak attenuation, slight volume and timbre attenuation (requires enable_air_absorb), allowing sound to travel farther than in real environments.
        ///  0.5: (default) Simulates real-world volume attenuation, same as not setting speaker_attenuation.
        ///  (0.5,1]: Strong attenuation, rapid volume and timbre attenuation (requires enable_air_absorb).
        /// </summary>
        ///
        public Optional<double> speaker_attenuation = new Optional<double>();

        ///
        /// <summary>
        /// This parameter applies to scenarios with fast-moving sound sources (e.g., racing games). It is not recommended for typical audio/video interaction scenarios (voice chat, co-hosting, online karaoke).
        ///  When enabled, it is recommended to update the relative distance between the source and receiver at regular intervals (e.g., every 30 ms) using UpdatePlayerPositionInfo, UpdateSelfPosition, and UpdateRemotePosition. The Doppler effect may not work as expected or may cause audio jitter if: the update interval is too long, updates are irregular, or distance info is lost due to packet loss or latency. Whether to enable Doppler effect: when there is relative motion between the sound source and receiver, the pitch changes. true : Enable Doppler effect. false : (default) Disable Doppler effect.
        /// </summary>
        ///
        public Optional<bool> enable_doppler = new Optional<bool>();

        public SpatialAudioParams(Optional<double> speaker_azimuth, Optional<double> speaker_elevation, Optional<double> speaker_distance, Optional<int> speaker_orientation, Optional<bool> enable_blur, Optional<bool> enable_air_absorb, Optional<double> speaker_attenuation, Optional<bool> enable_doppler)
        {
            this.speaker_azimuth = speaker_azimuth;
            this.speaker_elevation = speaker_elevation;
            this.speaker_distance = speaker_distance;
            this.speaker_orientation = speaker_orientation;
            this.enable_blur = enable_blur;
            this.enable_air_absorb = enable_air_absorb;
            this.speaker_attenuation = speaker_attenuation;
            this.enable_doppler = enable_doppler;
        }
        public SpatialAudioParams()
        {
        }

        ///
        /// @ignore
        ///
        public virtual void ToJson(JsonWriter writer)
        {
            writer.WriteObjectStart();

            if (this.speaker_azimuth.HasValue())
            {
                writer.WritePropertyName("speaker_azimuth");
                writer.Write(this.speaker_azimuth.GetValue());
            }

            if (this.speaker_elevation.HasValue())
            {
                writer.WritePropertyName("speaker_elevation");
                writer.Write(this.speaker_elevation.GetValue());
            }

            if (this.speaker_distance.HasValue())
            {
                writer.WritePropertyName("speaker_distance");
                writer.Write(this.speaker_distance.GetValue());
            }

            if (this.speaker_orientation.HasValue())
            {
                writer.WritePropertyName("speaker_orientation");
                writer.Write(this.speaker_orientation.GetValue());
            }

            if (this.enable_blur.HasValue())
            {
                writer.WritePropertyName("enable_blur");
                writer.Write(this.enable_blur.GetValue());
            }

            if (this.enable_air_absorb.HasValue())
            {
                writer.WritePropertyName("enable_air_absorb");
                writer.Write(this.enable_air_absorb.GetValue());
            }

            if (this.speaker_attenuation.HasValue())
            {
                writer.WritePropertyName("speaker_attenuation");
                writer.Write(this.speaker_attenuation.GetValue());
            }

            if (this.enable_doppler.HasValue())
            {
                writer.WritePropertyName("enable_doppler");
                writer.Write(this.enable_doppler.GetValue());
            }

            writer.WriteObjectEnd();
        }
    }

    ///
    /// <summary>
    /// Layout information of a sub video stream in a composite stream.
    /// </summary>
    ///
    public class VideoLayout
    {
        ///
        /// <summary>
        /// Channel name to which the sub video stream belongs.
        /// </summary>
        ///
        public string channelId;

        ///
        /// <summary>
        /// User ID that publishes the sub video stream.
        /// </summary>
        ///
        public uint uid;

        ///
        /// <summary>
        /// Reserved parameter.
        /// </summary>
        ///
        public string strUid;

        ///
        /// <summary>
        /// The x-coordinate (px) of the sub video stream on the composite canvas. That is, the horizontal offset of the top-left corner of the sub video relative to the top-left corner (origin) of the canvas.
        /// </summary>
        ///
        public uint x;

        ///
        /// <summary>
        /// The y-coordinate (px) of the sub video stream on the composite canvas. That is, the vertical offset of the top-left corner of the sub video relative to the top-left corner (origin) of the canvas.
        /// </summary>
        ///
        public uint y;

        ///
        /// <summary>
        /// Width (px) of the sub video stream.
        /// </summary>
        ///
        public uint width;

        ///
        /// <summary>
        /// Height (px) of the sub video stream.
        /// </summary>
        ///
        public uint height;

        ///
        /// <summary>
        /// State of the sub video stream on the composite canvas.
        ///  0: Normal. The video stream is rendered on the canvas.
        ///  1: Placeholder. The video stream has no video content and is displayed as a placeholder on the canvas.
        ///  2: Black image. The video stream is replaced with a black image.
        /// </summary>
        ///
        public uint videoState;

        public VideoLayout()
        {
            this.channelId = "";
            this.uid = 0;
            this.strUid = "";
            this.x = 0;
            this.y = 0;
            this.width = 0;
            this.height = 0;
            this.videoState = 0;
        }

        public VideoLayout(string channelId, uint uid, string strUid, uint x, uint y, uint width, uint height, uint videoState)
        {
            this.channelId = channelId;
            this.uid = uid;
            this.strUid = strUid;
            this.x = x;
            this.y = y;
            this.width = width;
            this.height = height;
            this.videoState = videoState;
        }
    }

}
