[
  {
    "id": "api_createagorartcengine",
    "name": "CreateAgoraRtcEngine",
    "description": "Creates one IRtcEngine object.\n\nCurrently, the Agora RTC SDK v4.x supports creating only one IRtcEngine object for each app.",
    "parameters": [],
    "returns": "One IRtcEngine object.",
    "is_hide": false
  },
  {
    "id": "api_createagorartcengineex",
    "name": "CreateAgoraRtcEngineEx",
    "description": "Creates one IRtcEngineEx object.\n\nCurrently, the Agora RTC v4.x SDK supports creating only one IRtcEngineEx object for each app.",
    "parameters": [],
    "returns": "One IRtcEngineEx object.",
    "is_hide": false
  },
  {
    "id": "api_getmediaplayercachemanager",
    "name": "GetMediaPlayerCacheManager",
    "description": "Gets one IMediaPlayerCacheManager instance.\n\nBefore calling any APIs in the IMediaPlayerCacheManager class, you need to call this method to get a cache manager instance of a media player.",
    "parameters": [],
    "returns": "The IMediaPlayerCacheManager instance.",
    "is_hide": false
  },
  {
    "id": "api_iagoraparameter_setparameters",
    "name": "SetParameters [2/2]",
    "description": "Provides the technical preview functionalities or special customizations by configuring the SDK with JSON options.\n\nContact to get the JSON configuration method.",
    "parameters": [
      {
        "parameters": "Pointer to the set parameters in a JSON string."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_enumerateplaybackdevices",
    "name": "EnumeratePlaybackDevices",
    "description": "Enumerates the audio playback devices.\n\nThis method is for Windows and macOS only.",
    "parameters": [],
    "returns": "Success: Returns a DeviceInfo array, which includes the device ID and device name of all the audio playback devices.\n Failure: NULL.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_enumeraterecordingdevices",
    "name": "EnumerateRecordingDevices",
    "description": "Enumerates the audio capture devices.\n\nThis method is for Windows and macOS only.",
    "parameters": [],
    "returns": "Success: A DeviceInfo array, which includes the device ID and device name of all the audio capture devices.\n Failure: NULL.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_followsystemloopbackdevice",
    "name": "FollowSystemLoopbackDevice",
    "description": "Sets whether the loopback device follows the system default playback device.\n\nThis method is for Windows and macOS only.",
    "parameters": [
      {
        "enable": "Whether to follow the system default audio playback device: true : Follow the system default audio playback device. When the default playback device of the system is changed, the SDK immediately switches to the loopback device. false : Do not follow the system default audio playback device. The SDK switches the audio loopback device to the system default audio playback device only when the current audio playback device is disconnected."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_followsystemplaybackdevice",
    "name": "FollowSystemPlaybackDevice",
    "description": "Sets the audio playback device used by the SDK to follow the system default audio playback device.\n\nThis method is for Windows and macOS only.",
    "parameters": [
      {
        "enable": "Whether to follow the system default audio playback device: true : Follow the system default audio playback device. The SDK immediately switches the audio playback device when the system default audio playback device changes. false : Do not follow the system default audio playback device. The SDK switches the audio playback device to the system default audio playback device only when the currently used audio playback device is disconnected."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_followsystemrecordingdevice",
    "name": "FollowSystemRecordingDevice",
    "description": "Sets the audio recording device used by the SDK to follow the system default audio recording device.\n\nThis method is for Windows and macOS only.",
    "parameters": [
      {
        "enable": "Whether to follow the system default audio recording device: true : Follow the system default audio playback device. The SDK immediately switches the audio recording device when the system default audio recording device changes. false : Do not follow the system default audio playback device. The SDK switches the audio recording device to the system default audio recording device only when the currently used audio recording device is disconnected."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getloopbackdevice",
    "name": "GetLoopbackDevice",
    "description": "Gets the current loopback device.\n\nThis method is for Windows and macOS only.",
    "parameters": [
      {
        "deviceId": "Output parameter, the ID of the current loopback device."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getplaybackdefaultdevice",
    "name": "GetPlaybackDefaultDevice [1/2]",
    "description": "Gets the default audio playback device.\n\nThis method is for Windows and macOS only.",
    "parameters": [
      {
        "deviceId": "Output parameter; indicates the ID of the default audio playback device."
      },
      {
        "deviceName": "Output parameter; indicates the name of the default audio playback device."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getplaybackdefaultdevice2",
    "name": "GetPlaybackDefaultDevice [2/2]",
    "description": "Get the system‘s default audio playback device and its type.\n\nThis method applies to macOS only.",
    "parameters": [
      {
        "deviceId": "Output parameter; indicates the ID of the default audio playback device."
      },
      {
        "deviceName": "Output parameter; indicates the name of the default audio playback device."
      },
      {
        "deviceTypeName": "Output parameter; indicates the type of audio devices, such as built-in, USB and HDMI."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getplaybackdevice",
    "name": "GetPlaybackDevice",
    "description": "Retrieves the audio playback device associated with the device ID.\n\nThis method is for Windows and macOS only.",
    "parameters": [
      {
        "deviceId": "Output parameter. The device ID of the audio playback device."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getplaybackdeviceinfo",
    "name": "GetPlaybackDeviceInfo [1/2]",
    "description": "Retrieves the information of the audio playback device.\n\nThis method is for Windows and macOS only.",
    "parameters": [
      {
        "deviceId": "Th ID of the audio playback device."
      },
      {
        "deviceName": "Output parameter; the name of the playback device."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getplaybackdeviceinfo2",
    "name": "GetPlaybackDeviceInfo [2/2]",
    "description": "Get the information and type of the audio playback device.\n\nThis method applies to macOS only.",
    "parameters": [
      {
        "deviceName": "Output parameter; the name of the playback device."
      },
      {
        "deviceId": "Th ID of the audio playback device."
      },
      {
        "deviceTypeName": "Output parameter; indicates the type of audio playback devices, such as built-in, USB and HDMI."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getplaybackdevicemute",
    "name": "GetPlaybackDeviceMute",
    "description": "Retrieves whether the audio playback device is muted.",
    "parameters": [],
    "returns": "true : The audio playback device is muted. false : The audio playback device is unmuted.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getplaybackdevicevolume",
    "name": "GetPlaybackDeviceVolume",
    "description": "Retrieves the volume of the audio playback device.",
    "parameters": [],
    "returns": "The volume of the audio playback device. The value range is [0,255].",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getrecordingdefaultdevice",
    "name": "GetRecordingDefaultDevice [1/2]",
    "description": "Gets the default audio capture device.\n\nThis method is for Windows and macOS only.",
    "parameters": [
      {
        "deviceId": "Output parameter; indicates the ID of the default audio capture device."
      },
      {
        "deviceName": "Output parameter; indicates the name of the default audio capture device."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getrecordingdefaultdevice2",
    "name": "GetRecordingDefaultDevice [2/2]",
    "description": "Gets the default audio capture device and its type.\n\nThis method applies to macOS only.",
    "parameters": [
      {
        "deviceTypeName": "Output parameter; indicates the type of audio devices, such as built-in, USB and HDMI."
      },
      {
        "deviceId": "Output parameter; indicates the ID of the default audio capture device."
      },
      {
        "deviceName": "Output parameter; indicates the name of the default audio capture device."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getrecordingdevice",
    "name": "GetRecordingDevice",
    "description": "Gets the current audio recording device.\n\nThis method is for Windows and macOS only.",
    "parameters": [
      {
        "deviceId": "An output parameter. The device ID of the recording device."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getrecordingdeviceinfo",
    "name": "GetRecordingDeviceInfo [1/2]",
    "description": "Retrieves the information of the audio recording device.\n\nThis method is for Windows and macOS only.",
    "parameters": [
      {
        "deviceId": "Th ID of the audio playback device."
      },
      {
        "deviceName": "Output parameter; the name of the playback device."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getrecordingdeviceinfo2",
    "name": "GetRecordingDeviceInfo [2/2]",
    "description": "Get the information and type of the audio capturing device.\n\nThis method applies to macOS only.",
    "parameters": [
      {
        "deviceName": "Output parameter; the name of the playback device."
      },
      {
        "deviceId": "Th ID of the audio playback device."
      },
      {
        "deviceTypeName": "Output parameter; indicates the type of audio capturing devices, such as built-in, USB and HDMI."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getrecordingdevicemute",
    "name": "GetRecordingDeviceMute",
    "description": "Gets whether the audio capture device is muted.",
    "parameters": [],
    "returns": "true : The microphone is muted. false : The microphone is unmuted.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_getrecordingdevicevolume",
    "name": "GetRecordingDeviceVolume",
    "description": "Retrieves the volume of the audio recording device.\n\nThis method applies to Windows only.",
    "parameters": [],
    "returns": "The volume of the audio recording device. The value range is [0,255].",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_setloopbackdevice",
    "name": "SetLoopbackDevice",
    "description": "Sets the loopback device.\n\nThe SDK uses the current playback device as the loopback device by default. If you want to specify another audio device as the loopback device, call this method, and set deviceId to the loopback device you want to specify. You can call this method to change the audio route currently being used, but this does not change the default audio route. For example, if the default audio route is microphone, you call this method to set the audio route as a sound card before joinging a channel and then start a device test, the SDK conducts device test on the sound card. After the device test is completed and you join a channel, the SDK still uses the microphone for audio capturing. This method is for Windows and macOS only. The scenarios where this method is applicable are as follows: Use app A to play music through a Bluetooth headset; when using app B for a video conference, play through the speakers.\n If the loopback device is set as the Bluetooth headset, the SDK publishes the music in app A to the remote end.\n If the loopback device is set as the speaker, the SDK does not publish the music in app A to the remote end.\n If you set the loopback device as the Bluetooth headset, and then use a wired headset to play the music in app A, you need to call this method again, set the loopback device as the wired headset, and the SDK continues to publish the music in app A to remote end.",
    "parameters": [
      {
        "deviceId": "Specifies the loopback device of the SDK. You can get the device ID by calling EnumeratePlaybackDevices. Connecting or disconnecting the audio device does not change the value of deviceId."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_setplaybackdevice",
    "name": "SetPlaybackDevice",
    "description": "Sets the audio playback device.\n\nThis method is for Windows and macOS only. You can call this method to change the audio route currently being used, but this does not change the default audio route. For example, if the default audio route is speaker 1, you call this method to set the audio route as speaker 2 before joinging a channel and then start a device test, the SDK conducts device test on speaker 2. After the device test is completed and you join a channel, the SDK still uses speaker 1, the default audio route.",
    "parameters": [
      {
        "deviceId": "The ID of the specified audio playback device. You can get the device ID by calling EnumeratePlaybackDevices. Connecting or disconnecting the audio device does not change the value of deviceId."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_setplaybackdevicemute",
    "name": "SetPlaybackDeviceMute",
    "description": "Mutes the audio playback device.",
    "parameters": [
      {
        "mute": "Whether to mute the audio playback device: true : Mute the audio playback device. false : Unmute the audio playback device."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_setplaybackdevicevolume",
    "name": "SetPlaybackDeviceVolume",
    "description": "Sets the volume of the audio playback device.\n\nThis method applies to Windows only.",
    "parameters": [
      {
        "volume": "The volume of the audio playback device. The value range is [0,255]."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_setrecordingdevice",
    "name": "SetRecordingDevice",
    "description": "Sets the audio capture device.\n\nThis method is for Windows and macOS only. You can call this method to change the audio route currently being used, but this does not change the default audio route. For example, if the default audio route is microphone, you call this method to set the audio route as bluetooth earphones before joinging a channel and then start a device test, the SDK conducts device test on the bluetooth earphones. After the device test is completed and you join a channel, the SDK still uses the microphone for audio capturing.",
    "parameters": [
      {
        "deviceId": "The ID of the audio capture device. You can get the Device ID by calling EnumerateRecordingDevices. Connecting or disconnecting the audio device does not change the value of deviceId."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_setrecordingdevicemute",
    "name": "SetRecordingDeviceMute",
    "description": "Sets the mute status of the audio capture device.",
    "parameters": [
      {
        "mute": "Whether to mute the audio recording device: true : Mute the audio capture device. false : Unmute the audio capture device."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_setrecordingdevicevolume",
    "name": "SetRecordingDeviceVolume",
    "description": "Sets the volume of the audio capture device.\n\nThis method is for Windows and macOS only.",
    "parameters": [
      {
        "volume": "The volume of the audio recording device. The value range is [0,255]. 0 means no sound, 255 means maximum volume."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_startaudiodeviceloopbacktest",
    "name": "StartAudioDeviceLoopbackTest",
    "description": "Starts an audio device loopback test.\n\nThis method tests whether the local audio capture device and playback device are working properly. After starting the test, the audio capture device records the local audio, and the audio playback device plays the captured audio. The SDK triggers two independent OnAudioVolumeIndication callbacks at the time interval set in this method, which reports the volume information of the capture device (uid = 0) and the volume information of the playback device (uid = 1) respectively.\n This method is for Windows and macOS only.\n You can call this method either before or after joining a channel.\n This method only takes effect when called by the host.\n This method tests local audio devices and does not report the network conditions.\n When you finished testing, call StopAudioDeviceLoopbackTest to stop the audio device loopback test.",
    "parameters": [
      {
        "indicationInterval": "The time interval (ms) at which the SDK triggers the OnAudioVolumeIndication callback. Agora recommends setting a value greater than 200 ms. This value must not be less than 10 ms; otherwise, you can not receive the OnAudioVolumeIndication callback."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_startplaybackdevicetest",
    "name": "StartPlaybackDeviceTest",
    "description": "Starts the audio playback device test.\n\nThis method tests whether the audio device for local playback works properly. Once a user starts the test, the SDK plays an audio file specified by the user. If the user can hear the audio, the playback device works properly. After calling this method, the SDK triggers the OnAudioVolumeIndication callback every 100 ms, reporting uid = 1 and the volume information of the playback device. The difference between this method and the StartEchoTest method is that the former checks if the local audio playback device is working properly, while the latter can check the audio and video devices and network conditions. Call this method before joining a channel. After the test is completed, call StopPlaybackDeviceTest to stop the test before joining a channel.",
    "parameters": [
      {
        "testAudioFilePath": "The path of the audio file. The data format is string in UTF-8.\n Supported file formats: wav, mp3, m4a, and aac.\n Supported file sample rates: 8000, 16000, 32000, 44100, and 48000 Hz."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_startrecordingdevicetest",
    "name": "StartRecordingDeviceTest",
    "description": "Starts the audio capturing device test.\n\nThis method tests whether the audio capturing device works properly. After calling this method, the SDK triggers the OnAudioVolumeIndication callback at the time interval set in this method, which reports uid = 0 and the volume information of the capturing device. The difference between this method and the StartEchoTest method is that the former checks if the local audio capturing device is working properly, while the latter can check the audio and video devices and network conditions. Call this method before joining a channel. After the test is completed, call StopRecordingDeviceTest to stop the test before joining a channel.",
    "parameters": [
      {
        "indicationInterval": "The interval (ms) for triggering the OnAudioVolumeIndication callback. This value should be set to greater than 10, otherwise, you will not receive the OnAudioVolumeIndication callback and the SDK returns the error code -2. Agora recommends that you set this value to 100."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: Invalid parameters. Check your parameter settings.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_stopaudiodeviceloopbacktest",
    "name": "StopAudioDeviceLoopbackTest",
    "description": "Stops the audio device loopback test.\n\nThis method is for Windows and macOS only.\n You can call this method either before or after joining a channel.\n This method only takes effect when called by the host.\n Ensure that you call this method to stop the loopback test after calling the StartAudioDeviceLoopbackTest method.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_stopplaybackdevicetest",
    "name": "StopPlaybackDeviceTest",
    "description": "Stops the audio playback device test.\n\nThis method stops the audio playback device test. You must call this method to stop the test after calling the StartPlaybackDeviceTest method. Call this method before joining a channel.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_iaudiodevicemanager_stoprecordingdevicetest",
    "name": "StopRecordingDeviceTest",
    "description": "Stops the audio capturing device test.\n\nThis method stops the audio capturing device test. You must call this method to stop the test after calling the StartRecordingDeviceTest method. Call this method before joining a channel.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ibasespatialaudioengine_muteallremoteaudiostreams",
    "name": "MuteAllRemoteAudioStreams",
    "description": "Stops or resumes subscribing to the audio streams of all remote users.\n\nAfter successfully calling this method, the local user stops or resumes subscribing to the audio streams of all remote users, including all subsequent users.\n Call this method after the JoinChannel [1/2] or JoinChannel [2/2] method.\n When using the spatial audio effect, if you need to set whether to stop subscribing to the audio streams of all remote users, Agora recommends calling this method instead of the MuteAllRemoteAudioStreams method in IRtcEngine.\n After calling this method, you need to call UpdateSelfPosition and UpdateRemotePosition to update the spatial location of the local user and the remote user; otherwise, the settings in this method do not take effect.",
    "parameters": [
      {
        "mute": "Whether to stop subscribing to the audio streams of all remote users: true : Stop subscribing to the audio streams of all remote users. false : Subscribe to the audio streams of all remote users."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ibasespatialaudioengine_mutelocalaudiostream",
    "name": "MuteLocalAudioStream",
    "description": "Stops or resumes publishing the local audio stream.\n\nThis method does not affect any ongoing audio recording, because it does not disable the audio capture device.\n Call this method after the JoinChannel [1/2] or JoinChannel [2/2] method.\n When using the spatial audio effect, if you need to set whether to stop subscribing to the audio stream of a specified user, Agora recommends calling this method instead of the MuteLocalAudioStream method in IRtcEngine.\n A successful call of this method triggers the OnUserMuteAudio and OnRemoteAudioStateChanged callbacks on the remote client.",
    "parameters": [
      {
        "mute": "Whether to stop publishing the local audio stream: true : Stop publishing the local audio stream. false : Publish the local audio stream."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ibasespatialaudioengine_muteremoteaudiostream",
    "name": "MuteRemoteAudioStream",
    "description": "Stops or resumes subscribing to the audio stream of a specified user.\n\nCall this method after the JoinChannel [1/2] or JoinChannel [2/2] method.\n When using the spatial audio effect, if you need to set whether to stop subscribing to the audio stream of a specified user, Agora recommends calling this method instead of the MuteRemoteAudioStream method in IRtcEngine.",
    "parameters": [
      {
        "uid": "The user ID. This parameter must be the same as the user ID passed in when the user joined the channel."
      },
      {
        "mute": "Whether to subscribe to the specified remote user's audio stream. true : Stop subscribing to the audio stream of the specified user. false : (Default) Subscribe to the audio stream of the specified user. The SDK decides whether to subscribe according to the distance between the local user and the remote user."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ibasespatialaudioengine_setaudiorecvrange",
    "name": "SetAudioRecvRange",
    "description": "Sets the audio reception range of the local user.\n\nAfter the setting is successful, the local user can only hear the remote users within the setting range or belonging to the same team. You can call this method at any time to update the audio reception range.",
    "parameters": [
      {
        "range": "The maximum audio reception range. The unit is meters. The value of this parameter must be greater than 0, and the default value is 20."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ibasespatialaudioengine_setdistanceunit",
    "name": "SetDistanceUnit",
    "description": "Sets the length (in meters) of the game engine distance per unit.\n\nIn a game engine, the unit of distance is customized, while in the Agora spatial audio algorithm, distance is measured in meters. By default, the SDK converts the game engine distance per unit to one meter. You can call this method to convert the game engine distance per unit to a specified number of meters.",
    "parameters": [
      {
        "unit": "The number of meters that the game engine distance per unit is equal to. The value of this parameter must be greater than 0.00, and the default value is 1.00. For example, setting unit as 2.00 means the game engine distance per unit equals 2 meters. The larger the value is, the faster the sound heard by the local user attenuates when the remote user moves far away from the local user."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ibasespatialaudioengine_setmaxaudiorecvcount",
    "name": "SetMaxAudioRecvCount",
    "description": "Sets the maximum number of streams that a user can receive in a specified audio reception range.\n\nIf the number of receivable streams exceeds the set value, the local user receives the maxCount streams that are closest to the local user.",
    "parameters": [
      {
        "maxCount": "The maximum number of streams that a user can receive within a specified audio reception range. The value of this parameter should be ≤ 16, and the default value is 10."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ibasespatialaudioengine_setplayerattenuation",
    "name": "SetPlayerAttenuation",
    "description": "Sets the sound attenuation properties of the media player.",
    "parameters": [
      {
        "playerId": "The ID of the media player."
      },
      {
        "attenuation": "The sound attenuation coefficient of the remote user or media player. The value range is [0,1]. The values are as follows:\n 0: Broadcast mode, where the volume and timbre are not attenuated with distance, and the volume and timbre heard by local users do not change regardless of distance.\n (0,0.5): Weak attenuation mode, that is, the volume and timbre are only weakly attenuated during the propagation process, and the sound can travel farther than the real environment.\n 0.5: (Default) simulates the attenuation of the volume in the real environment; the effect is equivalent to not setting the speaker_attenuation parameter.\n (0.5,1]: Strong attenuation mode, that is, the volume and timbre attenuate rapidly during the propagation process."
      },
      {
        "forceSet": "Whether to force the sound attenuation effect of the media player: true : Force attenuation to set the attenuation of the media player. At this time, the attenuation coefficient of the sound insulation are set in the audioAttenuation in the SpatialAudioZone does not take effect for the media player. false : Do not force attenuation to set the sound attenuation effect of the media player, as shown in the following two cases.\n If the sound source and listener are inside and outside the sound isolation area, the sound attenuation effect is determined by the audioAttenuation in SpatialAudioZone.\n If the sound source and the listener are in the same sound insulation area or outside the same sound insulation area, the sound attenuation effect is determined by attenuation in this method."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ibasespatialaudioengine_setzones",
    "name": "SetZones",
    "description": "Sets the sound insulation area.\n\nIn virtual interactive scenarios, you can use this method to set the sound insulation area and sound attenuation coefficient. When the sound source (which can be the user or the media player) and the listener belong to the inside and outside of the sound insulation area, they can experience the attenuation effect of sound similar to the real environment when it encounters a building partition.\n When the sound source and the listener belong to the inside and outside of the sound insulation area, the sound attenuation effect is determined by the sound attenuation coefficient in SpatialAudioZone.\n If the user or media player is in the same sound insulation area, it is not affected by SpatialAudioZone, and the sound attenuation effect is determined by the attenuation parameter in SetPlayerAttenuation or SetRemoteAudioAttenuation. If you do not call SetPlayerAttenuation or SetRemoteAudioAttenuation, the default sound attenuation coefficient of the SDK is 0.5, which simulates the attenuation of the sound in the real environment.\n If the sound source and the receiver belong to two sound insulation areas, the receiver cannot hear the sound source. If this method is called multiple times, the last sound insulation area set takes effect.",
    "parameters": [
      {
        "zones": "Sound insulation area settings. See SpatialAudioZone. When you set this parameter to NULL, it means clearing all sound insulation zones. On the Windows platform, it is necessary to ensure that the number of members in the zones array is equal to the value of zoneCount; otherwise, it may cause a crash."
      },
      {
        "zoneCount": "The number of sound insulation areas."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ibasespatialaudioengine_updateplayerpositioninfo",
    "name": "UpdatePlayerPositionInfo",
    "description": "Updates the spatial position of the media player.\n\nAfter a successful update, the local user can hear the change in the spatial position of the media player.",
    "parameters": [
      {
        "playerId": "The ID of the media player."
      },
      {
        "positionInfo": "The spatial position of the media player. See RemoteVoicePositionInfo."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ibasespatialaudioengine_updateselfposition",
    "name": "UpdateSelfPosition",
    "description": "Updates the spatial position of the local user.\n\nUnder the ILocalSpatialAudioEngine class, this method needs to be used with UpdateRemotePosition. The SDK calculates the relative position between the local and remote users according to this method and the parameter settings in UpdateRemotePosition, and then calculates the user's spatial audio effect parameters.",
    "parameters": [
      {
        "position": "The coordinates in the world coordinate system. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn. And the front, right, and top coordinates correspond to the positive directions of Unity's Vector3 axes (z, x, and y, respectively)."
      },
      {
        "axisForward": "The unit vector of the x axis in the coordinate system. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn. And the front, right, and top coordinates correspond to the positive directions of Unity's Vector3 axes (z, x, and y, respectively)."
      },
      {
        "axisRight": "The unit vector of the y axis in the coordinate system. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn."
      },
      {
        "axisUp": "The unit vector of the z axis in the coordinate system. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_icloudspatialaudioengine_addeventhandler",
    "name": "InitEventHandler",
    "description": "Adds the ICloudSpatialAudioEventHandler event handler.",
    "parameters": [
      {
        "engineEventHandler": "The callback to be added. See ICloudSpatialAudioEventHandler."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_icloudspatialaudioengine_enablespatializer",
    "name": "EnableSpatializer",
    "description": "Enables or disables the Agora spatial audio server from computing spatial audio parameters.\n\nOnce enabled, users can hear the spatial audio effect of remote users, as well as their spatial position changes. You can call this method either before or after EnterRoom, with the following differences:\n If you call this method before EnterRoom, this method takes effect when entering the room.\n If you call this method after EnterRoom, this method takes effect immediately.",
    "parameters": [
      {
        "enable": "Whether to enable the calculation of spatial audio effect parameters within the audio reception range: true : Enable the calculation. false : Disable the calculation."
      },
      {
        "applyToTeam": "Whether to enable the calculation of spatial audio effect parameters in the team: true : Enable the calculation. false : Disable the calculation. This parameter only takes effect when the enable parameter is true."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_icloudspatialaudioengine_enterroom",
    "name": "EnterRoom",
    "description": "Enters a room.\n\nThe spatial audio effect does not take effect until you enter a room. After you call this method, the SDK triggers the onConnectionStateChange callback. Call this method after JoinChannel [2/2].",
    "parameters": [
      {
        "uid": "The user ID. This parameter must be the same as the user ID passed in when the user joined the channel."
      },
      {
        "token": "The RTM token for authentication. You can generate the RTM token in the following ways:\n Use to generate a temporary token.\n Deploy your own server for generating tokens. The uid or userAccount for generating the RTM token is the combination of the roomName and uid set in EnterRoom. For example, if roomName is test and uid is 123, the uid or userAccount filled in when generating the RTM token is test123."
      },
      {
        "roomName": "The room name. This parameter must be the same as the channel name filled in when you join the channel."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_icloudspatialaudioengine_exitroom",
    "name": "ExitRoom",
    "description": "Exits the room.\n\nAfter the user exits the room, the spatial audio effect disappears immediately.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_icloudspatialaudioengine_getteammates",
    "name": "GetTeammates",
    "description": "Gets the information of teammates.\n\nAfter calling setTeamId to set the team ID and calling EnterRoom to enter the room, you can call this method to get the information of remote users in the same team (teammates).",
    "parameters": [
      {
        "uids": "Output parameter. The user IDs of teammates."
      },
      {
        "userCount": "Output parameter. The number of teammates."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_icloudspatialaudioengine_initialize",
    "name": "Initialize",
    "description": "Initializes . ICloudSpatialAudioEngine\n\nBefore calling other methods of the ICloudSpatialAudioEngine class, you need to call this method to initialize ICloudSpatialAudioEngine.\n The SDK supports creating only one ICloudSpatialAudioEngine instance for an app.",
    "parameters": [
      {
        "config": "The configuration of ICloudSpatialAudioEngine. See CloudSpatialAudioConfig."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).\n -2: The parameter is invalid.\n -7: The SDK is not initialized.\n -22: The resource request failed. The app uses too many system resources and fails to allocate any resources.\n -101: The App ID is invalid.",
    "is_hide": true
  },
  {
    "id": "api_icloudspatialaudioengine_setaudiorangemode",
    "name": "setAudioRangeMode",
    "description": "Sets the audio range mode.\n\nThe SDK supports two audio range modes: everyone mode and team mode. The SDK uses everyone mode by default. If you want to change to team mode, call this method. A user can only use one mode at a time in a room. You can call this method either before or after EnterRoom, with the following differences:\n If you call this method before EnterRoom, this method takes effect when entering the room.\n If you call EnterRoom this method after , this method takes effect immediately and changes the current audio range mode.",
    "parameters": [
      {
        "rangeMode": "The audio range mode. See RANGE_AUDIO_MODE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_icloudspatialaudioengine_setteamid",
    "name": "setTeamId",
    "description": "Sets the team ID.\n\nIn the same room, no matter what the audio range mode and audio reception range are, users with the same team ID can hear each other. Whether users with different team IDs can hear each other is determined by the audio range mode and audio reception range. Call this method before EnterRoom. A user can only have one team ID in a room, and the team ID cannot be changed after entering the room.",
    "parameters": [
      {
        "teamId": "The team ID. The value must be greater than 0. The default value is 0, which means that the user is not on a team with other users."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_ilocalspatialaudioengine_clearremotepositions",
    "name": "ClearRemotePositions",
    "description": "Removes the spatial positions of all remote users.\n\nAfter successfully calling this method, the local user no longer hears any remote users. After leaving the channel, to avoid wasting resources, you can also call this method to delete the spatial positions of all remote users.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ilocalspatialaudioengine_initialize",
    "name": "Initialize",
    "description": "Initializes ILocalSpatialAudioEngine.\n\nBefore calling other methods of the ILocalSpatialAudioEngine class, you need to call this method to initialize ILocalSpatialAudioEngine.\n The SDK supports creating only one ILocalSpatialAudioEngine instance for an app.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ilocalspatialaudioengine_release",
    "name": "Dispose",
    "description": "Destroys ILocalSpatialAudioEngine.\n\nThis method releases all resources under ILocalSpatialAudioEngine. When the user does not need to use the spatial audio effect, you can call this method to release resources for other operations. After calling this method, you can no longer use any of the APIs under ILocalSpatialAudioEngine. To use the spatial audio effect again, you need to wait until the Dispose method execution to complete before calling Initialize to create a new ILocalSpatialAudioEngine. Call this method before the Dispose method under IRtcEngine.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "api_ilocalspatialaudioengine_removeremoteposition",
    "name": "RemoveRemotePosition",
    "description": "Removes the spatial position of the specified remote user.\n\nAfter successfully calling this method, the local user no longer hears the specified remote user. After leaving the channel, to avoid wasting computing resources, call this method to delete the spatial position information of the specified remote user. Otherwise, the user's spatial position information will be saved continuously. When the number of remote users exceeds the number of audio streams that can be received as set in SetMaxAudioRecvCount, the system automatically unsubscribes from the audio stream of the user who is furthest away based on relative distance.",
    "parameters": [
      {
        "uid": "The user ID. This parameter must be the same as the user ID passed in when the user joined the channel."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ilocalspatialaudioengine_setremoteaudioattenuation",
    "name": "SetRemoteAudioAttenuation",
    "description": "Sets the sound attenuation effect for the specified user.",
    "parameters": [
      {
        "uid": "The user ID. This parameter must be the same as the user ID passed in when the user joined the channel."
      },
      {
        "attenuation": "For the user's sound attenuation coefficient, the value range is [0,1]. The values are as follows:\n 0: Broadcast mode, where the volume and timbre are not attenuated with distance, and the volume and timbre heard by local users do not change regardless of distance.\n (0,0.5): Weak attenuation mode, that is, the volume and timbre are only weakly attenuated during the propagation process, and the sound can travel farther than the real environment.\n 0.5: (Default) simulates the attenuation of the volume in the real environment; the effect is equivalent to not setting the speaker_attenuation parameter.\n (0.5,1]: Strong attenuation mode, that is, the volume and timbre attenuate rapidly during the propagation process."
      },
      {
        "forceSet": "Whether to force the user's sound attenuation effect: true : Force attenuation to set the sound attenuation of the user. At this time, the attenuation coefficient of the sound insulation area set in the audioAttenuation of the SpatialAudioZone does not take effect for the user.\n If the sound source and listener are inside and outside the sound isolation area, the sound attenuation effect is determined by the audioAttenuation in SpatialAudioZone.\n If the sound source and the listener are in the same sound insulation area or outside the same sound insulation area, the sound attenuation effect is determined by attenuation in this method. false : Do not force attenuation to set the user's sound attenuation effect, as shown in the following two cases."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ilocalspatialaudioengine_updateremoteposition",
    "name": "UpdateRemotePosition",
    "description": "Updates the spatial position of the specified remote user.\n\nAfter successfully calling this method, the SDK calculates the spatial audio parameters based on the relative position of the local and remote user. Call this method after the JoinChannel [1/2] or JoinChannel [2/2] method.",
    "parameters": [
      {
        "uid": "The user ID. This parameter must be the same as the user ID passed in when the user joined the channel."
      },
      {
        "posInfo": "The spatial position of the remote user. See RemoteVoicePositionInfo."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_createcustomaudiotrack",
    "name": "CreateCustomAudioTrack",
    "description": "Creates a custom audio track.\n\nCall this method before joining a channel. To publish a custom audio source, see the following steps:\n Call this method to create a custom audio track and get the audio track ID.\n Call JoinChannel [2/2] to join the channel. In ChannelMediaOptions, set publishCustomAudioTrackId to the audio track ID that you want to publish, and set publishCustomAudioTrack to true.\n Call PushAudioFrame and specify trackId as the audio track ID set in step 2. You can then publish the corresponding custom audio source in the channel.",
    "parameters": [
      {
        "trackType": "The type of the custom audio track. See AUDIO_TRACK_TYPE. If AUDIO_TRACK_DIRECT is specified for this parameter, you must set publishMicrophoneTrack to false in ChannelMediaOptions when calling JoinChannel [2/2] to join the channel; otherwise, joining the channel fails and returns the error code -2."
      },
      {
        "config": "The configuration of the custom audio track. See AudioTrackConfig."
      }
    ],
    "returns": "If the method call is successful, the audio track ID is returned as the unique identifier of the audio track.\n If the method call fails, 0xffffffff is returned.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_destroycustomaudiotrack",
    "name": "DestroyCustomAudioTrack",
    "description": "Destroys the specified audio track.",
    "parameters": [
      {
        "trackId": "The custom audio track ID returned in CreateCustomAudioTrack."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_pullaudioframe",
    "name": "PullAudioFrame",
    "description": "Pulls the remote audio data.\n\nAfter a successful call of this method, the app pulls the decoded and mixed audio data for playback.",
    "parameters": [
      {
        "frame": "Pointers to AudioFrame."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_pushaudioframe0",
    "name": "PushAudioFrame",
    "description": "Pushes the external audio frame.\n\nCall this method to push external audio frames through the audio track.",
    "parameters": [
      {
        "frame": "The external audio frame. See AudioFrame."
      },
      {
        "trackId": "The audio track ID. If you want to publish a custom external audio source, set this parameter to the ID of the corresponding custom audio track you want to publish."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_pushencodedvideoimage",
    "name": "PushEncodedVideoImage [1/2]",
    "description": "Pushes the external encoded video frame to the SDK.\n\nAfter calling SetExternalVideoSource to enable external video source and set the sourceType parameter to ENCODED_VIDEO_FRAME, you can call this method to push the encoded external video frame to the SDK.",
    "parameters": [
      {
        "imageBuffer": "The buffer of the external encoded video frame."
      },
      {
        "length": "Length of the externally encoded video frames."
      },
      {
        "videoEncodedFrameInfo": "Information about externally encoded video frames. See EncodedVideoFrameInfo."
      }
    ],
    "returns": "0: Pushes the external encoded video frame to the SDK successfully.\n < 0: Fails to push external encoded video frames to the SDK.",
    "is_hide": true
  },
  {
    "id": "api_imediaengine_pushvideoframe",
    "name": "PushVideoFrame",
    "description": "Pushes the external raw video frame to the SDK through video tracks.\n\nTo publish a custom video source, see the following steps:\n Call CreateCustomVideoTrack to create a video track and get the video track ID.\n Call JoinChannel [2/2] to join the channel. In ChannelMediaOptions, set customVideoTrackId to the video track ID that you want to publish, and set publishCustomVideoTrack to true.\n Call this method and specify videoTrackId as the video track ID set in step 2. You can then publish the corresponding custom video source in the channel. After calling this method, even if you stop pushing external video frames to the SDK, the custom video stream will still be counted as the video duration usage and incur charges. Agora recommends that you take appropriate measures based on the actual situation to avoid such video billing.\n If you no longer need to capture external video data, you can call DestroyCustomVideoTrack to destroy the custom video track.\n If you only want to use the external video data for local preview and not publish it in the channel, you can call MuteLocalVideoStream to cancel sending video stream or call UpdateChannelMediaOptions to set publishCustomVideoTrack to false.",
    "parameters": [
      {
        "frame": "The external raw video frame to be pushed. See ExternalVideoFrame."
      },
      {
        "videoTrackId": "The video track ID returned by calling the CreateCustomVideoTrack method. The default value is 0."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_registeraudioframeobserver",
    "name": "RegisterAudioFrameObserver",
    "description": "Registers an audio frame observer object.\n\nCall this method to register an audio frame observer object (register a callback). When you need the SDK to trigger the OnMixedAudioFrame, OnRecordAudioFrame, OnPlaybackAudioFrame, OnPlaybackAudioFrameBeforeMixing or OnEarMonitoringAudioFrame callback, you need to use this method to register the callbacks.",
    "parameters": [
      {
        "audioFrameObserver": "The observer instance. See IAudioFrameObserver. Set the value as NULL to release the instance. Agora recommends calling this method after receiving OnLeaveChannel to release the audio observer object."
      },
      {
        "mode": "The audio data callback mode. See OBSERVER_MODE."
      },
      {
        "position": "The frame position of the audio observer. AUDIO_FRAME_POSITION_PLAYBACK (0x0001): This position can observe the playback audio mixed by all remote users, corresponding to the OnPlaybackAudioFrame callback. AUDIO_FRAME_POSITION_RECORD (0x0002): This position can observe the collected local user's audio, corresponding to the OnRecordAudioFrame callback. AUDIO_FRAME_POSITION_MIXED (0x0004): This position can observe the playback audio mixed by the loacl user and all remote users, corresponding to the OnMixedAudioFrame callback. AUDIO_FRAME_POSITION_BEFORE_MIXING (0x0008): This position can observe the audio of a single remote user before mixing, corresponding to the OnPlaybackAudioFrameBeforeMixing callback. AUDIO_FRAME_POSITION_EAR_MONITORING (0x0010): This position can observe the in-ear monitoring audio of the local user, corresponding to the OnEarMonitoringAudioFrame callback."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_registerfaceinfoobserver",
    "name": "RegisterFaceInfoObserver",
    "description": "Registers a facial information observer.\n\nYou can call this method to register the OnFaceInfo callback to receive the facial information processed by Agora speech driven extension. When calling this method to register a facial information observer, you can register callbacks in the IFaceInfoObserver class as needed. After successfully registering the facial information observer, the SDK triggers the callback you have registered when it captures the facial information converted by the speech driven extension.\n Call this method before joining a channel.\n Before calling this method, you need to make sure that the speech driven extension has been enabled by calling EnableExtension.",
    "parameters": [
      {
        "observer": "Facial information observer, see IFaceInfoObserver."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_registervideoencodedframeobserver",
    "name": "RegisterVideoEncodedFrameObserver",
    "description": "Registers a receiver object for the encoded video image.\n\nIf you only want to observe encoded video frames (such as H.264 format) without decoding and rendering the video, Agora recommends that you implement one IVideoEncodedFrameObserver class through this method. Call this method before joining a channel.",
    "parameters": [
      {
        "videoEncodedImageReceiver": "The video frame observer object. See IVideoEncodedFrameObserver."
      },
      {
        "mode": "The video data callback mode. See OBSERVER_MODE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_registervideoframeobserver",
    "name": "RegisterVideoFrameObserver",
    "description": "Registers a raw video frame observer object.\n\nIf you want to observe raw video frames (such as YUV or RGBA format), Agora recommends that you implement one IVideoFrameObserver class with this method. When calling this method to register a video observer, you can register callbacks in the IVideoFrameObserver class as needed. After you successfully register the video frame observer, the SDK triggers the registered callbacks each time a video frame is received.",
    "parameters": [
      {
        "videoFrameObserver": "The observer instance. See IVideoFrameObserver. To release the instance, set the value as NULL."
      },
      {
        "mode": "The video data callback mode. See OBSERVER_MODE."
      },
      {
        "formatPreference": "The video frame type. See VIDEO_OBSERVER_FRAME_TYPE."
      },
      {
        "position": "A bit mask that controls the frame position of the video observer. See VIDEO_MODULE_POSITION."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_setexternalaudiosink",
    "name": "SetExternalAudioSink",
    "description": "Sets the external audio sink.\n\nAfter enabling the external audio sink, you can call PullAudioFrame to pull remote audio frames. The app can process the remote audio and play it with the audio effects that you want.",
    "parameters": [
      {
        "enabled": "Whether to enable or disable the external audio sink: true : Enables the external audio sink. false : (Default) Disables the external audio sink."
      },
      {
        "sampleRate": "The sample rate (Hz) of the external audio sink, which can be set as 16000, 32000, 44100, or 48000."
      },
      {
        "channels": "The number of audio channels of the external audio sink:\n 1: Mono.\n 2: Stereo."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_setexternalaudiosource2",
    "name": "SetExternalAudioSource",
    "description": "Sets the external audio source parameters.\n\nDeprecated: This method is deprecated, use CreateCustomAudioTrack instead.",
    "parameters": [
      {
        "enabled": "Whether to enable the external audio source: true : Enable the external audio source. false : (Default) Disable the external audio source."
      },
      {
        "sampleRate": "The sample rate (Hz) of the external audio source which can be set as 8000, 16000, 32000, 44100, or 48000."
      },
      {
        "channels": "The number of channels of the external audio source, which can be set as 1 (Mono) or 2 (Stereo)."
      },
      {
        "localPlayback": "Whether to play the external audio source: true : Play the external audio source. false : (Default) Do not play the external source."
      },
      {
        "publish": "Whether to publish audio to the remote users: true : (Default) Publish audio to the remote users. false : Do not publish audio to the remote users."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_setexternalvideosource",
    "name": "SetExternalVideoSource",
    "description": "Configures the external video source.\n\nAfter calling this method to enable an external video source, you can call PushVideoFrame to push external video data to the SDK.",
    "parameters": [
      {
        "enabled": "Whether to use the external video source: true : Use the external video source. The SDK prepares to accept the external video frame. false : (Default) Do not use the external video source."
      },
      {
        "useTexture": "Whether to use the external video frame in the Texture format. true : Use the external video frame in the Texture format. false : (Default) Do not use the external video frame in the Texture format."
      },
      {
        "sourceType": "Whether the external video frame is encoded. See EXTERNAL_VIDEO_SOURCE_TYPE."
      },
      {
        "encodedVideoOption": "Video encoding options. This parameter needs to be set if sourceType is ENCODED_VIDEO_FRAME. To set this parameter, contact."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_unregisterfaceinfoobserver",
    "name": "UnregisterFaceInfoObserver",
    "description": "Unregisters a facial information observer.",
    "parameters": [
      {
        "observer": "Facial information observer, see IFaceInfoObserver."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_unregistervideoencodedframeobserver",
    "name": "UnregisterVideoEncodedFrameObserver",
    "description": "Unregisters a receiver object for the encoded video frame.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaengine_unregistervideoframeobserver",
    "name": "UnregisterVideoFrameObserver",
    "description": "Unregisters the video frame observer.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_adjustplayoutvolume",
    "name": "AdjustPlayoutVolume",
    "description": "Adjusts the local playback volume.",
    "parameters": [
      {
        "volume": "The local playback volume, which ranges from 0 to 100:\n 0: Mute.\n 100: (Default) The original volume."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_adjustpublishsignalvolume",
    "name": "AdjustPublishSignalVolume",
    "description": "Adjusts the volume of the media file for publishing.\n\nAfter connected to the Agora server, you can call this method to adjust the volume of the media file heard by the remote user.",
    "parameters": [
      {
        "volume": "The volume, which ranges from 0 to 400:\n 0: Mute.\n 100: (Default) The original volume.\n 400: Four times the original volume (amplifying the audio signals by four times)."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_dispose",
    "name": "Dispose",
    "description": "Releases all the resources occupied by the media player.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_enableautoswitchagoracdn",
    "name": "EnableAutoSwitchAgoraCDN",
    "description": "Enables/Disables the automatic switch of the CDN routes for playing the media resource.\n\nYou can call this method if you want the SDK to automatically switch the CDN routes according to your network conditions. Call this method before OpenWithAgoraCDNSrc.",
    "parameters": [
      {
        "enable": "Whether to enable the automatic switch of the CDN routes for playing the media resource: true : Enables the automatic switch of the CDN routes. false : (Default) Disables the automatic switch of the CDN routes."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imediaplayer_getagoracdnlinecount",
    "name": "GetAgoraCDNLineCount",
    "description": "Gets the number of CDN routes for the media resource.",
    "parameters": [],
    "returns": "Returns the number of CDN routes for the media resource, if the method call succeeds.\n ≤ 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imediaplayer_getcurrentagoracdnindex",
    "name": "GetCurrentAgoraCDNIndex",
    "description": "Gets the CDN routes index of the current media resource.",
    "parameters": [],
    "returns": "The number of CDN routes for the media resource, if the method call succeeds. The value range is [0, GetAgoraCDNLineCount ()).\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imediaplayer_getduration",
    "name": "GetDuration",
    "description": "Gets the duration of the media resource.",
    "parameters": [
      {
        "duration": "An output parameter. The total duration (ms) of the media file."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_getmediaplayerid",
    "name": "GetId",
    "description": "Gets the ID of the media player.",
    "parameters": [],
    "returns": "Success. The ID of the media player.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_getmute",
    "name": "GetMute",
    "description": "Reports whether the media resource is muted.",
    "parameters": [
      {
        "muted": "An output parameter. Whether the media file is muted: true : The media file is muted. false : The media file is not muted."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_getplayoutvolume",
    "name": "GetPlayoutVolume",
    "description": "Gets the local playback volume.",
    "parameters": [
      {
        "volume": "An output parameter. The local playback volume, which ranges from 0 to 100:\n 0: Mute.\n 100: (Default) The original volume."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_getplayposition",
    "name": "GetPlayPosition",
    "description": "Gets current local playback progress.",
    "parameters": [
      {
        "pos": "The playback position (ms) of the audio effect file."
      }
    ],
    "returns": "Returns the current playback progress (ms) if the call succeeds.\n < 0: Failure. See MEDIA_PLAYER_REASON.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_getplaysrc",
    "name": "GetPlaySrc",
    "description": "Gets the path of the media resource being played.",
    "parameters": [],
    "returns": "The path of the media resource being played.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_getpublishsignalvolume",
    "name": "GetPublishSignalVolume",
    "description": "Gets the volume of the media file for publishing.",
    "parameters": [
      {
        "volume": "An output parameter. The remote playback volume."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_getstate",
    "name": "GetState",
    "description": "Gets current playback state.",
    "parameters": [],
    "returns": "The current playback state. See MEDIA_PLAYER_STATE.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_getstreamcount",
    "name": "GetStreamCount",
    "description": "Gets the number of the media streams in the media resource.\n\nCall this method after you call Open and receive the OnPlayerSourceStateChanged callback reporting the state PLAYER_STATE_OPEN_COMPLETED.",
    "parameters": [
      {
        "count": "An output parameter. The number of the media streams in the media resource."
      }
    ],
    "returns": "0: Success.\n < 0: Failure. See MEDIA_PLAYER_REASON.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_getstreaminfo",
    "name": "GetStreamInfo",
    "description": "Gets the detailed information of the media stream.",
    "parameters": [
      {
        "index": "The index of the media stream. This parameter needs to be less than the count parameter of GetStreamCount."
      },
      {
        "info": "An output parameter. The detailed information of the media stream. See PlayerStreamInfo."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_initeventhandler",
    "name": "InitEventHandler",
    "description": "Adds callback event for media player.",
    "parameters": [
      {
        "engineEventHandler": "Callback events to be added. See IMediaPlayerSourceObserver."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_mute",
    "name": "Mute",
    "description": "Sets whether to mute the media file.",
    "parameters": [
      {
        "muted": "Whether to mute the media file: true : Mute the media file. false : (Default) Unmute the media file."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_open",
    "name": "Open",
    "description": "Opens the media resource.\n\nThis method is called asynchronously.",
    "parameters": [
      {
        "url": "The path of the media file. Both local path and online path are supported."
      },
      {
        "startPos": "The starting position (ms) for playback. Default value is 0."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_open2",
    "name": "",
    "description": "Opens a media file through a URI address.\n\nThis method is called asynchronously.",
    "parameters": [
      {
        "uri": "The URI (Uniform Resource Identifier) of the media file."
      },
      {
        "startPos": "The starting position (ms) for playback. The default value is 0."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imediaplayer_openwithagoracdnsrc",
    "name": "OpenWithAgoraCDNSrc",
    "description": "Opens a media resource and requests all the CDN routes of the media resources through the self-developed scheduling center.\n\nThis method is called asynchronously. After you call this method, Agora opens the media resources and tries to obtain all the CDN routes for playing the media resource. By default, Agora uses the first CDN route for playing, and you can call SwitchAgoraCDNLineByIndex to switch routes. If you want to ensure the security of the connection and media files, to determine the sign and the ts fields for authentication. Once the fields are determined, use them as the query parameter of the URL to update the URL of the media resource. For example:\n The URL of the media file to be opened: rtmp://$domain/$appName/$streamName\n The URL updated by the authentication of the media file to be opened: rtmp://$domain/$appName/$streamName?ts=$ts&sign=$sign Authentication information: sign : An encrypted string calculated according to the MD5 algorithm based on authKey, appName, streamName, and ts. You need to for your authKey. ts : The timestamp when the authentication information expires. You can set the validity period of the authentication information according to your scenarios. For example, 24h or 1h30m20s.",
    "parameters": [
      {
        "src": "The URL of the media resource."
      },
      {
        "startPos": "The starting position (ms) for playback. The default value is 0. This value can be empty if the media resource to be played is live streams."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imediaplayer_openwithmediasource",
    "name": "OpenWithMediaSource",
    "description": "Opens a media file and configures the playback scenarios.\n\nThis method supports opening media files of different sources, including a custom media source, and allows you to configure the playback scenarios.",
    "parameters": [
      {
        "source": "Media resources. See MediaSource."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_pause",
    "name": "Pause",
    "description": "Pauses the playback.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_play",
    "name": "Play",
    "description": "Plays the media file.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_playpreloadedsrc",
    "name": "PlayPreloadedSrc",
    "description": "Plays preloaded media resources.\n\nAfter calling the PreloadSrc method to preload the media resource into the playlist, you can call this method to play the preloaded media resource. After calling this method, if you receive the OnPlayerSourceStateChanged callback which reports the PLAYER_STATE_PLAYING state, the playback is successful. If you want to change the preloaded media resource to be played, you can call this method again and specify the URL of the new media resource that you want to preload. If you want to replay the media resource, you need to call PreloadSrc to preload the media resource to the playlist again before playing. If you want to clear the playlist, call the Stop method. If you call this method when playback is paused, this method does not take effect until playback is resumed.",
    "parameters": [
      {
        "src": "The URL of the media resource in the playlist must be consistent with the src set by the PreloadSrc method; otherwise, the media resource cannot be played."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_preloadsrc",
    "name": "PreloadSrc",
    "description": "Preloads a media resource.\n\nYou can call this method to preload a media resource into the playlist. If you need to preload multiple media resources, you can call this method multiple times. If the preload is successful and you want to play the media resource, call PlayPreloadedSrc; if you want to clear the playlist, call Stop.\n Before calling this method, ensure that you have called Open or OpenWithMediaSource to open the media resource successfully.\n Agora does not support preloading duplicate media resources to the playlist. However, you can preload the media resources that are being played to the playlist again.",
    "parameters": [
      {
        "src": "The URL of the media resource."
      },
      {
        "startPos": "The starting position (ms) for playing after the media resource is preloaded to the playlist. When preloading a live stream, set this parameter to 0."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_registeraudioframeobserver",
    "name": "RegisterAudioFrameObserver [1/2]",
    "description": "Registers a PCM audio frame observer object.\n\nYou need to implement the IAudioPcmFrameSink class in this method and register callbacks according to your scenarios. After you successfully register the video frame observer, the SDK triggers the registered callbacks each time a video frame is received.",
    "parameters": [
      {
        "observer": "The audio frame observer, reporting the reception of each audio frame. See IAudioPcmFrameSink."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_registeraudioframeobserver2",
    "name": "RegisterAudioFrameObserver [2/2]",
    "description": "Registers an audio frame observer object.",
    "parameters": [
      {
        "observer": "The audio frame observer, reporting the reception of each audio frame. See IAudioPcmFrameSink."
      },
      {
        "mode": "The use mode of the audio frame. See RAW_AUDIO_FRAME_OP_MODE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_renewagoracdnsrctoken",
    "name": "RenewAgoraCDNSrcToken",
    "description": "Renew the authentication information for the URL of the media resource to be played.\n\nWhen the authentication information expires (exceeds the ts field), you can call the OpenWithAgoraCDNSrc method to reopen the media resource or the SwitchAgoraCDNSrc method to switch the media resource, and then pass in the authenticated URL (with the ts field updated) of the media resource. If your authentication information expires when you call the SwitchAgoraCDNLineByIndex to switch the CDN route for playing the media resource, you need to call this method to pass in the updated authentication information to update the authentication information of the media resource URL. After updating the authentication information, you need to call SwitchAgoraCDNLineByIndex to complete the route switching. To avoid frequent expiration of authentication information, ensure that you set the ts field appropriately or according to the scenario requirements.",
    "parameters": [
      {
        "token": "The authentication field. See the sign field of the authentication information."
      },
      {
        "ts": "The timestamp when the authentication information expires. See the ts field of the authentication information."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imediaplayer_resume",
    "name": "Resume",
    "description": "Resumes playing the media file.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_seek",
    "name": "Seek",
    "description": "Seeks to a new playback position.\n\nIf you call Seek after the playback has completed (upon receiving callback OnPlayerSourceStateChanged reporting playback status as PLAYER_STATE_PLAYBACK_COMPLETED or PLAYER_STATE_PLAYBACK_ALL_LOOPS_COMPLETED), the SDK will play the media file from the specified position. At this point, you will receive callback OnPlayerSourceStateChanged reporting playback status as PLAYER_STATE_PLAYING.\n If you call Seek while the playback is paused, upon successful call of this method, the SDK will seek to the specified position. To resume playback, call Resume or Play .",
    "parameters": [
      {
        "newPos": "The new playback position (ms)."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_selectaudiotrack",
    "name": "SelectAudioTrack [2/2]",
    "description": "Selects the audio track used during playback.\n\nAfter getting the track index of the audio file, you can call this method to specify any track to play. For example, if different tracks of a multi-track file store songs in different languages, you can call this method to set the playback language. You need to call this method after calling GetStreamInfo to get the audio stream index value.",
    "parameters": [
      {
        "index": "The index of the audio track."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_selectmultiaudiotrack",
    "name": "SelectMultiAudioTrack",
    "description": "Selects the audio tracks that you want to play on your local device and publish to the channel respectively.\n\nYou can call this method to determine the audio track to be played on your local device and published to the channel. Before calling this method, you need to open the media file with the OpenWithMediaSource method and set enableMultiAudioTrack in MediaSource as true.",
    "parameters": [
      {
        "playoutTrackIndex": "The index of audio tracks for local playback. You can obtain the index through GetStreamInfo."
      },
      {
        "publishTrackIndex": "The index of audio tracks to be published in the channel. You can obtain the index through GetStreamInfo."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_setaudiodualmonomode",
    "name": "SetAudioDualMonoMode",
    "description": "Sets the channel mode of the current audio file.\n\nIn a stereo music file, the left and right channels can store different audio data. According to your needs, you can set the channel mode to original mode, left channel mode, right channel mode, or mixed channel mode. For example, in the KTV scenario, the left channel of the music file stores the musical accompaniment, and the right channel stores the singing voice. If you only need to listen to the accompaniment, call this method to set the channel mode of the music file to left channel mode; if you need to listen to the accompaniment and the singing voice at the same time, call this method to set the channel mode to mixed channel mode.\n Call this method after calling Open.\n This method only applies to stereo audio files.",
    "parameters": [
      {
        "mode": "The channel mode. See AUDIO_DUAL_MONO_MODE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_setaudiopitch",
    "name": "SetAudioPitch",
    "description": "Sets the pitch of the current media resource.\n\nCall this method after calling Open.",
    "parameters": [
      {
        "pitch": "Sets the pitch of the local music file by the chromatic scale. The default value is 0, which means keeping the original pitch. The value ranges from -12 to 12, and the pitch value between consecutive values is a chromatic value. The greater the absolute value of this parameter, the higher or lower the pitch of the local music file."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_setloopcount",
    "name": "SetLoopCount",
    "description": "Sets the loop playback.\n\nIf you want to loop, call this method and set the number of the loops. When the loop finishes, the SDK triggers OnPlayerSourceStateChanged and reports the playback state as PLAYER_STATE_PLAYBACK_ALL_LOOPS_COMPLETED.",
    "parameters": [
      {
        "loopCount": "The number of times the audio effect loops:\n ≥0: Number of times for playing. For example, setting it to 0 means no loop playback, playing only once; setting it to 1 means loop playback once, playing a total of twice.\n -1: Play the audio file in an infinite loop."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_setplaybackspeed",
    "name": "SetPlaybackSpeed",
    "description": "Sets the channel mode of the current audio file.\n\nCall this method after calling Open.",
    "parameters": [
      {
        "speed": "The playback speed. Agora recommends that you set this to a value between 30 and 400, defined as follows:\n 30: 0.3 times the original speed.\n 100: The original speed.\n 400: 4 times the original speed."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_setplayeroption",
    "name": "SetPlayerOption [1/2]",
    "description": "Sets media player options.\n\nThe media player supports setting options through key and value. The difference between this method and setPlayerOption [2/2] is that the value parameter of this method is of type Int, while the value of setPlayerOption [2/2] is of type String. These two methods cannot be used together.",
    "parameters": [
      {
        "key": "The key of the option."
      },
      {
        "value": "The value of the key."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_setplayeroption2",
    "name": "setPlayerOption [2/2]",
    "description": "Sets media player options.\n\nThe media player supports setting options through key and value. The difference between this method and SetPlayerOption [1/2] is that the value parameter of this method is of type String, while the value of SetPlayerOption [1/2] is of type String. These two methods cannot be used together.",
    "parameters": [
      {
        "key": "The key of the option."
      },
      {
        "value": "The value of the key."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_setrendermode",
    "name": "SetRenderMode",
    "description": "Sets the render mode of the media player.",
    "parameters": [
      {
        "renderMode": "Sets the render mode of the view. See RENDER_MODE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_setspatialaudioparams",
    "name": "SetSpatialAudioParams",
    "description": "Enables or disables the spatial audio effect for the media player.\n\nAfter successfully setting the spatial audio effect parameters of the media player, the SDK enables the spatial audio effect for the media player, and the local user can hear the media resources with a sense of space. If you need to disable the spatial audio effect for the media player, set the params parameter to null.",
    "parameters": [
      {
        "spatial_audio_params": "The spatial audio effect parameters of the media player. See SpatialAudioParams."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_setview",
    "name": "SetView",
    "description": "Sets the view.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_stop",
    "name": "Stop",
    "description": "Stops playing the media track.\n\nAfter calling this method to stop playback, if you want to play again, you need to call Open or OpenWithMediaSource to open the media resource.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_switchagoracdnlinebyindex",
    "name": "SwitchAgoraCDNLineByIndex",
    "description": "Changes the CDN route for playing the media resource.\n\nAfter calling OpenWithAgoraCDNSrc to open the media resource, you can call this method if you want to change the CDN routes for playing the media resource.\n Call this method after calling OpenWithAgoraCDNSrc.\n You can call this method either before or after Play. If you call this method before Play, the switch does not take effect immediately. The SDK waits for the playback to complete before switching the CDN line of the media resource.",
    "parameters": [
      {
        "index": "The index of the CDN routes."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imediaplayer_switchagoracdnsrc",
    "name": "SwitchAgoraCDNSrc",
    "description": "Switches the media resource being played.\n\nIf you want to ensure the security of the connection and media files, to determine the sign and the ts fields for authentication. Once the fields are determined, use them as the query parameter of the URL to update the URL of the media resource. For example:\n The URL of the media file to be opened: rtmp://$domain/$appName/$streamName\n The URL updated by the authentication of the media file to be opened: rtmp://$domain/$appName/$streamName?ts=$ts&sign=$sign Authentication information: sign : An encrypted string calculated according to the MD5 algorithm based on authKey, appName, streamName, and ts. You need to for your authKey. ts : The timestamp when the authentication information expires. You can set the validity period of the authentication information according to your scenarios. For example, 24h or 1h30m20s. If you want to customize the CDN routes for playing the media resource, call this method to switch media resources. Agora changes the CDN route through the self-developed scheduling center to improve the viewing user experience. If you do not need to customize CDN routes for playing the media resource, call the SwitchSrc method to switch media resources.\n Call this method after calling OpenWithAgoraCDNSrc.\n You can call this method either before or after Play. If you call this method before Play, the SDK waits for you to call Play before completing the route switch.",
    "parameters": [
      {
        "src": "The URL of the media resource."
      },
      {
        "syncPts": "Whether to synchronize the playback position (ms) before and after the switch: true : Synchronize the playback position before and after the switch. false : (Default) Do not synchronize the playback position before and after the switch. Make sure to set this parameter as false if you need to play live streams, or the switch fails. If you need to play on-demand streams, you can set the value of this parameter according to your scenarios."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imediaplayer_switchsrc",
    "name": "SwitchSrc",
    "description": "Switches the media resource being played.\n\nYou can call this method to switch the media resource to be played according to the current network status. For example:\n When the network is poor, the media resource to be played is switched to a media resource address with a lower bitrate.\n When the network is good, the media resource to be played is switched to a media resource address with a higher bitrate. After calling this method, if you receive the OnPlayerEvent callback report the PLAYER_EVENT_SWITCH_COMPLETE event, the switching is successful. If the switching fails, the SDK will automatically retry 3 times. If it still fails, you will receive the OnPlayerEvent callback reporting the PLAYER_EVENT_SWITCH_ERROR event indicating an error occurred during media resource switching.\n Ensure that you call this method after Open.\n To ensure normal playback, pay attention to the following when calling this method:\n Do not call this method when playback is paused.\n Do not call the Seek method during switching.\n Before switching the media resource, make sure that the playback position does not exceed the total duration of the media resource to be switched.",
    "parameters": [
      {
        "src": "The URL of the media resource."
      },
      {
        "syncPts": "Whether to synchronize the playback position (ms) before and after the switch: true : Synchronize the playback position before and after the switch. false : (Default) Do not synchronize the playback position before and after the switch."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_unloadsrc",
    "name": "UnloadSrc",
    "description": "Unloads media resources that are preloaded.\n\nThis method cannot release the media resource being played.",
    "parameters": [
      {
        "src": "The URL of the media resource."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayer_unregisteraudioframeobserver",
    "name": "UnregisterAudioFrameObserver",
    "description": "Unregisters an audio frame observer.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayercachemanager_enableautoremovecache",
    "name": "EnableAutoRemoveCache",
    "description": "Sets whether to delete cached media files automatically.\n\nIf you enable this function to remove cached media files automatically, when the cached media files exceed either the number or size limit you set, the SDK automatically deletes the least recently used cache file.",
    "parameters": [
      {
        "enable": "Whether to enable the SDK to delete cached media files automatically: true : Delete cached media files automatically. false : (Default) Do not delete cached media files automatically."
      }
    ],
    "returns": "0: Success.\n < 0: Failure. See MEDIA_PLAYER_REASON.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayercachemanager_getcachedir",
    "name": "GetCacheDir",
    "description": "Gets the storage path of the cached media files.\n\nIf you have not called the SetCacheDir method to set the storage path for the media files to be cached before calling this method, you get the default storage path used by the SDK.",
    "parameters": [
      {
        "path": "An output parameter; the storage path for the media file to be cached."
      },
      {
        "length": "An input parameter; the maximum length of the cache file storage path string. Fill in according to the cache file storage path string you obtained from path."
      }
    ],
    "returns": "0: Success.\n < 0: Failure. See MEDIA_PLAYER_REASON.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayercachemanager_getcachefilecount",
    "name": "GetCacheFileCount",
    "description": "Gets the number of media files that are cached.",
    "parameters": [],
    "returns": "≥ 0: The call succeeds and returns the number of media files that are cached.\n < 0: Failure. See MEDIA_PLAYER_REASON.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayercachemanager_getmaxcachefilecount",
    "name": "GetMaxCacheFileCount",
    "description": "Gets the maximum number of media files that can be cached.\n\nBy default, the maximum number of media files that can be cached is 1,000.",
    "parameters": [],
    "returns": "> 0: The call succeeds and returns the maximum number of media files that can be cached.\n < 0: Failure. See MEDIA_PLAYER_REASON.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayercachemanager_getmaxcachefilesize",
    "name": "GetMaxCacheFileSize",
    "description": "Gets the maximum size of the aggregate storage space for cached media files.\n\nBy default, the maximum size of the aggregate storage space for cached media files is 1 GB. You can call the SetMaxCacheFileSize method to set the limit according to your scenarios.",
    "parameters": [],
    "returns": "> 0: The call succeeds and returns the maximum size (in bytes) of the aggregate storage space for cached media files.\n < 0: Failure. See MEDIA_PLAYER_REASON.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayercachemanager_removeallcaches",
    "name": "RemoveAllCaches",
    "description": "Deletes all cached media files in the media player.\n\nThe cached media file currently being played will not be deleted.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure. See MEDIA_PLAYER_REASON.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayercachemanager_removecachebyuri",
    "name": "RemoveCacheByUri",
    "description": "Deletes a cached media file.\n\nThe cached media file currently being played will not be deleted.",
    "parameters": [
      {
        "uri": "The URI (Uniform Resource Identifier) of the media file to be deleted."
      }
    ],
    "returns": "0: Success.\n < 0: Failure. See MEDIA_PLAYER_REASON.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayercachemanager_removeoldcache",
    "name": "RemoveOldCache",
    "description": "Deletes a cached media file that is the least recently used.\n\nYou can call this method to delete a cached media file when the storage space for the cached files is about to reach its limit. After you call this method, the SDK deletes the cached media file that is least used. The cached media file currently being played will not be deleted.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure. See MEDIA_PLAYER_REASON.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayercachemanager_setcachedir",
    "name": "SetCacheDir",
    "description": "Sets the storage path for the media files that you want to cache.\n\nMake sure IRtcEngine is initialized before you call this method.",
    "parameters": [
      {
        "path": "The absolute path of the media files to be cached. Ensure that the directory for the media files exists and is writable."
      }
    ],
    "returns": "0: Success.\n < 0: Failure. See MEDIA_PLAYER_REASON.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayercachemanager_setmaxcachefilecount",
    "name": "SetMaxCacheFileCount",
    "description": "Sets the maximum number of media files that can be cached.",
    "parameters": [
      {
        "count": "The maximum number of media files that can be cached. The default value is 1,000."
      }
    ],
    "returns": "0: Success.\n < 0: Failure. See MEDIA_PLAYER_REASON.",
    "is_hide": false
  },
  {
    "id": "api_imediaplayercachemanager_setmaxcachefilesize",
    "name": "SetMaxCacheFileSize",
    "description": "Sets the maximum size of the aggregate storage space for cached media files.",
    "parameters": [
      {
        "cacheSize": "The maximum size (bytes) of the aggregate storage space for cached media files. The default value is 1 GB."
      }
    ],
    "returns": "0: Success.\n < 0: Failure. See MEDIA_PLAYER_REASON.",
    "is_hide": false
  },
  {
    "id": "api_imediarecorder_setmediarecorderobserver",
    "name": "SetMediaRecorderObserver",
    "description": "Registers one IMediaRecorderObserver oberver.\n\nThis method is used to set the callbacks of audio and video recording, so as to notify the app of the recording status and information of the audio and video stream during recording. Before calling this method, ensure the following:\n The IRtcEngine object is created and initialized.\n The recording object is created through CreateMediaRecorder.",
    "parameters": [
      {
        "callback": "The callbacks for recording audio and video streams. See IMediaRecorderObserver."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imediarecorder_startrecording",
    "name": "StartRecording",
    "description": "Starts recording audio and video streams.\n\nYou can call this method to enable the recording function. Agora supports recording the media streams of local and remote users at the same time. Before you call this method, ensure the following:\n The recording object is created through CreateMediaRecorder.\n The recording observer is registered through SetMediaRecorderObserver.\n You have joined the channel which the remote user that you want to record is in. Supported formats of recording are listed as below:\n AAC-encoded audio captured by the microphone.\n Video captured by a camera and encoded in H.264 or H.265. Once the recording is started, if the video resolution is changed, the SDK stops the recording; if the sampling rate and audio channel changes, the SDK continues recording and generates audio files respectively. The SDK can generate a recording file only when it detects audio and video streams; when there are no audio and video streams to be recorded or the audio and video streams are interrupted for more than 5 seconds, the SDK stops the recording and triggers the OnRecorderStateChanged (RECORDER_STATE_ERROR, RECORDER_ERROR_NO_STREAM ) callback.\n If you want to record the media streams of the local user, ensure the role of the local user is set as broadcaster.\n If you want to record the media streams of a remote user, ensure you have subscribed to the user's media streams before starting the recording.",
    "parameters": [
      {
        "config": "The recording configuration. See MediaRecorderConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid. Ensure the following:\n The specified path of the recording file exists and is writable.\n The specified format of the recording file is supported.\n The maximum recording duration is correctly set.\n -4: IRtcEngine does not support the request. The recording is ongoing or the recording stops because an error occurs.\n -7: The method is called before IRtcEngine is initialized. Ensure the IMediaRecorder object is created before calling this method.",
    "is_hide": true
  },
  {
    "id": "api_imediarecorder_stoprecording",
    "name": "StopRecording",
    "description": "Stops recording audio and video streams.\n\nAfter calling StartRecording, if you want to stop the recording, you must call this method; otherwise, the generated recording files may not be playable.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.\n -7: The method is called before IRtcEngine is initialized. Ensure the IMediaRecorder object is created before calling this method.",
    "is_hide": true
  },
  {
    "id": "api_imusiccontentcenter_createmusicplayer",
    "name": "CreateMusicPlayer",
    "description": "Creates a music player.\n\nIf you need to play music in the music content center, you need to call this method to create a music player before playing.",
    "parameters": [],
    "returns": "The IMusicPlayer instance, if the method call succeeds.\n An empty pointer, if the method call fails.",
    "is_hide": true
  },
  {
    "id": "api_imusiccontentcenter_getcaches",
    "name": "GetCaches",
    "description": "获取已缓存的音乐资源信息。\n\n当你不再需要使用已缓存的音乐资源时，你需要及时释放内存以防止内存泄漏。",
    "parameters": [],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "api_imusiccontentcenter_getlyric",
    "name": "GetLyric",
    "description": "Gets the downloading url for lyrics.\n\nAfter calling this method successfully, the SDK triggers the callback to OnLyricResult report the downloading url for lyrics.",
    "parameters": [
      {
        "requestId": "The request ID. 本次请求的唯一标识。"
      },
      {
        "songCode": "The code of the music, which is an unique identifier of the music."
      },
      {
        "LyricType": "Lyric types:\n 0: xml.\n 1: lrc."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imusiccontentcenter_getmusiccharts",
    "name": "GetMusicCharts",
    "description": "Gets all music charts.\n\nAfter you call this method successfully, the SDK triggers the callback to OnMusicChartsResult report the detailed information of the music charts.",
    "parameters": [
      {
        "requestId": "The request ID. 本次请求的唯一标识。"
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imusiccontentcenter_getmusiccollectionbymusicchartid",
    "name": "GetMusicCollectionByMusicChartId",
    "description": "Gets the music list of a specified music chart through a music chart ID.\n\nAfter call this method successfully, the SDK triggers the callback to report OnMusicCollectionResult the detailed information of music lists.",
    "parameters": [
      {
        "requestId": "The request ID. 本次请求的唯一标识。"
      },
      {
        "musicChartType": "The ID of a music chart, which can be obtained from the OnMusicChartsResult callback. You can also through the corresponding RESTful APIs."
      },
      {
        "page": "当前页面编号，默认从 1 开始。"
      },
      {
        "pageSize": "当前音乐资源列表的总页面数量，最大值为 50。"
      },
      {
        "jsonOption": "The json field. The default value is NULL."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "api_imusiccontentcenter_initialize",
    "name": "Initialize",
    "description": "Initializes . IMusicContentCenter\n\nBefore calling IMusicContentCenter any other methods in the class, you need to call this method to initialize IMusicContentCenter.",
    "parameters": [
      {
        "configuration": "IMusicContentCenter The configuration of . See MusicContentCenterConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imusiccontentcenter_ispreloaded",
    "name": "IsPreloaded",
    "description": "Checks that whether the music has been preloaded.\n\nThis method can be called synchronously. 如需预加载新的音乐资源，可调用 Preload [1/2] 。",
    "parameters": [
      {
        "songCode": "The code of the music, which is an unique identifier of the music."
      }
    ],
    "returns": "0: The method call succeeds and the music has been preloaded.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imusiccontentcenter_registereventhandler",
    "name": "RegisterEventHandler",
    "description": "Registers callback events of the music content center.",
    "parameters": [
      {
        "eventHandler": "Callback events to be registered.See IMusicContentCenterEventHandler."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imusiccontentcenter_removecache",
    "name": "RemoveCache",
    "description": "删除已缓存的音乐资源。\n\n你可以调用该方法删除某一已缓存的音乐资源，如需删除多个音乐资源，你可以多次调用该方法。 The cached media file currently being played will not be deleted.",
    "parameters": [
      {
        "songCode": "待删除的音乐资源的编号。"
      }
    ],
    "returns": "0: 方法调用成功，音乐资源已删除。\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imusiccontentcenter_renewtoken",
    "name": "RenewToken",
    "description": "Renews the token.\n\nYou can call this method to pass in a new Token if your Token expires or is to be expired.",
    "parameters": [
      {
        "token": "The new token."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imusiccontentcenter_searchmusic",
    "name": "SearchMusic",
    "description": "Searches music.\n\nAfter calling this method successfully, the SDK triggers the callback to OnMusicCollectionResult report the list of search results.",
    "parameters": [
      {
        "requestId": "The request ID. 本次请求的唯一标识。"
      },
      {
        "keyword": "搜索关键词，支持歌曲名、歌手搜索。"
      },
      {
        "page": "The page number of the searched music."
      },
      {
        "pageSize": "The number of music displayed on each page. The maximum value is 50."
      },
      {
        "jsonOption": "The json field. The default value is NULL."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "api_imusiccontentcenter_unregistereventhandler",
    "name": "UnregisterEventHandler",
    "description": "取消注册音乐内容中心事件回调。",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imusicontentcenter_preload",
    "name": "Preload [1/2]",
    "description": "Preloads music.\n\nYou can call this method to preload music that you want to play. After calling this method successfully, the SDK triggers the callback to OnPreLoadEvent report preloading events. Before calling this method to preload music, you need to call GetMusicCollectionByMusicChartId or SearchMusic method and obtain the songCode of the music through the callback that triggered OnMusicCollectionResult. 如需销毁 IRtcEngine 对象，请在收到 OnPreloadEvent 回调后，再调用 Dispose 方法。",
    "parameters": [
      {
        "songCode": "The code of the music, which is an unique identifier of the music."
      },
      {
        "jsonOption": "扩展 JSON 字段。 Agora 会根据你在场景字段（ sceneType ）传入的应用场景进行收费。 不同的应用场景对应不同的费率，你可以参考 查看详细的计费。\n 1：直播场景：K 歌及背景音乐播放。\n 2：直播场景：背景音乐播放。\n 3：（默认）语聊场景：K 歌。\n 4：语聊场景：背景音乐播放。\n 5：VR 场景：K 歌及背景音乐播放。 如果需要切换到不同场景，需要重新调用此方法并在该字段中传入 sceneType 的值。 示例： {\"sceneType\":1}"
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_imusicplayer_open",
    "name": "Open",
    "description": "通过音乐资源编号打开音乐资源。\n\nAfter calling this method, the SDK triggers the OnPlayerSourceStateChanged callback. After receiving the report of the playback status as PLAYER_STATE_OPEN_COMPLETED, you can call the Play method to play the media file. If the music you want to open is protected by Digital Rights Management(DRM), you need to call this method to open the music. For music that are not DRM protected, you can either call this method or IMediaPlayer Open method for opening. Before calling this method, make sure the music you want to play has been preloaded. 你可以调用 IsPreloaded 方法来检测音乐资源是否已被预加载，或通过 OnPreLoadEvent 回调得知。",
    "parameters": [
      {
        "songCode": "The code of the music, which is an unique identifier of the music."
      },
      {
        "startPos": "The starting position (ms) for playback. Default value is 0."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_irtcengine_addhandler",
    "name": "InitEventHandler",
    "description": "Adds event handlers\n\nThe SDK uses the IRtcEngineEventHandler class to send callbacks to the app. The app inherits the methods of this class to receive these callbacks. All methods in this class have default (empty) implementations. Therefore, apps only need to inherits callbacks according to the scenarios. In the callbacks, avoid time-consuming tasks or calling APIs that can block the thread, such as the SendStreamMessage method. Otherwise, the SDK may not work properly.",
    "parameters": [
      {
        "engineEventHandler": "Callback events to be added. See IRtcEngineEventHandler."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_addvideowatermark",
    "name": "AddVideoWatermark [1/2]",
    "description": "Adds a watermark image to the local video.\n\nDeprecated: This method is deprecated. Use AddVideoWatermark [2/2] instead. This method adds a PNG watermark image to the local video stream in a live streaming session. Once the watermark image is added, all the users in the channel (CDN audience included) and the video capturing device can see and capture it. If you only want to add a watermark to the CDN live streaming, see StartRtmpStreamWithTranscoding.\n The URL descriptions are different for the local video and CDN live streaming: In a local video stream, URL refers to the absolute path of the added watermark image file in the local video stream. In a CDN live stream, URL refers to the URL address of the added watermark image in the CDN live streaming.\n The source file of the watermark image must be in the PNG file format. If the width and height of the PNG file differ from your settings in this method, the PNG file will be cropped to conform to your settings.\n The Agora SDK supports adding only one watermark image onto a local video or CDN live stream. The newly added watermark image replaces the previous one.",
    "parameters": [
      {
        "watermark": "The watermark image to be added to the local live streaming: RtcImage."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_addvideowatermark2",
    "name": "AddVideoWatermark [2/2]",
    "description": "Adds a watermark image to the local video.\n\nThis method adds a PNG watermark image to the local video in the live streaming. Once the watermark image is added, all the audience in the channel (CDN audience included), and the capturing device can see and capture it. The Agora SDK supports adding only one watermark image onto a local video or CDN live stream. The newly added watermark image replaces the previous one. The watermark coordinates are dependent on the settings in the SetVideoEncoderConfiguration method:\n If the orientation mode of the encoding video (ORIENTATION_MODE) is fixed landscape mode or the adaptive landscape mode, the watermark uses the landscape orientation.\n If the orientation mode of the encoding video (ORIENTATION_MODE) is fixed portrait mode or the adaptive portrait mode, the watermark uses the portrait orientation.\n When setting the watermark position, the region must be less than the dimensions set in the SetVideoEncoderConfiguration method; otherwise, the watermark image will be cropped.\n Ensure that calling this method after EnableVideo.\n If you only want to add a watermark to the media push, you can call this method or the StartRtmpStreamWithTranscoding method.\n This method supports adding a watermark image in the PNG file format only. Supported pixel formats of the PNG image are RGBA, RGB, Palette, Gray, and Alpha_gray.\n If the dimensions of the PNG image differ from your settings in this method, the image will be cropped or zoomed to conform to your settings.\n If you have enabled the mirror mode for the local video, the watermark on the local video is also mirrored. To avoid mirroring the watermark, Agora recommends that you do not use the mirror and watermark functions for the local video at the same time. You can implement the watermark function in your application layer.",
    "parameters": [
      {
        "watermarkUrl": "The local file path of the watermark image to be added. This method supports adding a watermark image from the local absolute or relative file path."
      },
      {
        "options": "The options of the watermark image to be added. See WatermarkOptions."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_adjustaudiomixingplayoutvolume",
    "name": "AdjustAudioMixingPlayoutVolume",
    "description": "Adjusts the volume of audio mixing for local playback.",
    "parameters": [
      {
        "volume": "The volume of audio mixing for local playback. The value ranges between 0 and 100 (default). 100 represents the original volume."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_adjustaudiomixingpublishvolume",
    "name": "AdjustAudioMixingPublishVolume",
    "description": "Adjusts the volume of audio mixing for publishing.\n\nThis method adjusts the volume of audio mixing for publishing (sending to other users).",
    "parameters": [
      {
        "volume": "The volume of audio mixing for local playback. The value ranges between 0 and 100 (default). 100 represents the original volume."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_adjustaudiomixingvolume",
    "name": "AdjustAudioMixingVolume",
    "description": "Adjusts the volume during audio mixing.\n\nThis method adjusts the audio mixing volume on both the local client and remote clients. This method does not affect the volume of the audio file set in the PlayEffect method.",
    "parameters": [
      {
        "volume": "Audio mixing volume. The value ranges between 0 and 100. The default value is 100, which means the original volume."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_adjustcustomaudioplayoutvolume",
    "name": "AdjustCustomAudioPlayoutVolume",
    "description": "Adjusts the volume of the custom audio track played locally.\n\nEnsure you have called the CreateCustomAudioTrack method to create a custom audio track before calling this method. If you want to change the volume of the audio to be played locally, you need to call this method again.",
    "parameters": [
      {
        "volume": "The volume of the audio source. The value can range from 0 to 100. 0 means mute; 100 means the original volume."
      },
      {
        "trackId": "The audio track ID. Set this parameter to the custom audio track ID returned in CreateCustomAudioTrack."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_adjustcustomaudiopublishvolume",
    "name": "AdjustCustomAudioPublishVolume",
    "description": "Adjusts the volume of the custom audio track played remotely.\n\nEnsure you have called the CreateCustomAudioTrack method to create a custom audio track before calling this method. If you want to change the volume of the audio played remotely, you need to call this method again.",
    "parameters": [
      {
        "trackId": "The audio track ID. Set this parameter to the custom audio track ID returned in CreateCustomAudioTrack."
      },
      {
        "volume": "The volume of the audio source. The value can range from 0 to 100. 0 means mute; 100 means the original volume."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_adjustloopbacksignalvolume",
    "name": "AdjustLoopbackSignalVolume",
    "description": "Adjusts the volume of the signal captured by the sound card.\n\nAfter calling EnableLoopbackRecording to enable loopback audio capturing, you can call this method to adjust the volume of the signal captured by the sound card.",
    "parameters": [
      {
        "volume": "Audio mixing volume. The value ranges between 0 and 100. The default value is 100, which means the original volume."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_adjustplaybacksignalvolume",
    "name": "AdjustPlaybackSignalVolume",
    "description": "Adjusts the playback signal volume of all remote users.\n\nThis method is used to adjust the signal volume of all remote users mixed and played locally. If you need to adjust the signal volume of a specified remote user played locally, it is recommended that you call AdjustUserPlaybackSignalVolume instead.",
    "parameters": [
      {
        "volume": "The volume of the user. The value range is [0,400].\n 0: Mute.\n 100: (Default) The original volume.\n 400: Four times the original volume (amplifying the audio signals by four times)."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_adjustrecordingsignalvolume",
    "name": "AdjustRecordingSignalVolume",
    "description": "Adjusts the capturing signal volume.\n\nIf you only need to mute the audio signal, Agora recommends that you use MuteRecordingSignal instead.",
    "parameters": [
      {
        "volume": "The volume of the user. The value range is [0,400].\n 0: Mute.\n 100: (Default) The original volume.\n 400: Four times the original volume (amplifying the audio signals by four times)."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_adjustuserplaybacksignalvolume",
    "name": "AdjustUserPlaybackSignalVolume",
    "description": "Adjusts the playback signal volume of a specified remote user.\n\nYou can call this method to adjust the playback volume of a specified remote user. To adjust the playback volume of different remote users, call the method as many times, once for each remote user.",
    "parameters": [
      {
        "volume": "The volume of the user. The value range is [0,400].\n 0: Mute.\n 100: (Default) The original volume.\n 400: Four times the original volume (amplifying the audio signals by four times)."
      },
      {
        "uid": "The user ID of the remote user."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_clearvideowatermarks",
    "name": "ClearVideoWatermarks",
    "description": "Removes the watermark image from the video stream.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_complain",
    "name": "Complain",
    "description": "Allows a user to complain about the call quality after a call ends.\n\nThis method allows users to complain about the quality of the call. Call this method after the user leaves the channel.",
    "parameters": [
      {
        "callId": "The current call ID. You can get the call ID by calling GetCallId."
      },
      {
        "description": "(Optional) A description of the call. The string length should be less than 800 bytes."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).\n -2: The parameter is invalid.\n -7: The method is called before IRtcEngine is initialized.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_configrhythmplayer",
    "name": "ConfigRhythmPlayer",
    "description": "Configures the virtual metronome.\n\nAfter calling StartRhythmPlayer, you can call this method to reconfigure the virtual metronome.\n After enabling the virtual metronome, the SDK plays the specified audio effect file from the beginning, and controls the playback duration of each file according to beatsPerMinute you set in AgoraRhythmPlayerConfig. For example, if you set beatsPerMinute as 60, the SDK plays one beat every second. If the file duration exceeds the beat duration, the SDK only plays the audio within the beat duration.\n By default, the sound of the virtual metronome is published in the channel. If you want the sound to be heard by the remote users, you can set publishRhythmPlayerTrack in ChannelMediaOptions as true.",
    "parameters": [
      {
        "config": "The metronome configuration. See AgoraRhythmPlayerConfig."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_createcustomvideotrack",
    "name": "CreateCustomVideoTrack",
    "description": "Creates a custom video track.\n\nTo publish a custom video source, see the following steps:\n Call this method to create a video track and get the video track ID.\n Call JoinChannel [2/2] to join the channel. In ChannelMediaOptions, set customVideoTrackId to the video track ID that you want to publish, and set publishCustomVideoTrack to true.\n Call PushVideoFrame and specify videoTrackId as the video track ID set in step 2. You can then publish the corresponding custom video source in the channel.",
    "parameters": [],
    "returns": "If the method call is successful, the video track ID is returned as the unique identifier of the video track.\n If the method call fails, 0xffffffff is returned.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_createdatastream",
    "name": "CreateDataStream [1/2]",
    "description": "Creates a data stream.\n\nYou can call this method to create a data stream and improve the reliability and ordering of data transmission.",
    "parameters": [
      {
        "streamId": "An output parameter; the ID of the data stream created."
      },
      {
        "reliable": "Sets whether the recipients are guaranteed to receive the data stream within five seconds: true : The recipients receive the data from the sender within five seconds. If the recipient does not receive the data within five seconds, the SDK triggers the OnStreamMessageError callback and returns an error code. false : There is no guarantee that the recipients receive the data stream within five seconds and no error message is reported for any delay or missing data stream. Please ensure that reliable and ordered are either both set to true or both set to false."
      },
      {
        "ordered": "Sets whether the recipients receive the data stream in the sent order: true : The recipients receive the data in the sent order. false : The recipients do not receive the data in the sent order."
      }
    ],
    "returns": "0: The data stream is successfully created.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_createdatastream2",
    "name": "CreateDataStream [2/2]",
    "description": "Creates a data stream.\n\nCompared to CreateDataStream [1/2], this method does not guarantee the reliability of data transmission. If a data packet is not received five seconds after it was sent, the SDK directly discards the data.",
    "parameters": [
      {
        "streamId": "An output parameter; the ID of the data stream created."
      },
      {
        "config": "The configurations for the data stream. See DataStreamConfig."
      }
    ],
    "returns": "0: The data stream is successfully created.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_createmediaplayer",
    "name": "CreateMediaPlayer",
    "description": "Creates a media player object.\n\nBefore calling any APIs in the IMediaPlayer class, you need to call this method to create an instance of the media player. If you need to create multiple instances, you can call this method multiple times.",
    "parameters": [],
    "returns": "An IMediaPlayer object, if the method call succeeds.\n An empty pointer, if the method call fails.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_createmediarecorder",
    "name": "CreateMediaRecorder",
    "description": "Creates a recording object for audio and video recording.\n\nBefore you start recording, you need to call this method to create a recording object. Agora SDKs support recording the audio and video streams of both local and remote users. You can call this method as needed to create muitiple recording objects and specify the streams that you want to record through the info parameter. After successfully creating a recording object, you need to call SetMediaRecorderObserver to register a recording observer to listen for recording callbacks, and then call StartRecording to start recording.",
    "parameters": [
      {
        "info": "The information about the media streams you want to record. See RecorderStreamInfo."
      }
    ],
    "returns": "The IMediaRecorder object, if the method call succeeds.\n An empty pointer, if the method call fails.",
    "is_hide": true
  },
  {
    "id": "api_irtcengine_destroycustomvideotrack",
    "name": "DestroyCustomVideoTrack",
    "description": "Destroys the specified video track.",
    "parameters": [
      {
        "video_track_id": "The video track ID returned by calling the CreateCustomVideoTrack method."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_destroymediaplayer",
    "name": "DestroyMediaPlayer",
    "description": "Destroys the media player instance.",
    "parameters": [
      {
        "mediaPlayer": "One IMediaPlayer object."
      }
    ],
    "returns": "≥ 0: Success. Returns the ID of media player instance.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_destroymediarecorder",
    "name": "DestroyMediaRecorder",
    "description": "Destroys a recording object for audio and video recording.\n\nWhen you do not need to record any audio and video streams, you can call this method to destroy the recording object. Before you call this method, if you are recording a media stream, you need to call StopRecording to stop recording.",
    "parameters": [
      {
        "mediaRecorder": "待销毁的 IMediaRecorder 对象。"
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_irtcengine_disableaudio",
    "name": "DisableAudio",
    "description": "Disables the audio module.\n\nThe audio module is enabled by default, and you can call this method to disable the audio module.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_disableaudiospectrummonitor",
    "name": "DisableAudioSpectrumMonitor",
    "description": "Disables audio spectrum monitoring.\n\nAfter calling EnableAudioSpectrumMonitor, if you want to disable audio spectrum monitoring, you can call this method. You can call this method either before or after joining a channel.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_disablevideo",
    "name": "DisableVideo",
    "description": "Disables the video module.\n\nThis method is used to disable the video module.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enableaudio",
    "name": "EnableAudio",
    "description": "Enables the audio module.\n\nThe audio module is enabled by default After calling DisableAudio to disable the audio module, you can call this method to re-enable it.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enableaudiospectrummonitor",
    "name": "EnableAudioSpectrumMonitor",
    "description": "Turns on audio spectrum monitoring.\n\nIf you want to obtain the audio spectrum data of local or remote users, you can register the audio spectrum observer and enable audio spectrum monitoring. You can call this method either before or after joining a channel.",
    "parameters": [
      {
        "intervalInMS": "The interval (in milliseconds) at which the SDK triggers the OnLocalAudioSpectrum and OnRemoteAudioSpectrum callbacks. The default value is 100. Do not set this parameter to a value less than 10, otherwise calling this method would fail."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: Invalid parameters.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enableaudiovolumeindication",
    "name": "EnableAudioVolumeIndication",
    "description": "Enables the reporting of users' volume indication.\n\nThis method enables the SDK to regularly report the volume information to the app of the local user who sends a stream and remote users (three users at most) whose instantaneous volumes are the highest.",
    "parameters": [
      {
        "interval": "Sets the time interval between two consecutive volume indications:\n ≤ 0: Disables the volume indication.\n > 0: Time interval (ms) between two consecutive volume indications. Ensure this parameter is set to a value greater than 10, otherwise you will not receive the OnAudioVolumeIndication callback. Agora recommends that this value is set as greater than 100."
      },
      {
        "smooth": "The smoothing factor that sets the sensitivity of the audio volume indicator. The value ranges between 0 and 10. The recommended value is 3. The greater the value, the more sensitive the indicator."
      },
      {
        "reportVad": "true : Enables the voice activity detection of the local user. Once it is enabled, the vad parameter of the OnAudioVolumeIndication callback reports the voice activity status of the local user. false : (Default) Disables the voice activity detection of the local user. Once it is disabled, the vad parameter of the OnAudioVolumeIndication callback does not report the voice activity status of the local user, except for the scenario where the engine automatically detects the voice activity of the local user."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablecameracenterstage",
    "name": "EnableCameraCenterStage",
    "description": "Enables or disables portrait center stage.\n\nThe portrait center stage feature is off by default. You need to call this method to turn it on. If you need to disable this feature, you need to call this method again and set enabled to false. This method is for iOS and macOS only.",
    "parameters": [
      {
        "enabled": "Whether to enable the portrait center stage: true : Enable portrait center stage. false : Disable portrait center stage."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablecontentinspect",
    "name": "EnableContentInspect",
    "description": "Enables or disables video screenshot and upload.\n\nWhen video screenshot and upload function is enabled, the SDK takes screenshots and uploads videos sent by local users based on the type and frequency of the module you set in ContentInspectConfig. After video screenshot and upload, the Agora server sends the callback notification to your app server in HTTPS requests and sends all screenshots to the third-party cloud storage service.",
    "parameters": [
      {
        "enabled": "Whether to enalbe video screenshot and upload: true : Enables video screenshot and upload. false : Disables video screenshot and upload."
      },
      {
        "config": "Screenshot and upload configuration. See ContentInspectConfig."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablecustomaudiolocalplayback",
    "name": "EnableCustomAudioLocalPlayback",
    "description": "Sets whether to enable the local playback of external audio source.\n\nEnsure you have called the CreateCustomAudioTrack method to create a custom audio track before calling this method. After calling this method to enable the local playback of external audio source, if you need to stop local playback, you can call this method again and set enabled to false. You can call AdjustCustomAudioPlayoutVolume to adjust the local playback volume of the custom audio track.",
    "parameters": [
      {
        "trackId": "The audio track ID. Set this parameter to the custom audio track ID returned in CreateCustomAudioTrack."
      },
      {
        "enabled": "Whether to play the external audio source: true : Play the external audio source. false : (Default) Do not play the external source."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enabledualstreammode",
    "name": "EnableDualStreamMode [1/2]",
    "description": "Enables or disables dual-stream mode on the sender side.\n\nThis method is applicable to all types of streams from the sender, including but not limited to video streams collected from cameras, screen sharing streams, and custom-collected video streams.\n If you need to enable dual video streams in a multi-channel scenario, you can call the EnableDualStreamModeEx method.\n You can call this method either before or after joining a channel. Deprecated: This method is deprecated as of v4.2.0. Use SetDualStreamMode [1/2] instead. Dual streams are a pairing of a high-quality video stream and a low-quality video stream:\n High-quality video stream: High bitrate, high resolution.\n Low-quality video stream: Low bitrate, low resolution. After you enable dual-stream mode, you can call SetRemoteVideoStreamType to choose to receive either the high-quality video stream or the low-quality video stream on the subscriber side.",
    "parameters": [
      {
        "enabled": "Whether to enable dual-stream mode: true : Enable dual-stream mode. false : (Default) Disable dual-stream mode."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enabledualstreammode2",
    "name": "",
    "description": "Enables or disables dual-stream mode.\n\nDeprecated: This method is deprecated as of v4.2.0. Use SetDualStreamMode [2/2] instead. You can call this method to enable or disable the dual-stream mode on the publisher side. Dual streams are a pairing of a high-quality video stream and a low-quality video stream:\n High-quality video stream: High bitrate, high resolution.\n Low-quality video stream: Low bitrate, low resolution. After you enable dual-stream mode, you can call SetRemoteVideoStreamType to choose to receive either the high-quality video stream or the low-quality video stream on the subscriber side.\n This method is applicable to all types of streams from the sender, including but not limited to video streams collected from cameras, screen sharing streams, and custom-collected video streams.\n If you need to enable dual video streams in a multi-channel scenario, you can call the EnableDualStreamModeEx method.\n You can call this method either before or after joining a channel.",
    "parameters": [
      {
        "sourceType": "The type of the video source. See VIDEO_SOURCE_TYPE."
      },
      {
        "enabled": "Whether to enable dual-stream mode: true : Enable dual-stream mode. false : Disable dual-stream mode."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_irtcengine_enabledualstreammode3",
    "name": "EnableDualStreamMode [2/2]",
    "description": "Sets the dual-stream mode on the sender side and the low-quality video stream.\n\nDeprecated: This method is deprecated as of v4.2.0. Use SetDualStreamMode [2/2] instead. You can call this method to enable or disable the dual-stream mode on the publisher side. Dual streams are a pairing of a high-quality video stream and a low-quality video stream:\n High-quality video stream: High bitrate, high resolution.\n Low-quality video stream: Low bitrate, low resolution. After you enable dual-stream mode, you can call SetRemoteVideoStreamType to choose to receive either the high-quality video stream or the low-quality video stream on the subscriber side.\n This method is applicable to all types of streams from the sender, including but not limited to video streams collected from cameras, screen sharing streams, and custom-collected video streams.\n If you need to enable dual video streams in a multi-channel scenario, you can call the EnableDualStreamModeEx method.\n You can call this method either before or after joining a channel.",
    "parameters": [
      {
        "enabled": "Whether to enable dual-stream mode: true : Enable dual-stream mode. false : (Default) Disable dual-stream mode."
      },
      {
        "streamConfig": "The configuration of the low-quality video stream. See SimulcastStreamConfig. When setting mode to DISABLE_SIMULCAST_STREAM, setting streamConfig will not take effect."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enableechocancellationexternal",
    "name": "enableEchoCancellationExternal",
    "description": "Enables or disables the custom echo cancellation.\n\nAfter calling this method, you can push external audio frames to the custom audio module for echo cancellation. Before calling this method, ensure it is called before joining a channel and SetExternalAudioSink is called.",
    "parameters": [
      {
        "enabled": "Set whether to enable custom echo cancellation: true : Enable echo cancellation. false : Disable echo cancellation."
      },
      {
        "audioSourceDelay": "Sets the time (in milliseconds) between pushing audio frames and publishing audio frames. The value range is [0, 30].\n To call, ensure this parameter is set to 0.\n To call or process the audio frame captured by the sound card, ensure that this parameter is an integer multiple of 10."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_irtcengine_enableencryption",
    "name": "EnableEncryption",
    "description": "Enables or disables the built-in encryption.\n\nAfter the user leaves the channel, the SDK automatically disables the built-in encryption. To enable the built-in encryption, call this method before the user joins the channel again.",
    "parameters": [
      {
        "enabled": "Whether to enable built-in encryption: true : Enable the built-in encryption. false : (Default) Disable the built-in encryption."
      },
      {
        "config": "Built-in encryption configurations. See EncryptionConfig."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: An invalid parameter is used. Set the parameter with a valid value.\n -4: The built-in encryption mode is incorrect or the SDK fails to load the external encryption library. Check the enumeration or reload the external encryption library.\n -7: The SDK is not initialized. Initialize the IRtcEngine instance before calling this method.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enableextension",
    "name": "EnableExtension",
    "description": "Enables or disables extensions.",
    "parameters": [
      {
        "extension": "The name of the extension."
      },
      {
        "provider": "The name of the extension provider."
      },
      {
        "enable": "Whether to enable the extension: true : Enable the extension. false : Disable the extension."
      },
      {
        "type": "Source type of the extension. See MEDIA_SOURCE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -3: The extension library is not loaded. Agora recommends that you check the storage location or the name of the dynamic library.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablefacedetection",
    "name": "EnableFaceDetection",
    "description": "Enables or disables face detection for the local user.\n\nThis method is for Android and iOS only.",
    "parameters": [
      {
        "enabled": "Whether to enable face detection for the local user: true : Enable face detection. false : (Default) Disable face detection."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enableinearmonitoring2",
    "name": "EnableInEarMonitoring",
    "description": "Enables in-ear monitoring.\n\nThis method enables or disables in-ear monitoring.",
    "parameters": [
      {
        "enabled": "Enables or disables in-ear monitoring. true : Enables in-ear monitoring. false : (Default) Disables in-ear monitoring."
      },
      {
        "includeAudioFilters": "The audio filter types of in-ear monitoring. See EAR_MONITORING_FILTER_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n - 8: Make sure the current audio routing is Bluetooth or headset.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enableinstantmediarendering",
    "name": "EnableInstantMediaRendering",
    "description": "Enables audio and video frame instant rendering.\n\nAfter successfully calling this method, the SDK enables the instant frame rendering mode, which can speed up the first frame rendering after the user joins the channel.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.\n -7: The method is called before IRtcEngine is initialized.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablelocalaudio",
    "name": "EnableLocalAudio",
    "description": "Enables or disables the local audio capture.\n\nThe audio function is enabled by default when users joining a channel. This method disables or re-enables the local audio function to stop or restart local audio capturing. The difference between this method and MuteLocalAudioStream are as follows: EnableLocalAudio : Disables or re-enables the local audio capturing and processing. If you disable or re-enable local audio capturing using the EnableLocalAudio method, the local user might hear a pause in the remote audio playback. MuteLocalAudioStream : Sends or stops sending the local audio streams without affecting the audio capture status.",
    "parameters": [
      {
        "enabled": "true : (Default) Re-enable the local audio function, that is, to start the local audio capturing device (for example, the microphone). false : Disable the local audio function, that is, to stop local audio capturing."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablelocalvideo",
    "name": "EnableLocalVideo",
    "description": "Enables/Disables the local video capture.\n\nThis method disables or re-enables the local video capture, and does not affect receiving the remote video stream. After calling EnableVideo, the local video capture is enabled by default. If you call EnableLocalVideo (false) to disable local video capture within the channel, it also simultaneously stops publishing the video stream within the channel. If you want to restart video catpure, you can call EnableLocalVideo (true) and then call UpdateChannelMediaOptions to set the options parameter to publish the locally captured video stream in the channel. After the local video capturer is successfully disabled or re-enabled, the SDK triggers the OnRemoteVideoStateChanged callback on the remote client.\n You can call this method either before or after joining a channel.\n This method enables the internal engine and is valid after leaving the channel.",
    "parameters": [
      {
        "enabled": "Whether to enable the local video capture. true : (Default) Enable the local video capture. false : Disable the local video capture. Once the local video is disabled, the remote users cannot receive the video stream of the local user, while the local user can still receive the video streams of remote users. When set to false, this method does not require a local camera."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enableloopbackrecording",
    "name": "EnableLoopbackRecording",
    "description": "Enables loopback audio capturing.\n\nIf you enable loopback audio capturing, the output of the sound card is mixed into the audio stream sent to the other end.\n This method applies to the macOS and Windows only.\n The macOS system's default sound card does not support recording functionality. As of v4.5.0, when you call this method for the first time, the SDK will automatically install the built-in AgoraALD virtual sound card developed by Agora. After successful installation, the audio routing will automatically switch to the virtual sound card and use it for audio capturing.\n You can call this method either before or after joining a channel.\n If you call the DisableAudio method to disable the audio module, audio capturing will be disabled as well. If you need to enable audio capturing, call the EnableAudio method to enable the audio module and then call the EnableLoopbackRecording method.",
    "parameters": [
      {
        "enabled": "Sets whether to enable loopback audio capturing. true : Enable sound card capturing. You can find the name of the virtual sound card in your system's Audio Devices > Output. false : Disable sound card capturing. The name of the virtual sound card will not be shown in your system's Audio Devices > Output."
      },
      {
        "deviceName": "macOS: The device name of the virtual sound card. The default value is set to NULL, which means using AgoraALD for loopback audio capturing.\n Windows: The device name of the sound card. The default is set to NULL, which means the SDK uses the sound card of your device for loopback audio capturing."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablemulticamera",
    "name": "EnableMultiCamera",
    "description": "Enables or disables multi-camera capture.\n\nIn scenarios where there are existing cameras to capture video, Agora recommends that you use the following steps to capture and publish video with multiple cameras:\n Call this method to enable multi-channel camera capture.\n Call StartPreview [2/2] to start the local video preview.\n Call StartCameraCapture, and set sourceType to start video capture with the second camera.\n Call JoinChannelEx, and set publishSecondaryCameraTrack to true to publish the video stream captured by the second camera in the channel. If you want to disable multi-channel camera capture, use the following steps:\n Call StopCameraCapture.\n Call this method with enabled set to false. You can call this method before and after StartPreview [2/2] to enable multi-camera capture:\n If it is enabled before StartPreview [2/2], the local video preview shows the image captured by the two cameras at the same time.\n If it is enabled after StartPreview [2/2], the SDK stops the current camera capture first, and then enables the primary camera and the second camera. The local video preview appears black for a short time, and then automatically returns to normal. This method applies to iOS only. When using this function, ensure that the system version is 13.0 or later. The minimum iOS device types that support multi-camera capture are as follows:\n iPhone XR\n iPhone XS\n iPhone XS Max\n iPad Pro 3rd generation and later",
    "parameters": [
      {
        "enabled": "Whether to enable multi-camera video capture mode: true : Enable multi-camera capture mode; the SDK uses multiple cameras to capture video. false : Disable multi-camera capture mode; the SDK uses a single camera to capture video."
      },
      {
        "config": "Capture configuration for the second camera. See CameraCapturerConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablesoundpositionindication",
    "name": "EnableSoundPositionIndication",
    "description": "Enables or disables stereo panning for remote users.\n\nEnsure that you call this method before joining a channel to enable stereo panning for remote users so that the local user can track the position of a remote user by calling SetRemoteVoicePosition.",
    "parameters": [
      {
        "enabled": "Whether to enable stereo panning for remote users: true : Enable stereo panning. false : Disable stereo panning."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablespatialaudio",
    "name": "EnableSpatialAudio",
    "description": "Enables or disables the spatial audio effect.\n\nAfter enabling the spatial audio effect, you can call SetRemoteUserSpatialAudioParams to set the spatial audio effect parameters of the remote user.\n You can call this method either before or after joining a channel.\n This method relies on the spatial audio dynamic library libagora_spatial_audio_extension.dll. If the dynamic library is deleted, the function cannot be enabled normally.",
    "parameters": [
      {
        "enabled": "Whether to enable the spatial audio effect: true : Enable the spatial audio effect. false : Disable the spatial audio effect."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablevideo",
    "name": "EnableVideo",
    "description": "Enables the video module.\n\nThe video module is disabled by default, call this method to enable it. If you need to disable the video module later, you need to call DisableVideo.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablevideoimagesource",
    "name": "EnableVideoImageSource",
    "description": "Sets whether to replace the current video feeds with images when publishing video streams.\n\nWhen publishing video streams, you can call this method to replace the current video feeds with custom images. Once you enable this function, you can select images to replace the video feeds through the ImageTrackOptions parameter. If you disable this function, the remote users see the video feeds that you publish.",
    "parameters": [
      {
        "enable": "Whether to replace the current video feeds with custom images: true : Replace the current video feeds with custom images. false : (Default) Do not replace the current video feeds with custom images."
      },
      {
        "options": "Image configurations. See ImageTrackOptions."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablevirtualbackground",
    "name": "EnableVirtualBackground",
    "description": "Enables/Disables the virtual background.\n\nThe virtual background feature enables the local user to replace their original background with a static image, dynamic video, blurred background, or portrait-background segmentation to achieve picture-in-picture effect. Once the virtual background feature is enabled, all users in the channel can see the custom background. Call this method after calling EnableVideo or StartPreview [2/2].\n This feature has high requirements on device performance. When calling this method, the SDK automatically checks the capabilities of the current device. Agora recommends you use virtual background on devices with the following processors:\n Snapdragon 700 series 750G and later\n Snapdragon 800 series 835 and later\n Dimensity 700 series 720 and later\n Kirin 800 series 810 and later\n Kirin 900 series 980 and later\n Devices with an i5 CPU and better\n Devices with an A9 chip and better, as follows:\n iPhone 6S and later\n iPad Air 3rd generation and later\n iPad 5th generation and later\n iPad Pro 1st generation and later\n iPad mini 5th generation and later\n Agora recommends that you use this feature in scenarios that meet the following conditions:\n A high-definition camera device is used, and the environment is uniformly lit.\n There are few objects in the captured video. Portraits are half-length and unobstructed. Ensure that the background is a solid color that is different from the color of the user's clothing.\n This method relies on the virtual background dynamic library libagora_segmentation_extension.dll. If the dynamic library is deleted, the function cannot be enabled normally.",
    "parameters": [
      {
        "type": "The type of the media source to which the filter effect is applied. See MEDIA_SOURCE_TYPE. In this method, this parameter supports only the following two settings:\n Use the default value PRIMARY_CAMERA_SOURCE if you use camera to capture local video.\n Set this parameter to CUSTOM_VIDEO_SOURCE if you use custom video source."
      },
      {
        "enabled": "Whether to enable virtual background: true : Enable virtual background. false : Disable virtual background."
      },
      {
        "backgroundSource": "The custom background. See VirtualBackgroundSource. To adapt the resolution of the custom background image to that of the video captured by the SDK, the SDK scales and crops the custom background image while ensuring that the content of the custom background image is not distorted."
      },
      {
        "segproperty": "Processing properties for background images. See SegmentationProperty."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -4: The device capabilities do not meet the requirements for the virtual background feature. Agora recommends you try it on devices with higher performance.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablevoiceaituner",
    "name": "EnableVoiceAITuner",
    "description": "Enables or disables the voice AI tuner.\n\nThe voice AI tuner supports enhancing sound quality and adjusting tone style.",
    "parameters": [
      {
        "enabled": "Whether to enable the voice AI tuner: true : Enables the voice AI tuner. false : (Default) Disable the voice AI tuner."
      },
      {
        "type": "Voice AI tuner sound types, see VOICE_AI_TUNER_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_enablewebsdkinteroperability",
    "name": "EnableWebSdkInteroperability",
    "description": "Enables interoperability with the Agora Web SDK (applicable only in the live streaming scenarios).\n\nDeprecated: The SDK automatically enables interoperability with the Web SDK, so you no longer need to call this method. You can call this method to enable or disable interoperability with the Agora Web SDK. If a channel has Web SDK users, ensure that you call this method, or the video of the Native user will be a black screen for the Web user. This method is only applicable in live streaming scenarios, and interoperability is enabled by default in communication scenarios.",
    "parameters": [
      {
        "enabled": "Whether to enable interoperability: true : Enable interoperability. false : (Default) Disable interoperability."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getaudiodeviceinfo",
    "name": "GetAudioDeviceInfo",
    "description": "Gets the audio device information.\n\nAfter calling this method, you can get whether the audio device supports ultra-low-latency capture and playback.\n This method is for Android only.\n You can call this method either before or after joining a channel.",
    "parameters": [
      {
        "deviceInfo": "Audio frame information. See DeviceInfoMobile."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getaudiodevicemanager",
    "name": "GetAudioDeviceManager",
    "description": "Gets the IAudioDeviceManager object to manage audio devices.",
    "parameters": [],
    "returns": "One IAudioDeviceManager object.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getaudiomixingcurrentposition",
    "name": "GetAudioMixingCurrentPosition",
    "description": "Retrieves the playback position (ms) of the music file.\n\nRetrieves the playback position (ms) of the audio. You need to call this method after calling StartAudioMixing [2/2] and receiving the OnAudioMixingStateChanged (AUDIO_MIXING_STATE_PLAYING) callback.\n If you need to call GetAudioMixingCurrentPosition multiple times, ensure that the time interval between calling this method is more than 500 ms.",
    "parameters": [],
    "returns": "≥ 0: The current playback position (ms) of the audio mixing, if this method call succeeds. 0 represents that the current music file does not start playing.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getaudiomixingduration",
    "name": "GetAudioMixingDuration",
    "description": "Retrieves the duration (ms) of the music file.\n\nRetrieves the total duration (ms) of the audio.",
    "parameters": [],
    "returns": "≥ 0: The audio mixing duration, if this method call succeeds.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getaudiomixingplayoutvolume",
    "name": "GetAudioMixingPlayoutVolume",
    "description": "Retrieves the audio mixing volume for local playback.\n\nYou can call this method to get the local playback volume of the mixed audio file, which helps in troubleshooting volume‑related issues.",
    "parameters": [],
    "returns": "≥ 0: The audio mixing volume, if this method call succeeds. The value range is [0,100].\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getaudiomixingpublishvolume",
    "name": "GetAudioMixingPublishVolume",
    "description": "Retrieves the audio mixing volume for publishing.\n\nThis method helps troubleshoot audio volume‑related issues. You need to call this method after calling StartAudioMixing [2/2] and receiving the OnAudioMixingStateChanged (AUDIO_MIXING_STATE_PLAYING) callback.",
    "parameters": [],
    "returns": "≥ 0: The audio mixing volume, if this method call succeeds. The value range is [0,100].\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getaudiotrackcount",
    "name": "GetAudioTrackCount",
    "description": "Gets the index of audio tracks of the current music file.\n\nYou need to call this method after calling StartAudioMixing [2/2] and receiving the OnAudioMixingStateChanged (AUDIO_MIXING_STATE_PLAYING) callback.",
    "parameters": [],
    "returns": "The SDK returns the index of the audio tracks if the method call succeeds.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getcallid",
    "name": "GetCallId",
    "description": "Retrieves the call ID.\n\nWhen a user joins a channel on a client, a callId is generated to identify the call from the client. You can call this method to get callId, and pass it in when calling methods such as Rate and Complain.",
    "parameters": [
      {
        "callId": "Output parameter, the current call ID."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getcameramaxzoomfactor",
    "name": "GetCameraMaxZoomFactor",
    "description": "Gets the maximum zoom ratio supported by the camera.\n\nThis method must be called after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n This method is for Android and iOS only.",
    "parameters": [],
    "returns": "The maximum zoom factor.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getconnectionstate",
    "name": "GetConnectionState",
    "description": "Gets the current connection state of the SDK.",
    "parameters": [],
    "returns": "The current connection state. See CONNECTION_STATE_TYPE.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getcurrentmonotonictimeinms",
    "name": "GetCurrentMonotonicTimeInMs",
    "description": "Gets the current Monotonic Time of the SDK.\n\nMonotonic Time refers to a monotonically increasing time series whose value increases over time. The unit is milliseconds. In custom video capture and custom audio capture scenarios, in order to ensure audio and video synchronization, Agora recommends that you call this method to obtain the current Monotonic Time of the SDK, and then pass this value into the timestamp parameter in the captured video frame (VideoFrame) and audio frame (AudioFrame).",
    "parameters": [],
    "returns": "≥0: The method call is successful, and returns the current Monotonic Time of the SDK (in milliseconds).\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_geteffectcurrentposition",
    "name": "GetEffectCurrentPosition",
    "description": "Retrieves the playback position of the audio effect file.\n\nCall this method after PlayEffect.",
    "parameters": [
      {
        "soundId": "The audio effect ID. The ID of each audio effect file is unique."
      }
    ],
    "returns": "The playback position (ms) of the specified audio effect file, if the method call succeeds.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_geteffectduration",
    "name": "GetEffectDuration",
    "description": "Retrieves the duration of the audio effect file.\n\nCall this method after joining a channel.",
    "parameters": [
      {
        "filePath": "File path:\n Android: The file path, which needs to be accurate to the file name and suffix. Agora supports URL addresses, absolute paths, or file paths that start with /assets/. You might encounter permission issues if you use an absolute path to access a local file, so Agora recommends using a URI address instead. For example : content://com.android.providers.media.documents/document/audio%3A14441\n Windows: The absolute path or URL address (including the suffixes of the filename) of the audio effect file. For example : C:\\music\\audio.mp4.\n iOS or macOS: The absolute path or URL address (including the suffixes of the filename) of the audio effect file. For example: /var/mobile/Containers/Data/audio.mp4."
      }
    ],
    "returns": "The total duration (ms) of the specified audio effect file, if the method call succeeds.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_geteffectsvolume",
    "name": "GetEffectsVolume",
    "description": "Retrieves the volume of the audio effects.\n\nThe volume is an integer ranging from 0 to 100. The default value is 100, which means the original volume. Call this method after PlayEffect.",
    "parameters": [],
    "returns": "Volume of the audio effects, if this method call succeeds.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_geterrordescription",
    "name": "GetErrorDescription",
    "description": "Gets the warning or error description.",
    "parameters": [
      {
        "code": "The error code reported by the SDK."
      }
    ],
    "returns": "The specific error description.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getextensionproperty2",
    "name": "GetExtensionProperty",
    "description": "Gets detailed information on the extensions.",
    "parameters": [
      {
        "provider": "An output parameter. The name of the extension provider."
      },
      {
        "extension": "An output parameter. The name of the extension."
      },
      {
        "key": "An output parameter. The key of the extension."
      },
      {
        "value": "An output parameter. The value of the extension key."
      },
      {
        "type": "Source type of the extension. See MEDIA_SOURCE_TYPE."
      },
      {
        "buf_len": "Maximum length of the JSON string indicating the extension property. The maximum value is 512 bytes."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getlocalspatialaudioengine",
    "name": "GetLocalSpatialAudioEngine",
    "description": "Gets one ILocalSpatialAudioEngine object.\n\nMake sure the IRtcEngine is initialized before you call this method.",
    "parameters": [],
    "returns": "One ILocalSpatialAudioEngine object.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getmusiccontentcenter.dita",
    "name": "GetMusicContentCenter",
    "description": "Gets IMusicContentCenter.",
    "parameters": [],
    "returns": "One IMusicContentCenter object.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getnativehandle",
    "name": "GetNativeHandler",
    "description": "Gets the C++ handle of the Native SDK.\n\nThis method retrieves the C++ handle of the SDK, which is used for registering the audio and video frame observer.",
    "parameters": [
      {
        "nativeHandler": "Output parameter, the native handle of the SDK."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getnetworktype",
    "name": "GetNetworkType",
    "description": "Gets the type of the local network connection.\n\nYou can use this method to get the type of network in use at any stage. You can call this method either before or after joining a channel.",
    "parameters": [],
    "returns": "≥ 0: The method call is successful, and the local network connection type is returned.\n 0: The SDK disconnects from the network.\n 1: The network type is LAN.\n 2: The network type is Wi-Fi (including hotspots).\n 3: The network type is mobile 2G.\n 4: The network type is mobile 3G.\n 5: The network type is mobile 4G.\n 6: The network type is mobile 5G.\n < 0: The method call failed with an error code.\n -1: The network type is unknown.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getntpwalltimeinms",
    "name": "GetNtpWallTimeInMs",
    "description": "Gets the current NTP (Network Time Protocol) time.\n\nIn the real-time chorus scenario, especially when the downlink connections are inconsistent due to network issues among multiple receiving ends, you can call this method to obtain the current NTP time as the reference time, in order to align the lyrics and music of multiple receiving ends and achieve chorus synchronization.",
    "parameters": [],
    "returns": "The Unix timestamp (ms) of the current NTP time.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getscreencapturesources",
    "name": "GetScreenCaptureSources",
    "description": "Gets a list of shareable screens and windows.\n\nYou can call this method before sharing a screen or window to get a list of shareable screens and windows, which enables a user to use thumbnails in the list to easily choose a particular screen or window to share. This list also contains important information such as window ID and screen ID, with which you can call StartScreenCaptureByWindowId or StartScreenCaptureByDisplayId to start the sharing. This method applies to macOS and Windows only.",
    "parameters": [
      {
        "thumbSize": "The target size of the screen or window thumbnail (the width and height are in pixels). The SDK scales the original image to make the length of the longest side of the image the same as that of the target size without distorting the original image. For example, if the original image is 400 × 300 and thumbSize is 100 × 100, the actual size of the thumbnail is 100 × 75. If the target size is larger than the original size, the thumbnail is the original image and the SDK does not scale it."
      },
      {
        "iconSize": "The target size of the icon corresponding to the application program (the width and height are in pixels). The SDK scales the original image to make the length of the longest side of the image the same as that of the target size without distorting the original image. For example, if the original image is 400 × 300 and iconSize is 100 × 100, the actual size of the icon is 100 × 75. If the target size is larger than the original size, the icon is the original image and the SDK does not scale it."
      },
      {
        "includeScreen": "Whether the SDK returns the screen information in addition to the window information: true : The SDK returns screen and window information. false : The SDK returns window information only."
      }
    ],
    "returns": "The ScreenCaptureSourceInfo array.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getuserinfobyuid",
    "name": "GetUserInfoByUid",
    "description": "Gets the user information by passing in the user ID.\n\nAfter a remote user joins the channel, the SDK gets the UID and user account of the remote user, caches them in a mapping table object, and triggers the OnUserInfoUpdated callback on the local client. After receiving the callback, you can call this method and passi in the UID.to get the user account of the specified user from the UserInfo object.",
    "parameters": [
      {
        "uid": "The user ID."
      },
      {
        "userInfo": "Input and output parameter. The UserInfo object that identifies the user information.\n Input value: A UserInfo object.\n Output: A UserInfo object that contains both the user account and UID."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getuserinfobyuseraccount",
    "name": "GetUserInfoByUserAccount",
    "description": "Gets the user information by passing in the user account.\n\nAfter a remote user joins the channel, the SDK gets the UID and user account of the remote user, caches them in a mapping table object, and triggers the OnUserInfoUpdated callback on the local client. After receiving the callback, you can call this method and pass in the user account to get the UID of the remote user from the UserInfo object.",
    "parameters": [
      {
        "userInfo": "Input and output parameter. The UserInfo object that identifies the user information.\n Input value: A UserInfo object.\n Output: A UserInfo object that contains both the user account and UID."
      },
      {
        "userAccount": "The user account."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getversion",
    "name": "GetVersion",
    "description": "Gets the SDK version.",
    "parameters": [
      {
        "build": "The SDK build index."
      }
    ],
    "returns": "The SDK version number. The format is a string.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getvideodevicemanager",
    "name": "GetVideoDeviceManager",
    "description": "Gets the IVideoDeviceManager object to manage video devices.",
    "parameters": [],
    "returns": "One IVideoDeviceManager object.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_getvolumeofeffect",
    "name": "GetVolumeOfEffect",
    "description": "Gets the volume of a specified audio effect file.",
    "parameters": [
      {
        "soundId": "The ID of the audio effect file."
      }
    ],
    "returns": "≥ 0: Returns the volume of the specified audio effect, if the method call is successful. The value ranges between 0 and 100. 100 represents the original volume.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_initialize",
    "name": "Initialize",
    "description": "Initializes IRtcEngine.\n\nAll called methods provided by the IRtcEngine class are executed asynchronously. Agora recommends calling these methods in the same thread.",
    "parameters": [
      {
        "context": "Configurations for the IRtcEngine instance. See RtcEngineContext."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).\n -2: The parameter is invalid.\n -7: The SDK is not initialized.\n -22: The resource request failed. The SDK fails to allocate resources because your app consumes too much system resource or the system resources are insufficient.\n -101: The App ID is invalid.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_iscameraautoexposurefacemodesupported",
    "name": "IsCameraAutoExposureFaceModeSupported",
    "description": "Checks whether the device supports auto exposure.\n\nThis method must be called after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n This method applies to iOS only.",
    "parameters": [],
    "returns": "true : The device supports auto exposure. false : The device does not support auto exposure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_iscameraautofocusfacemodesupported",
    "name": "IsCameraAutoFocusFaceModeSupported",
    "description": "Checks whether the device supports the face auto-focus function.\n\nThis method must be called after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n This method is for Android and iOS only.",
    "parameters": [],
    "returns": "true : The device supports the face auto-focus function. false : The device does not support the face auto-focus function.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_iscameracenterstagesupported",
    "name": "IsCameraCenterStageSupported",
    "description": "Check if the camera supports portrait center stage.\n\nThis method is for iOS and macOS only. Before calling EnableCameraCenterStage to enable portrait center stage, it is recommended to call this method to check if the current device supports the feature.",
    "parameters": [],
    "returns": "true : The current camera supports the portrait center stage. false : The current camera supports the portrait center stage.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_iscameraexposurepositionsupported",
    "name": "IsCameraExposurePositionSupported",
    "description": "Checks whether the device supports manual exposure.\n\nThis method must be called after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n This method is for Android and iOS only.",
    "parameters": [],
    "returns": "true : The device supports manual exposure. false : The device does not support manual exposure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_iscameraexposuresupported",
    "name": "IsCameraExposureSupported",
    "description": "Queries whether the current camera supports adjusting exposure value.\n\nThis method is for Android and iOS only.\n This method must be called after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n Before calling SetCameraExposureFactor, Agora recoomends that you call this method to query whether the current camera supports adjusting the exposure value.\n By calling this method, you adjust the exposure value of the currently active camera, that is, the camera specified when calling SetCameraCapturerConfiguration.",
    "parameters": [],
    "returns": "true : Success. false : Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_iscamerafacedetectsupported",
    "name": "IsCameraFaceDetectSupported",
    "description": "Checks whether the device camera supports face detection.\n\nThis method must be called after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n This method is for Android and iOS only.",
    "parameters": [],
    "returns": "true : The device camera supports face detection. false : The device camera does not support face detection.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_iscamerafocussupported",
    "name": "IsCameraFocusSupported",
    "description": "Check whether the device supports the manual focus function.\n\nThis method must be called after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n This method is for Android and iOS only.",
    "parameters": [],
    "returns": "true : The device supports the manual focus function. false : The device does not support the manual focus function.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_iscameratorchsupported",
    "name": "IsCameraTorchSupported",
    "description": "Checks whether the device supports camera flash.\n\nThis method must be called after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n This method is for Android and iOS only.\n The app enables the front camera by default. If your front camera does not support flash, this method returns false. If you want to check whether the rear camera supports the flash function, call SwitchCamera before this method.\n On iPads with system version 15, even if IsCameraTorchSupported returns true, you might fail to successfully enable the flash by calling SetCameraTorchOn due to system issues.",
    "parameters": [],
    "returns": "true : The device supports camera flash. false : The device does not support camera flash.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_iscamerazoomsupported",
    "name": "IsCameraZoomSupported",
    "description": "Checks whether the device supports camera zoom.\n\nThis method is for Android and iOS only.",
    "parameters": [],
    "returns": "true : The device supports camera zoom. false : The device does not support camera zoom.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_isfeatureavailableondevice",
    "name": "IsFeatureAvailableOnDevice",
    "description": "Checks whether the device supports the specified advanced feature.\n\nChecks whether the capabilities of the current device meet the requirements for advanced features such as virtual background and image enhancement.",
    "parameters": [
      {
        "type": "The type of the advanced feature, see FeatureType."
      }
    ],
    "returns": "true : The current device supports the specified feature. false : The current device does not support the specified feature.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_isspeakerphoneenabled",
    "name": "IsSpeakerphoneEnabled",
    "description": "Checks whether the speakerphone is enabled.\n\nThis method is for Android and iOS only.",
    "parameters": [],
    "returns": "true : The speakerphone is enabled, and the audio plays from the speakerphone. false : The speakerphone is not enabled, and the audio plays from devices other than the speakerphone. For example, the headset or earpiece.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_joinchannel",
    "name": "JoinChannel [1/2]",
    "description": "Joins a channel.\n\nBy default, the user subscribes to the audio and video streams of all the other users in the channel, giving rise to usage and billings. To stop subscribing to a specified stream or all remote streams, call the corresponding mute methods.",
    "parameters": [
      {
        "channelId": "The channel name. This parameter signifies the channel in which users engage in real-time audio and video interaction. Under the premise of the same App ID, users who fill in the same channel ID enter the same channel for audio and video interaction. The string length must be less than 64 bytes. Supported characters (89 characters in total):\n All lowercase English letters: a to z.\n All uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n \"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"<\", \"=\", \".\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\""
      },
      {
        "token": "The token generated on your server for authentication.\n (Recommended) If your project has enabled the security mode (using APP ID and Token for authentication), this parameter is required.\n If you have only enabled the testing mode (using APP ID for authentication), this parameter is optional. You will automatically exit the channel 24 hours after successfully joining in.\n If you need to join different channels at the same time or switch between channels, Agora recommends using a wildcard token so that you don't need to apply for a new token every time joining a channel."
      },
      {
        "info": "(Optional) Reserved for future use."
      },
      {
        "uid": "The user ID. This parameter is used to identify the user in the channel for real-time audio and video interaction. You need to set and manage user IDs yourself, and ensure that each user ID in the same channel is unique. This parameter is a 32-bit unsigned integer. The value range is 1 to 2 32 -1. If the user ID is not assigned (or set to 0), the SDK assigns a random user ID and OnJoinChannelSuccess returns it in the callback. Your application must record and maintain the returned user ID, because the SDK does not do so."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid. For example, the token is invalid, the uid parameter is not set to an integer, or the value of a member in ChannelMediaOptions is invalid. You need to pass in a valid parameter and join the channel again.\n -3: Fails to initialize the IRtcEngine object. You need to reinitialize the IRtcEngine object.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.\n -8: The internal state of the IRtcEngine object is wrong. The typical cause is that after calling StartEchoTest to start a call loop test, you call this method to join the channel without calling StopEchoTest to stop the test. You need to call StopEchoTest before calling this method.\n -17: The request to join the channel is rejected. The typical cause is that the user is already in the channel. Agora recommends that you use the OnConnectionStateChanged callback to see whether the user is in the channel. Do not call this method to join the channel unless you receive the CONNECTION_STATE_DISCONNECTED (1) state.\n -102: The channel name is invalid. You need to pass in a valid channel name in channelId to rejoin the channel.\n -121: The user ID is invalid. You need to pass in a valid user ID in uid to rejoin the channel.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_joinchannel2",
    "name": "JoinChannel [2/2]",
    "description": "Joins a channel with media options.\n\nCompared to JoinChannel [1/2], this method has the options parameter which is used to set media options, such as whether to publish audio and video streams within a channel, or whether to automatically subscribe to the audio and video streams of all remote users when joining a channel. By default, the user subscribes to the audio and video streams of all the other users in the channel, giving rise to usage and billings. To stop subscribing to other streams, set the options parameter or call the corresponding mute methods.",
    "parameters": [
      {
        "token": "The token generated on your server for authentication.\n (Recommended) If your project has enabled the security mode (using APP ID and Token for authentication), this parameter is required.\n If you have only enabled the testing mode (using APP ID for authentication), this parameter is optional. You will automatically exit the channel 24 hours after successfully joining in.\n If you need to join different channels at the same time or switch between channels, Agora recommends using a wildcard token so that you don't need to apply for a new token every time joining a channel."
      },
      {
        "channelId": "The channel name. This parameter signifies the channel in which users engage in real-time audio and video interaction. Under the premise of the same App ID, users who fill in the same channel ID enter the same channel for audio and video interaction. The string length must be less than 64 bytes. Supported characters (89 characters in total):\n All lowercase English letters: a to z.\n All uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n \"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"<\", \"=\", \".\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\""
      },
      {
        "uid": "The user ID. This parameter is used to identify the user in the channel for real-time audio and video interaction. You need to set and manage user IDs yourself, and ensure that each user ID in the same channel is unique. This parameter is a 32-bit unsigned integer. The value range is 1 to 2 32 -1. If the user ID is not assigned (or set to 0), the SDK assigns a random user ID and OnJoinChannelSuccess returns it in the callback. Your application must record and maintain the returned user ID, because the SDK does not do so."
      },
      {
        "options": "The channel media options. See ChannelMediaOptions."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid. For example, the token is invalid, the uid parameter is not set to an integer, or the value of a member in ChannelMediaOptions is invalid. You need to pass in a valid parameter and join the channel again.\n -3: Fails to initialize the IRtcEngine object. You need to reinitialize the IRtcEngine object.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.\n -8: The internal state of the IRtcEngine object is wrong. The typical cause is that after calling StartEchoTest to start a call loop test, you call this method to join the channel without calling StopEchoTest to stop the test. You need to call StopEchoTest before calling this method.\n -17: The request to join the channel is rejected. The typical cause is that the user is already in the channel. Agora recommends that you use the OnConnectionStateChanged callback to see whether the user is in the channel. Do not call this method to join the channel unless you receive the CONNECTION_STATE_DISCONNECTED (1) state.\n -102: The channel name is invalid. You need to pass in a valid channel name in channelId to rejoin the channel.\n -121: The user ID is invalid. You need to pass in a valid user ID in uid to rejoin the channel.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_joinchannelwithuseraccount",
    "name": "JoinChannelWithUserAccount [1/2]",
    "description": "Joins a channel with a User Account and Token.\n\nTo ensure smooth communication, use the same parameter type to identify the user. For example, if a user joins the channel with a UID, then ensure all the other users use the UID too. The same applies to the user account. If a user joins the channel with the Agora Web SDK, ensure that the ID of the user is set to the same parameter type. Before calling this method, if you have not called RegisterLocalUserAccount to register a user account, when you call this method to join a channel, the SDK automatically creates a user account for you. Calling the RegisterLocalUserAccount method to register a user account, and then calling this method to join a channel can shorten the time it takes to enter the channel. Once a user joins the channel, the user subscribes to the audio and video streams of all the other users in the channel by default, giving rise to usage and billings. To stop subscribing to a specified stream or all remote streams, call the corresponding mute methods.",
    "parameters": [
      {
        "userAccount": "The user account. This parameter is used to identify the user in the channel for real-time audio and video engagement. You need to set and manage user accounts yourself and ensure that each user account in the same channel is unique. The maximum length of this parameter is 255 bytes. Ensure that you set this parameter and do not set it as NULL. Supported characters are as follows(89 in total):\n The 26 lowercase English letters: a to z.\n The 26 uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n Space\n \"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"<\", \"=\", \".\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\""
      },
      {
        "token": "The token generated on your server for authentication.\n (Recommended) If your project has enabled the security mode (using APP ID and Token for authentication), this parameter is required.\n If you have only enabled the testing mode (using APP ID for authentication), this parameter is optional. You will automatically exit the channel 24 hours after successfully joining in.\n If you need to join different channels at the same time or switch between channels, Agora recommends using a wildcard token so that you don't need to apply for a new token every time joining a channel."
      },
      {
        "channelId": "The channel name. This parameter signifies the channel in which users engage in real-time audio and video interaction. Under the premise of the same App ID, users who fill in the same channel ID enter the same channel for audio and video interaction. The string length must be less than 64 bytes. Supported characters (89 characters in total):\n All lowercase English letters: a to z.\n All uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n \"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"<\", \"=\", \".\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\""
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid. For example, the token is invalid, the uid parameter is not set to an integer, or the value of a member in ChannelMediaOptions is invalid. You need to pass in a valid parameter and join the channel again.\n -3: Fails to initialize the IRtcEngine object. You need to reinitialize the IRtcEngine object.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.\n -8: The internal state of the IRtcEngine object is wrong. The typical cause is that after calling StartEchoTest to start a call loop test, you call this method to join the channel without calling StopEchoTest to stop the test. You need to call StopEchoTest before calling this method.\n -17: The request to join the channel is rejected. The typical cause is that the user is already in the channel. Agora recommends that you use the OnConnectionStateChanged callback to see whether the user is in the channel. Do not call this method to join the channel unless you receive the CONNECTION_STATE_DISCONNECTED (1) state.\n -102: The channel name is invalid. You need to pass in a valid channel name in channelId to rejoin the channel.\n -121: The user ID is invalid. You need to pass in a valid user ID in uid to rejoin the channel.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_joinchannelwithuseraccount2",
    "name": "JoinChannelWithUserAccount [2/2]",
    "description": "Join a channel using a user account and token, and set the media options.\n\nBefore calling this method, if you have not called RegisterLocalUserAccount to register a user account, when you call this method to join a channel, the SDK automatically creates a user account for you. Calling the RegisterLocalUserAccount method to register a user account, and then calling this method to join a channel can shorten the time it takes to enter the channel. Compared to JoinChannelWithUserAccount [1/2], this method has the options parameter which is used to set media options, such as whether to publish audio and video streams within a channel. By default, the user subscribes to the audio and video streams of all the other users in the channel, giving rise to usage and billings. To stop subscribing to other streams, set the options parameter or call the corresponding mute methods. To ensure smooth communication, use the same parameter type to identify the user. For example, if a user joins the channel with a UID, then ensure all the other users use the UID too. The same applies to the user account. If a user joins the channel with the Agora Web SDK, ensure that the ID of the user is set to the same parameter type.",
    "parameters": [
      {
        "options": "The channel media options. See ChannelMediaOptions."
      },
      {
        "token": "The token generated on your server for authentication.\n (Recommended) If your project has enabled the security mode (using APP ID and Token for authentication), this parameter is required.\n If you have only enabled the testing mode (using APP ID for authentication), this parameter is optional. You will automatically exit the channel 24 hours after successfully joining in.\n If you need to join different channels at the same time or switch between channels, Agora recommends using a wildcard token so that you don't need to apply for a new token every time joining a channel."
      },
      {
        "channelId": "The channel name. This parameter signifies the channel in which users engage in real-time audio and video interaction. Under the premise of the same App ID, users who fill in the same channel ID enter the same channel for audio and video interaction. The string length must be less than 64 bytes. Supported characters (89 characters in total):\n All lowercase English letters: a to z.\n All uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n \"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"<\", \"=\", \".\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\""
      },
      {
        "userAccount": "The user account. This parameter is used to identify the user in the channel for real-time audio and video engagement. You need to set and manage user accounts yourself and ensure that each user account in the same channel is unique. The maximum length of this parameter is 255 bytes. Ensure that you set this parameter and do not set it as NULL. Supported characters are as follows(89 in total):\n The 26 lowercase English letters: a to z.\n The 26 uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n Space\n \"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"<\", \"=\", \".\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\""
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid. For example, the token is invalid, the uid parameter is not set to an integer, or the value of a member in ChannelMediaOptions is invalid. You need to pass in a valid parameter and join the channel again.\n -3: Fails to initialize the IRtcEngine object. You need to reinitialize the IRtcEngine object.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.\n -8: The internal state of the IRtcEngine object is wrong. The typical cause is that after calling StartEchoTest to start a call loop test, you call this method to join the channel without calling StopEchoTest to stop the test. You need to call StopEchoTest before calling this method.\n -17: The request to join the channel is rejected. The typical cause is that the user is already in the channel. Agora recommends that you use the OnConnectionStateChanged callback to see whether the user is in the channel. Do not call this method to join the channel unless you receive the CONNECTION_STATE_DISCONNECTED (1) state.\n -102: The channel name is invalid. You need to pass in a valid channel name in channelId to rejoin the channel.\n -121: The user ID is invalid. You need to pass in a valid user ID in uid to rejoin the channel.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_leavechannel",
    "name": "LeaveChannel [1/2]",
    "description": "Leaves a channel.\n\nAfter calling this method, the SDK terminates the audio and video interaction, leaves the current channel, and releases all resources related to the session. After joining the channel, you must call this method to end the call; otherwise, the next call cannot be started.\n This method call is asynchronous. When this method returns, it does not necessarily mean that the user has left the channel.\n If you have called JoinChannelEx to join multiple channels, calling this method will leave all the channels you joined.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).\n -2: The parameter is invalid.\n -7: The SDK is not initialized.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_leavechannel2",
    "name": "LeaveChannel [2/2]",
    "description": "Sets channel options and leaves the channel.\n\nAfter calling this method, the SDK terminates the audio and video interaction, leaves the current channel, and releases all resources related to the session. After joining a channel, you must call this method or LeaveChannel [1/2] to end the call, otherwise, the next call cannot be started. If you have called JoinChannelEx to join multiple channels, calling this method will leave all the channels you joined. This method call is asynchronous. When this method returns, it does not necessarily mean that the user has left the channel.",
    "parameters": [
      {
        "options": "The options for leaving the channel. See LeaveChannelOptions."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_loadextensionprovider",
    "name": "LoadExtensionProvider",
    "description": "Loads an extension.\n\nThis method is used to add extensions external to the SDK (such as those from Extensions Marketplace and SDK extensions) to the SDK.",
    "parameters": [
      {
        "path": "The extension library path and name. For example: /library/libagora_segmentation_extension.dll."
      },
      {
        "unload_after_use": "Whether to uninstall the current extension when you no longer using it: true : Uninstall the extension when the IRtcEngine is destroyed. false : (Rcommended) Do not uninstall the extension until the process terminates."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_muteallremoteaudiostreams",
    "name": "MuteAllRemoteAudioStreams",
    "description": "Stops or resumes subscribing to the audio streams of all remote users.\n\nAfter successfully calling this method, the local user stops or resumes subscribing to the audio streams of all remote users, including all subsequent users. By default, the SDK subscribes to the audio streams of all remote users when joining a channel. To modify this behavior, you can set autoSubscribeAudio to false when calling JoinChannel [2/2] to join the channel, which will cancel the subscription to the audio streams of all users upon joining the channel.",
    "parameters": [
      {
        "mute": "Whether to stop subscribing to the audio streams of all remote users: true : Stops subscribing to the audio streams of all remote users. false : (Default) Subscribes to the audio streams of all remote users by default."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_muteallremotevideostreams",
    "name": "MuteAllRemoteVideoStreams",
    "description": "Stops or resumes subscribing to the video streams of all remote users.\n\nAfter successfully calling this method, the local user stops or resumes subscribing to the video streams of all remote users, including all subsequent users. By default, the SDK subscribes to the video streams of all remote users when joining a channel. To modify this behavior, you can set autoSubscribeVideo to false when calling JoinChannel [2/2] to join the channel, which will cancel the subscription to the video streams of all users upon joining the channel.",
    "parameters": [
      {
        "mute": "Whether to stop subscribing to the video streams of all remote users. true : Stop subscribing to the video streams of all remote users. false : (Default) Subscribe to the video streams of all remote users by default."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_mutelocalaudiostream",
    "name": "MuteLocalAudioStream",
    "description": "Stops or resumes publishing the local audio stream.\n\nThis method is used to control whether to publish the locally captured audio stream. If you call this method to stop publishing locally captured audio streams, the audio capturing device will still work and won't be affected.",
    "parameters": [
      {
        "mute": "Whether to stop publishing the local audio stream: true : Stops publishing the local audio stream. false : (Default) Resumes publishing the local audio stream."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_mutelocalvideostream",
    "name": "MuteLocalVideoStream",
    "description": "Stops or resumes publishing the local video stream.\n\nThis method is used to control whether to publish the locally captured video stream. If you call this method to stop publishing locally captured video streams, the video capturing device will still work and won't be affected. Compared to EnableLocalVideo (false), which can also cancel the publishing of local video stream by turning off the local video stream capture, this method responds faster.",
    "parameters": [
      {
        "mute": "Whether to stop publishing the local video stream. true : Stop publishing the local video stream. false : (Default) Publish the local video stream."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_muterecordingsignal",
    "name": "MuteRecordingSignal",
    "description": "Whether to mute the recording signal.\n\nIf you have already called AdjustRecordingSignalVolume to adjust the recording signal volume, when you call this method and set it to true, the SDK behaves as follows:\n Records the adjusted volume.\n Mutes the recording signal. When you call this method again and set it to false, the recording signal volume will be restored to the volume recorded by the SDK before muting.",
    "parameters": [
      {
        "mute": "true : Mute the recording signal. false : (Default) Do not mute the recording signal."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_muteremoteaudiostream",
    "name": "MuteRemoteAudioStream",
    "description": "Stops or resumes subscribing to the audio stream of a specified user.",
    "parameters": [
      {
        "uid": "The user ID of the specified user."
      },
      {
        "mute": "Whether to subscribe to the specified remote user's audio stream. true : Stop subscribing to the audio stream of the specified user. false : (Default) Subscribe to the audio stream of the specified user."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_muteremotevideostream",
    "name": "MuteRemoteVideoStream",
    "description": "Stops or resumes subscribing to the video stream of a specified user.",
    "parameters": [
      {
        "uid": "The user ID of the specified user."
      },
      {
        "mute": "Whether to subscribe to the specified remote user's video stream. true : Stop subscribing to the video streams of the specified user. false : (Default) Subscribe to the video stream of the specified user."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_pauseallchannelmediarelay",
    "name": "PauseAllChannelMediaRelay",
    "description": "Pauses the media stream relay to all target channels.\n\nAfter the cross-channel media stream relay starts, you can call this method to pause relaying media streams to all target channels; after the pause, if you want to resume the relay, call ResumeAllChannelMediaRelay. Call this method after StartOrUpdateChannelMediaRelay.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.\n -5: The method call was rejected. There is no ongoing channel media relay.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_pausealleffects",
    "name": "PauseAllEffects",
    "description": "Pauses all audio effects.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_pauseaudiomixing",
    "name": "PauseAudioMixing",
    "description": "Pauses playing and mixing the music file.\n\nAfter calling StartAudioMixing [2/2] to play a music file, you can call this method to pause the playing. If you need to stop the playback, call StopAudioMixing.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_pauseeffect",
    "name": "PauseEffect",
    "description": "Pauses a specified audio effect file.",
    "parameters": [
      {
        "soundId": "The audio effect ID. The ID of each audio effect file is unique."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_playalleffects",
    "name": "PlayAllEffects",
    "description": "Plays all audio effect files.\n\nAfter calling PreloadEffect multiple times to preload multiple audio effects into the memory, you can call this method to play all the specified audio effects for all users in the channel.",
    "parameters": [
      {
        "loopCount": "The number of times the audio effect loops:\n -1: Play the audio effect files in an indefinite loop until you call StopEffect or StopAllEffects.\n 0: Play the audio effect once.\n 1: Play the audio effect twice."
      },
      {
        "pitch": "The pitch of the audio effect. The value ranges between 0.5 and 2.0. The default value is 1.0 (original pitch). The lower the value, the lower the pitch."
      },
      {
        "pan": "The spatial position of the audio effect. The value ranges between -1.0 and 1.0:\n -1.0: The audio effect shows on the left.\n 0: The audio effect shows ahead.\n 1.0: The audio effect shows on the right."
      },
      {
        "gain": "The volume of the audio effect. The value range is [0, 100]. The default value is 100 (original volume). The smaller the value, the lower the volume."
      },
      {
        "publish": "Whether to publish the audio effect to the remote users: true : Publish the audio effect to the remote users. Both the local user and remote users can hear the audio effect. false : (Default) Do not publish the audio effect to the remote users. Only the local user can hear the audio effect."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_playeffect2",
    "name": "",
    "description": "Plays the specified local or online audio effect file.\n\nTo play multiple audio effect files at the same time, call this method multiple times with different soundId and filePath. To achieve the optimal user experience, Agora recommends that you do not playing more than three audio files at the same time.",
    "parameters": [
      {
        "filePath": "The file path. The SDK supports URLs and absolute path of local files. The absolute path needs to be accurate to the file name and extension. Supported audio formats include MP3, AAC, M4A, MP4, WAV, and 3GP. If you have preloaded an audio effect into memory by calling PreloadEffect, ensure that the value of this parameter is the same as that of filePath in PreloadEffect."
      },
      {
        "soundId": "The audio effect ID. The ID of each audio effect file is unique. If you have preloaded an audio effect into memory by calling PreloadEffect, ensure that the value of this parameter is the same as that of soundId in PreloadEffect."
      },
      {
        "loopCount": "The number of times the audio effect loops.\n ≥ 0: The number of playback times. For example, 1 means looping one time, which means playing the audio effect two times in total.\n -1: Play the audio file in an infinite loop."
      },
      {
        "pitch": "The pitch of the audio effect. The value range is 0.5 to 2.0. The default value is 1.0, which means the original pitch. The lower the value, the lower the pitch."
      },
      {
        "pan": "The spatial position of the audio effect. The value ranges between -1.0 and 1.0:\n -1.0: The audio effect is heard on the left of the user.\n 0.0: The audio effect is heard in front of the user.\n 1.0: The audio effect is heard on the right of the user."
      },
      {
        "gain": "The volume of the audio effect. The value range is 0.0 to 100.0. The default value is 100.0, which means the original volume. The smaller the value, the lower the volume."
      },
      {
        "publish": "Whether to publish the audio effect to the remote users. true : Publish the audio effect to the remote users. Both the local user and remote users can hear the audio effect. false : Do not publish the audio effect to the remote users. Only the local user can hear the audio effect."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_irtcengine_playeffect3",
    "name": "PlayEffect",
    "description": "Plays the specified local or online audio effect file.\n\nTo play multiple audio effect files at the same time, call this method multiple times with different soundId and filePath. To achieve the optimal user experience, Agora recommends that you do not playing more than three audio files at the same time.",
    "parameters": [
      {
        "soundId": "The audio effect ID. The ID of each audio effect file is unique. If you have preloaded an audio effect into memory by calling PreloadEffect, ensure that the value of this parameter is the same as that of soundId in PreloadEffect."
      },
      {
        "filePath": "The file path. The SDK supports URLs and absolute path of local files. The absolute path needs to be accurate to the file name and extension. Supported audio formats include MP3, AAC, M4A, MP4, WAV, and 3GP. If you have preloaded an audio effect into memory by calling PreloadEffect, ensure that the value of this parameter is the same as that of filePath in PreloadEffect."
      },
      {
        "loopCount": "The number of times the audio effect loops.\n ≥ 0: The number of playback times. For example, 1 means looping one time, which means playing the audio effect two times in total.\n -1: Play the audio file in an infinite loop."
      },
      {
        "pitch": "The pitch of the audio effect. The value range is 0.5 to 2.0. The default value is 1.0, which means the original pitch. The lower the value, the lower the pitch."
      },
      {
        "pan": "The spatial position of the audio effect. The value ranges between -1.0 and 1.0:\n -1.0: The audio effect is heard on the left of the user.\n 0.0: The audio effect is heard in front of the user.\n 1.0: The audio effect is heard on the right of the user."
      },
      {
        "gain": "The volume of the audio effect. The value range is 0.0 to 100.0. The default value is 100.0, which means the original volume. The smaller the value, the lower the volume."
      },
      {
        "publish": "Whether to publish the audio effect to the remote users: true : Publish the audio effect to the remote users. Both the local user and remote users can hear the audio effect. false : Do not publish the audio effect to the remote users. Only the local user can hear the audio effect."
      },
      {
        "startPos": "The playback position (ms) of the audio effect file."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_preloadchannel",
    "name": "PreloadChannel",
    "description": "Preloads a channel with token, channelId, and uid.\n\nWhen audience members need to switch between different channels frequently, calling the method can help shortening the time of joining a channel, thus reducing the time it takes for audience members to hear and see the host. If you join a preloaded channel, leave it and want to rejoin the same channel, you do not need to call this method unless the token for preloading the channel expires. Failing to preload a channel does not mean that you can't join a channel, nor will it increase the time of joining a channel.",
    "parameters": [
      {
        "uid": "The user ID. This parameter is used to identify the user in the channel for real-time audio and video interaction. You need to set and manage user IDs yourself, and ensure that each user ID in the same channel is unique. This parameter is a 32-bit unsigned integer. The value range is 1 to 2 32 -1. If the user ID is not assigned (or set to 0), the SDK assigns a random user ID and OnJoinChannelSuccess returns it in the callback. Your application must record and maintain the returned user ID, because the SDK does not do so."
      },
      {
        "token": "The token generated on your server for authentication. When the token for preloading channels expires, you can update the token based on the number of channels you preload.\n When preloading one channel, calling this method to pass in the new token.\n When preloading more than one channels:\n If you use a wildcard token for all preloaded channels, call UpdatePreloadChannelToken to update the token. When generating a wildcard token, ensure the user ID is not set as 0.\n If you use different tokens to preload different channels, call this method to pass in your user ID, channel name and the new token."
      },
      {
        "channelId": "The channel name that you want to preload. This parameter signifies the channel in which users engage in real-time audio and video interaction. Under the premise of the same App ID, users who fill in the same channel ID enter the same channel for audio and video interaction. The string length must be less than 64 bytes. Supported characters (89 characters in total):\n All lowercase English letters: a to z.\n All uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n \"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"<\", \"=\", \".\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\""
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.\n -102: The channel name is invalid. You need to pass in a valid channel name and join the channel again.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_preloadchannelwithuseraccount",
    "name": "PreloadChannelWithUserAccount",
    "description": "Preloads a channel with token, channelId, and userAccount.\n\nWhen audience members need to switch between different channels frequently, calling the method can help shortening the time of joining a channel, thus reducing the time it takes for audience members to hear and see the host. If you join a preloaded channel, leave it and want to rejoin the same channel, you do not need to call this method unless the token for preloading the channel expires. Failing to preload a channel does not mean that you can't join a channel, nor will it increase the time of joining a channel.",
    "parameters": [
      {
        "userAccount": "The user account. This parameter is used to identify the user in the channel for real-time audio and video engagement. You need to set and manage user accounts yourself and ensure that each user account in the same channel is unique. The maximum length of this parameter is 255 bytes. Ensure that you set this parameter and do not set it as NULL. Supported characters are as follows(89 in total):\n The 26 lowercase English letters: a to z.\n The 26 uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n Space\n \"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"<\", \"=\", \".\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\""
      },
      {
        "channelId": "The channel name that you want to preload. This parameter signifies the channel in which users engage in real-time audio and video interaction. Under the premise of the same App ID, users who fill in the same channel ID enter the same channel for audio and video interaction. The string length must be less than 64 bytes. Supported characters (89 characters in total):\n All lowercase English letters: a to z.\n All uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n \"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"<\", \"=\", \".\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\""
      },
      {
        "token": "The token generated on your server for authentication. When the token for preloading channels expires, you can update the token based on the number of channels you preload.\n When preloading one channel, calling this method to pass in the new token.\n When preloading more than one channels:\n If you use a wildcard token for all preloaded channels, call UpdatePreloadChannelToken to update the token. When generating a wildcard token, ensure the user ID is not set as 0.\n If you use different tokens to preload different channels, call this method to pass in your user ID, channel name and the new token."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid. For example, the User Account is empty. You need to pass in a valid parameter and join the channel again.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.\n -102: The channel name is invalid. You need to pass in a valid channel name and join the channel again.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_preloadeffect",
    "name": "PreloadEffect",
    "description": "Preloads a specified audio effect file into the memory.\n\nEnsure the size of all preloaded files does not exceed the limit. For the audio file formats supported by this method, see What formats of audio files does the Agora RTC SDK support.",
    "parameters": [
      {
        "soundId": "The audio effect ID. The ID of each audio effect file is unique."
      },
      {
        "filePath": "File path:\n Android: The file path, which needs to be accurate to the file name and suffix. Agora supports URL addresses, absolute paths, or file paths that start with /assets/. You might encounter permission issues if you use an absolute path to access a local file, so Agora recommends using a URI address instead. For example : content://com.android.providers.media.documents/document/audio%3A14441\n Windows: The absolute path or URL address (including the suffixes of the filename) of the audio effect file. For example : C:\\music\\audio.mp4.\n iOS or macOS: The absolute path or URL address (including the suffixes of the filename) of the audio effect file. For example: /var/mobile/Containers/Data/audio.mp4."
      },
      {
        "startPos": "The playback position (ms) of the audio effect file."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_querycamerafocallengthcapability",
    "name": "QueryCameraFocalLengthCapability",
    "description": "Queries the focal length capability supported by the camera.\n\nIf you want to enable the wide-angle or ultra-wide-angle mode for camera capture, it is recommended to start by calling this method to check whether the device supports the required focal length capability. Then, adjust the camera's focal length configuration based on the query result by calling SetCameraCapturerConfiguration, ensuring the best camera capture performance. This method is for Android and iOS only.",
    "parameters": [
      {
        "focalLengthInfos": "An output parameter. After the method is executed, output an array of FocalLengthInfo objects containing camera focal length information."
      },
      {
        "size": "An output parameter. After the method is executed, output the number of focal length information items retrieved."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_querycodeccapability",
    "name": "QueryCodecCapability",
    "description": "Queries the video codec capabilities of the SDK.",
    "parameters": [
      {
        "codecInfo": "Input and output parameter. An array representing the video codec capabilities of the SDK. See CodecCapInfo.\n Input value: One CodecCapInfo defined by the user when executing this method, representing the video codec capability to be queried.\n Output value: The CodecCapInfo after the method is executed, representing the actual video codec capabilities of the SDK."
      },
      {
        "size": "Input and output parameter, represent the size of the CodecCapInfo array.\n Input value: Size of the CodecCapInfo defined by the user when executing the method.\n Output value: Size of the output CodecCapInfo after this method is executed."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_querydevicescore",
    "name": "QueryDeviceScore",
    "description": "Queries device score.",
    "parameters": [],
    "returns": ">0: The method call succeeeds, the value is the current device's score, the range is [0,100], the larger the value, the stronger the device capability. Most devices are rated between 60 and 100.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_queryscreencapturecapability",
    "name": "QueryScreenCaptureCapability",
    "description": "Queries the highest frame rate supported by the device during screen sharing.",
    "parameters": [],
    "returns": "The highest frame rate supported by the device, if the method is called successfully. See SCREEN_CAPTURE_FRAMERATE_CAPABILITY.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_rate",
    "name": "Rate",
    "description": "Allows a user to rate a call after the call ends.\n\nEnsure that you call this method after leaving a channel.",
    "parameters": [
      {
        "callId": "The current call ID. You can get the call ID by calling GetCallId."
      },
      {
        "rating": "The value is between 1 (the lowest score) and 5 (the highest score)."
      },
      {
        "description": "(Optional) A description of the call. The string length should be less than 800 bytes."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).\n -2: The parameter is invalid.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_registeraudioencodedframeobserver",
    "name": "RegisterAudioEncodedFrameObserver",
    "description": "Registers an encoded audio observer.\n\nCall this method after joining a channel.\n You can call this method or StartAudioRecording [3/3] to set the recording type and quality of audio files, but Agora does not recommend using this method and StartAudioRecording [3/3] at the same time. Only the method called later will take effect.",
    "parameters": [
      {
        "config": "Observer settings for the encoded audio. See AudioEncodedFrameObserverConfig."
      },
      {
        "observer": "The encoded audio observer. See IAudioEncodedFrameObserver."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_registeraudiospectrumobserver",
    "name": "RegisterAudioSpectrumObserver",
    "description": "Register an audio spectrum observer.\n\nAfter successfully registering the audio spectrum observer and calling EnableAudioSpectrumMonitor to enable the audio spectrum monitoring, the SDK reports the callback that you implement in the IAudioSpectrumObserver class according to the time interval you set. You can call this method either before or after joining a channel.",
    "parameters": [
      {
        "observer": "The audio spectrum observer. See IAudioSpectrumObserver."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_registerextension",
    "name": "RegisterExtension",
    "description": "Registers an extension.\n\nFor extensions external to the SDK (such as those from Extensions Marketplace and SDK Extensions), you need to load them before calling this method. Extensions internal to the SDK (those included in the full SDK package) are automatically loaded and registered after the initialization of IRtcEngine.",
    "parameters": [
      {
        "type": "Source type of the extension. See MEDIA_SOURCE_TYPE."
      },
      {
        "extension": "The name of the extension."
      },
      {
        "provider": "The name of the extension provider."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -3: The extension library is not loaded. Agora recommends that you check the storage location or the name of the dynamic library.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_registerlocaluseraccount",
    "name": "RegisterLocalUserAccount",
    "description": "Registers a user account.\n\nOnce registered, the user account can be used to identify the local user when the user joins the channel. After the registration is successful, the user account can identify the identity of the local user, and the user can use it to join the channel. This method is optional. If you want to join a channel using a user account, you can choose one of the following methods:\n Call the RegisterLocalUserAccount method to register a user account, and then call the JoinChannelWithUserAccount [1/2] or JoinChannelWithUserAccount [2/2] method to join a channel, which can shorten the time it takes to enter the channel.\n Call the JoinChannelWithUserAccount [1/2] or JoinChannelWithUserAccount [2/2] method to join a channel.\n Ensure that the userAccount is unique in the channel.\n To ensure smooth communication, use the same parameter type to identify the user. For example, if a user joins the channel with a UID, then ensure all the other users use the UID too. The same applies to the user account. If a user joins the channel with the Agora Web SDK, ensure that the ID of the user is set to the same parameter type.",
    "parameters": [
      {
        "appId": "The App ID of your project on Agora Console."
      },
      {
        "userAccount": "The user account. This parameter is used to identify the user in the channel for real-time audio and video engagement. You need to set and manage user accounts yourself and ensure that each user account in the same channel is unique. The maximum length of this parameter is 255 bytes. Ensure that you set this parameter and do not set it as NULL. Supported characters are as follow(89 in total):\n The 26 lowercase English letters: a to z.\n The 26 uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n Space\n \"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"<\", \"=\", \".\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\""
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_registermediametadataobserver",
    "name": "RegisterMediaMetadataObserver",
    "description": "Registers the metadata observer.\n\nYou need to implement the IMetadataObserver class and specify the metadata type in this method. This method enables you to add synchronized metadata in the video stream for more diversified\n live interactive streaming, such as sending shopping links, digital coupons, and online quizzes. Call this method before JoinChannel [2/2].",
    "parameters": [
      {
        "observer": "The metadata observer. See IMetadataObserver."
      },
      {
        "type": "The metadata type. The SDK currently only supports VIDEO_METADATA. See METADATA_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_release",
    "name": "Dispose",
    "description": "Releases the IRtcEngine instance.\n\nThis method releases all resources used by the Agora SDK. Use this method for apps in which users occasionally make voice or video calls. When users do not make calls, you can free up resources for other operations. After a successful method call, you can no longer use any method or callback in the SDK anymore. If you want to use the real-time communication functions again, you must call CreateAgoraRtcEngine and Initialize to create a new IRtcEngine instance.\n This method can be called synchronously. You need to wait for the resource of IRtcEngine to be released before performing other operations (for example, create a new IRtcEngine object). Therefore, Agora recommends calling this method in the child thread to avoid blocking the main thread.\n Besides, Agora does not recommend you calling Dispose in any callback of the SDK. Otherwise, the SDK cannot release the resources until the callbacks return results, which may result in a deadlock.",
    "parameters": [
      {
        "sync": "Whether the method is called synchronously: true : Synchronous call. false : Asynchronous call. Currently this method only supports synchronous calls. Do not set this parameter to this value."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_removeinjectstreamurl",
    "name": "RemoveInjectStreamUrl",
    "description": "Removes the voice or video stream URL address from the live streaming.\n\nAfter a successful method, the SDK triggers the OnUserOffline callback with the uid of 666.",
    "parameters": [
      {
        "url": "The URL address of the injected stream to be removed."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_irtcengine_renewtoken",
    "name": "RenewToken",
    "description": "Renews the token.\n\nYou can call this method to pass a new token to the SDK. A token will expire after a certain period of time, at which point the SDK will be unable to establish a connection with the server.",
    "parameters": [
      {
        "token": "The new token."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid. For example, the token is empty.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.\n 110: Invalid token. Ensure the following:\n The user ID specified when generating the token is consistent with the user ID used when joining the channel.\n The generated token is the same as the token passed in to join the channel.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_resumeallchannelmediarelay",
    "name": "ResumeAllChannelMediaRelay",
    "description": "Resumes the media stream relay to all target channels.\n\nAfter calling the PauseAllChannelMediaRelay method, you can call this method to resume relaying media streams to all destination channels. Call this method after PauseAllChannelMediaRelay.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.\n -5: The method call was rejected. There is no paused channel media relay.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_resumealleffects",
    "name": "ResumeAllEffects",
    "description": "Resumes playing all audio effect files.\n\nAfter you call PauseAllEffects to pause the playback, you can call this method to resume the playback.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_resumeaudiomixing",
    "name": "ResumeAudioMixing",
    "description": "Resumes playing and mixing the music file.\n\nAfter calling PauseAudioMixing to pause the playback, you can call this method to resume the playback.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_resumeeffect",
    "name": "ResumeEffect",
    "description": "Resumes playing a specified audio effect.",
    "parameters": [
      {
        "soundId": "The audio effect ID. The ID of each audio effect file is unique."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_selectaudiotrack",
    "name": "SelectAudioTrack [1/2]",
    "description": "Selects the audio track used during playback.\n\nAfter getting the track index of the audio file, you can call this method to specify any track to play. For example, if different tracks of a multi-track file store songs in different languages, you can call this method to set the playback language.\n For the supported formats of audio files, see.\n You need to call this method after calling StartAudioMixing [2/2] and receiving the OnAudioMixingStateChanged (AUDIO_MIXING_STATE_PLAYING) callback.",
    "parameters": [
      {
        "index": "The audio track you want to specify. The value should be greater than 0 and less than that of returned by GetAudioTrackCount."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_sendcustomreportmessage",
    "name": "SendCustomReportMessage",
    "description": "Reports customized messages.\n\nAgora supports reporting and analyzing customized messages. This function is in the beta stage with a free trial. The ability provided in its beta test version is reporting a maximum of 10 message pieces within 6 seconds, with each message piece not exceeding 256 bytes and each string not exceeding 100 bytes. To try out this function, contact and discuss the format of customized messages with us.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_sendmetadata",
    "name": "SendMetadata",
    "description": "Sends media metadata.\n\nIf the metadata is sent successfully, the SDK triggers the OnMetadataReceived callback on the receiver.",
    "parameters": [
      {
        "metadata": "Media metadata. See Metadata."
      },
      {
        "source_type": "The type of the video source. See VIDEO_SOURCE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_sendstreammessage",
    "name": "SendStreamMessage",
    "description": "Sends data stream messages.\n\nAfter calling CreateDataStream [2/2], you can call this method to send data stream messages to all users in the channel. The SDK has the following restrictions on this method:\n Each client within the channel can have up to 5 data channels simultaneously, with a total shared packet bitrate limit of 30 KB/s for all data channels.\n Each data channel can send up to 60 packets per second, with each packet being a maximum of 1 KB. A successful method call triggers the OnStreamMessage callback on the remote client, from which the remote user gets the stream message. A failed method call triggers the OnStreamMessageError callback on the remote client.\n This method needs to be called after CreateDataStream [2/2] and joining the channel.\n In live streaming scenarios, this method only applies to hosts.",
    "parameters": [
      {
        "streamId": "The data stream ID. You can get the data stream ID by calling CreateDataStream [2/2]."
      },
      {
        "data": "The message to be sent."
      },
      {
        "length": "The length of the data."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setadvancedaudiooptions",
    "name": "SetAdvancedAudioOptions",
    "description": "Sets audio advanced options.\n\nIf you have advanced audio processing requirements, such as capturing and sending stereo audio, you can call this method to set advanced audio options. Call this method after calling JoinChannel [2/2], EnableAudio and EnableLocalAudio.",
    "parameters": [
      {
        "options": "The advanced options for audio. See AdvancedAudioOptions."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setainsmode",
    "name": "SetAINSMode",
    "description": "Sets whether to enable the AI ​​noise suppression function and set the noise suppression mode.\n\nYou can call this method to enable AI noise suppression function. Once enabled, the SDK automatically detects and reduces stationary and non-stationary noise from your audio on the premise of ensuring the quality of human voice. Stationary noise refers to noise signal with constant average statistical properties and negligibly small fluctuations of level within the period of observation. Common sources of stationary noises are:\n Television;\n Air conditioner;\n Machinery, etc. Non-stationary noise refers to noise signal with huge fluctuations of level within the period of observation; common sources of non-stationary noises are:\n Thunder;\n Explosion;\n Cracking, etc.",
    "parameters": [
      {
        "enabled": "Whether to enable the AI noise suppression function: true : Enable the AI noise suppression. false : (Default) Disable the AI noise suppression."
      },
      {
        "mode": "The AI noise suppression modes. See AUDIO_AINS_MODE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setaudioeffectparameters",
    "name": "SetAudioEffectParameters",
    "description": "Sets parameters for SDK preset audio effects.\n\nTo achieve better vocal effects, it is recommended that you call the following APIs before calling this method:\n Call SetAudioScenario to set the audio scenario to high-quality audio scenario, namely AUDIO_SCENARIO_GAME_STREAMING (3).\n Call SetAudioProfile [2/2] to set the profile parameter to AUDIO_PROFILE_MUSIC_HIGH_QUALITY (4) or AUDIO_PROFILE_MUSIC_HIGH_QUALITY_STEREO (5). Call this method to set the following parameters for the local user who sends an audio stream:\n 3D voice effect: Sets the cycle period of the 3D voice effect.\n Pitch correction effect: Sets the basic mode and tonic pitch of the pitch correction effect. Different songs have different modes and tonic pitches. Agora recommends bounding this method with interface elements to enable users to adjust the pitch correction interactively. After setting the audio parameters, all users in the channel can hear the effect.\n Do not set the profile parameter in SetAudioProfile [2/2] to AUDIO_PROFILE_SPEECH_STANDARD (1) or AUDIO_PROFILE_IOT (6), or the method does not take effect.\n You can call this method either before or after joining a channel.\n This method has the best effect on human voice processing, and Agora does not recommend calling this method to process audio data containing music.\n After calling SetAudioEffectParameters, Agora does not recommend you to call the following methods, otherwise the effect set by SetAudioEffectParameters will be overwritten: SetAudioEffectPreset SetVoiceBeautifierPreset SetLocalVoicePitch SetLocalVoiceEqualization SetLocalVoiceReverb SetVoiceBeautifierParameters SetVoiceConversionPreset\n This method relies on the voice beautifier dynamic library libagora_audio_beauty_extension.dll. If the dynamic library is deleted, the function cannot be enabled normally.",
    "parameters": [
      {
        "preset": "The options for SDK preset audio effects: ROOM_ACOUSTICS_3D_VOICE, 3D voice effect:\n You need to set the profile parameter in SetAudioProfile [2/2] to AUDIO_PROFILE_MUSIC_STANDARD_STEREO (3) or AUDIO_PROFILE_MUSIC_HIGH_QUALITY_STEREO (5) before setting this enumerator; otherwise, the enumerator setting does not take effect.\n If the 3D voice effect is enabled, users need to use stereo audio playback devices to hear the anticipated voice effect. PITCH_CORRECTION, Pitch correction effect:"
      },
      {
        "param1": "If you set preset to ROOM_ACOUSTICS_3D_VOICE, param1 sets the cycle period of the 3D voice effect. The value range is [1,60] and the unit is seconds. The default value is 10, indicating that the voice moves around you every 10 seconds.\n If you set preset to PITCH_CORRECTION, param1 indicates the basic mode of the pitch correction effect: 1 : (Default) Natural major scale. 2 : Natural minor scale. 3 : Japanese pentatonic scale."
      },
      {
        "param2": "If you set preset to ROOM_ACOUSTICS_3D_VOICE , you need to set param2 to 0.\n If you set preset to PITCH_CORRECTION, param2 indicates the tonic pitch of the pitch correction effect: 1 : A 2 : A# 3 : B 4 : (Default) C 5 : C# 6 : D 7 : D# 8 : E 9 : F 10 : F# 11 : G 12 : G#"
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setaudioeffectpreset",
    "name": "SetAudioEffectPreset",
    "description": "Sets an SDK preset audio effect.\n\nCall this method to set an SDK preset audio effect for the local user who sends an audio stream. This audio effect does not change the gender characteristics of the original voice. After setting an audio effect, all users in the channel can hear the effect.",
    "parameters": [
      {
        "preset": "The options for SDK preset audio effects. See AUDIO_EFFECT_PRESET."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setaudiomixingdualmonomode",
    "name": "SetAudioMixingDualMonoMode",
    "description": "Sets the channel mode of the current audio file.\n\nIn a stereo music file, the left and right channels can store different audio data. According to your needs, you can set the channel mode to original mode, left channel mode, right channel mode, or mixed channel mode.",
    "parameters": [
      {
        "mode": "The channel mode. See AUDIO_MIXING_DUAL_MONO_MODE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setaudiomixingpitch",
    "name": "SetAudioMixingPitch",
    "description": "Sets the pitch of the local music file.\n\nWhen a local music file is mixed with a local human voice, call this method to set the pitch of the local music file only.",
    "parameters": [
      {
        "pitch": "Sets the pitch of the local music file by the chromatic scale. The default value is 0, which means keeping the original pitch. The value ranges from -12 to 12, and the pitch value between consecutive values is a chromatic value. The greater the absolute value of this parameter, the higher or lower the pitch of the local music file."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setaudiomixingplaybackspeed",
    "name": "SetAudioMixingPlaybackSpeed",
    "description": "Sets the playback speed of the current audio file.\n\nEnsure you call this method after calling StartAudioMixing [2/2] receiving the OnAudioMixingStateChanged callback reporting the state as AUDIO_MIXING_STATE_PLAYING.",
    "parameters": [
      {
        "speed": "The playback speed. Agora recommends that you set this to a value between 50 and 400, defined as follows:\n 50: Half the original speed.\n 100: The original speed.\n 400: 4 times the original speed."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setaudiomixingposition",
    "name": "SetAudioMixingPosition",
    "description": "Sets the audio mixing position.\n\nCall this method to set the playback position of the music file to a different starting position (the default plays from the beginning).",
    "parameters": [
      {
        "pos": "Integer. The playback position (ms)."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setaudioprofile",
    "name": "SetAudioProfile [1/2]",
    "description": "Sets the audio profile and audio scenario.\n\nDeprecated: This method is deprecated. If you need to set the audio profile, use SetAudioProfile [2/2]; if you need to set the audio scenario, use SetAudioScenario.",
    "parameters": [
      {
        "profile": "The audio profile, including the sampling rate, bitrate, encoding mode, and the number of channels. See AUDIO_PROFILE_TYPE."
      },
      {
        "scenario": "The audio scenarios. Under different audio scenarios, the device uses different volume types. See AUDIO_SCENARIO_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setaudioprofile2",
    "name": "SetAudioProfile [2/2]",
    "description": "Sets audio profiles.\n\nIf you need to set the audio scenario, you can either call SetAudioScenario, or Initialize and set the audioScenario in RtcEngineContext.",
    "parameters": [
      {
        "profile": "The audio profile, including the sampling rate, bitrate, encoding mode, and the number of channels. See AUDIO_PROFILE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setaudioscenario",
    "name": "SetAudioScenario",
    "description": "Sets audio scenarios.",
    "parameters": [
      {
        "scenario": "The audio scenarios. Under different audio scenarios, the device uses different volume types. See AUDIO_SCENARIO_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setaudiosessionoperationrestriction",
    "name": "SetAudioSessionOperationRestriction",
    "description": "Sets the operational permission of the SDK on the audio session.\n\nThe SDK and the app can both configure the audio session by default. If you need to only use the app to configure the audio session, this method restricts the operational permission of the SDK on the audio session. You can call this method either before or after joining a channel. Once you call this method to restrict the operational permission of the SDK on the audio session, the restriction takes effect when the SDK needs to change the audio session.\n This method is only available for iOS platforms.\n This method does not restrict the operational permission of the app on the audio session.",
    "parameters": [
      {
        "restriction": "The operational permission of the SDK on the audio session. See AUDIO_SESSION_OPERATION_RESTRICTION. This parameter is in bit mask format, and each bit corresponds to a permission."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setavsyncsource",
    "name": "SetAVSyncSource",
    "description": "Sets the pitch of the local music file.\n\nThe same user may use two devices to send audio and video streams respectively. In order to ensure the time synchronization of the audio and video heard and seen by the receiver, you can call this method on the video sender and pass in the user ID and channel name of the audio sender. The SDK automatically adjusts the sent video stream based on the timestamp of the sent audio stream to ensure that even when the uplink network conditions of the two senders are inconsistent (such as using Wi-Fi and 4G networks respectively), the received audio and video are time-synchronized. Agora recommends that you call this method before joining a channel.",
    "parameters": [
      {
        "channelId": "The channel name that identifies the channel where the audio sender is located."
      },
      {
        "uid": "The user ID of the audio sender."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_irtcengine_setbeautyeffectoptions",
    "name": "SetBeautyEffectOptions",
    "description": "Sets the image enhancement options.\n\nEnables or disables image enhancement, and sets the options.",
    "parameters": [
      {
        "type": "The type of the media source to which the filter effect is applied. See MEDIA_SOURCE_TYPE. In this method, this parameter supports only the following two settings:\n Use the default value PRIMARY_CAMERA_SOURCE if you use camera to capture local video.\n Set this parameter to CUSTOM_VIDEO_SOURCE if you use custom video source."
      },
      {
        "enabled": "Whether to enable the image enhancement function: true : Enable the image enhancement function. false : (Default) Disable the image enhancement function."
      },
      {
        "options": "The image enhancement options. See BeautyOptions."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -4: The current device does not support this feature. Possible reasons include:\n The current device capabilities do not meet the requirements for image enhancement. Agora recommends you replace it with a high-performance device.\n The current device version is lower than Android 5.0 and does not support this feature. Agora recommends you replace the device or upgrade the operating system.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setcameraautoexposurefacemodeenabled",
    "name": "SetCameraAutoExposureFaceModeEnabled",
    "description": "Sets whether to enable auto exposure.\n\nYou must call this method after EnableVideo. The setting result will take effect after the camera is successfully turned on, that is, after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n This method applies to iOS only.",
    "parameters": [
      {
        "enabled": "Whether to enable auto exposure: true : Enable auto exposure. false : Disable auto exposure."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setcameraautofocusfacemodeenabled",
    "name": "SetCameraAutoFocusFaceModeEnabled",
    "description": "Enables the camera auto-face focus function.\n\nBy default, the SDK disables face autofocus on Android and enables face autofocus on iOS. To set face autofocus, call this method. This method is for Android and iOS only.",
    "parameters": [
      {
        "enabled": "Whether to enable face autofocus: true : Enable the camera auto-face focus function. false : Disable face autofocus."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setcameracapturerconfiguration",
    "name": "SetCameraCapturerConfiguration",
    "description": "Sets the camera capture configuration.",
    "parameters": [
      {
        "config": "The camera capture configuration. See CameraCapturerConfiguration. In this method, you do not need to set the deviceId parameter."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setcameradeviceorientation",
    "name": "SetCameraDeviceOrientation",
    "description": "Sets the rotation angle of the captured video.\n\nThis method applies to Windows only.\n You must call this method after EnableVideo. The setting result will take effect after the camera is successfully turned on, that is, after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n When the video capture device does not have the gravity sensing function, you can call this method to manually adjust the rotation angle of the captured video.",
    "parameters": [
      {
        "type": "The video source type. See VIDEO_SOURCE_TYPE."
      },
      {
        "orientation": "The clockwise rotation angle. See VIDEO_ORIENTATION."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setcameraexposurefactor",
    "name": "SetCameraExposureFactor",
    "description": "Sets the camera exposure value.\n\nInsufficient or excessive lighting in the shooting environment can affect the image quality of video capture. To achieve optimal video quality, you can use this method to adjust the camera's exposure value.\n This method is for Android and iOS only.\n You must call this method after EnableVideo. The setting result will take effect after the camera is successfully turned on, that is, after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n Before calling this method, Agora recommends calling IsCameraExposureSupported to check whether the current camera supports adjusting the exposure value.\n By calling this method, you adjust the exposure value of the currently active camera, that is, the camera specified when calling SetCameraCapturerConfiguration.",
    "parameters": [
      {
        "factor": "The camera exposure value. The default value is 0, which means using the default exposure of the camera. The larger the value, the greater the exposure. When the video image is overexposed, you can reduce the exposure value; when the video image is underexposed and the dark details are lost, you can increase the exposure value. If the exposure value you specified is beyond the range supported by the device, the SDK will automatically adjust it to the actual supported range of the device. On Android, the value range is [-20.0, 20.0]. On iOS, the value range is [-8.0, 8.0]."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setcameraexposureposition",
    "name": "SetCameraExposurePosition",
    "description": "Sets the camera exposure position.\n\nYou must call this method after EnableVideo. The setting result will take effect after the camera is successfully turned on, that is, after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n This method is for Android and iOS only.\n After a successful method call, the SDK triggers the OnCameraExposureAreaChanged callback.",
    "parameters": [
      {
        "positionXinView": "The horizontal coordinate of the touchpoint in the view."
      },
      {
        "positionYinView": "The vertical coordinate of the touchpoint in the view."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setcamerafocuspositioninpreview",
    "name": "SetCameraFocusPositionInPreview",
    "description": "Sets the camera manual focus position.\n\nYou must call this method after EnableVideo. The setting result will take effect after the camera is successfully turned on, that is, after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n This method is for Android and iOS only.\n After a successful method call, the SDK triggers the OnCameraFocusAreaChanged callback.",
    "parameters": [
      {
        "positionX": "The horizontal coordinate of the touchpoint in the view."
      },
      {
        "positionY": "The vertical coordinate of the touchpoint in the view."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setcamerastabilizationmode",
    "name": "SetCameraStabilizationMode",
    "description": "Set the camera stabilization mode.\n\nThis method applies to iOS only. The camera stabilization mode is off by default. You need to call this method to turn it on and set the appropriate stabilization mode.",
    "parameters": [
      {
        "mode": "Camera stabilization mode. See CAMERA_STABILIZATION_MODE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setcameratorchon",
    "name": "SetCameraTorchOn",
    "description": "Enables the camera flash.\n\nYou must call this method after EnableVideo. The setting result will take effect after the camera is successfully turned on, that is, after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n This method is for Android and iOS only.",
    "parameters": [
      {
        "isOn": "Whether to turn on the camera flash: true : Turn on the flash. false : (Default) Turn off the flash."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setcamerazoomfactor",
    "name": "SetCameraZoomFactor",
    "description": "Sets the camera zoom factor.\n\nFor iOS devices equipped with multi-lens rear cameras, such as those featuring dual-camera (wide-angle and ultra-wide-angle) or triple-camera (wide-angle, ultra-wide-angle, and telephoto), you can call SetCameraCapturerConfiguration first to set the cameraFocalLengthType as CAMERA_FOCAL_LENGTH_DEFAULT (0) (standard lens). Then, adjust the camera zoom factor to a value less than 1.0. This configuration allows you to capture video with an ultra-wide-angle perspective.\n You must call this method after EnableVideo. The setting result will take effect after the camera is successfully turned on, that is, after the SDK triggers the OnLocalVideoStateChanged callback and returns the local video state as LOCAL_VIDEO_STREAM_STATE_CAPTURING (1).\n This method is for Android and iOS only.",
    "parameters": [
      {
        "factor": "The camera zoom factor. For devices that do not support ultra-wide-angle, the value ranges from 1.0 to the maximum zoom factor; for devices that support ultra-wide-angle, the value ranges from 0.5 to the maximum zoom factor. You can get the maximum zoom factor supported by the device by calling the GetCameraMaxZoomFactor method."
      }
    ],
    "returns": "The camera zoom factor value, if successful.\n < 0: if the method if failed.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setchannelprofile",
    "name": "SetChannelProfile",
    "description": "Sets the channel profile.\n\nYou can call this method to set the channel profile. The SDK adopts different optimization strategies for different channel profiles. For example, in a live streaming scenario, the SDK prioritizes video quality. After initializing the SDK, the default channel profile is the live streaming profile.",
    "parameters": [
      {
        "profile": "The channel profile. See CHANNEL_PROFILE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid.\n -7: The SDK is not initialized.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setclientrole",
    "name": "SetClientRole [1/2]",
    "description": "Sets the client role.\n\nBy default,the SDK sets the user role as audience. You can call this method to set the user role as host. The user role (roles) determines the users' permissions at the SDK level, including whether they can publish audio and video streams in a channel.",
    "parameters": [
      {
        "role": "The user role. See CLIENT_ROLE_TYPE. If you set the user role as an audience member, you cannot publish audio and video streams in the channel. If you want to publish media streams in a channel during live streaming, ensure you set the user role as broadcaster."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).\n -2: The parameter is invalid.\n -7: The SDK is not initialized.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setclientrole2",
    "name": "SetClientRole [2/2]",
    "description": "Set the user role and the audience latency level in a live streaming scenario.\n\nBy default,the SDK sets the user role as audience. You can call this method to set the user role as host. The user role (roles) determines the users' permissions at the SDK level, including whether they can publish audio and video streams in a channel. The difference between this method and SetClientRole [1/2] is that, the former supports setting the audienceLatencyLevel. audienceLatencyLevel needs to be used together with role to determine the level of service that users can enjoy within their permissions. For example, an audience member can choose to receive remote streams with low latency or ultra-low latency. Latency of different levels differs in billing.",
    "parameters": [
      {
        "role": "The user role. See CLIENT_ROLE_TYPE. If you set the user role as an audience member, you cannot publish audio and video streams in the channel. If you want to publish media streams in a channel during live streaming, ensure you set the user role as broadcaster."
      },
      {
        "options": "The detailed options of a user, including the user level. See ClientRoleOptions."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).\n -2: The parameter is invalid.\n -5: The request is rejected.\n -7: The SDK is not initialized.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setcloudproxy",
    "name": "SetCloudProxy",
    "description": "Sets up cloud proxy service.\n\nWhen users' network access is restricted by a firewall, configure the firewall to allow specific IP addresses and ports provided by Agora; then, call this method to enable the cloud proxyType and set the cloud proxy type with the proxyType parameter. After successfully connecting to the cloud proxy, the SDK triggers the OnConnectionStateChanged (CONNECTION_STATE_CONNECTING, CONNECTION_CHANGED_SETTING_PROXY_SERVER) callback. To disable the cloud proxy that has been set, call the SetCloudProxy (NONE_PROXY). To change the cloud proxy type that has been set, call the SetCloudProxy (NONE_PROXY) first, and then call the SetCloudProxy to set the proxyType you want.\n Agora recommends that you call this method before joining a channel.\n When a user is behind a firewall and uses the Force UDP cloud proxy, the services for Media Push and cohosting across channels are not available.\n When you use the Force TCP cloud proxy, note that an error would occur when calling the StartAudioMixing [2/2] method to play online music files in the HTTP protocol. The services for Media Push and cohosting across channels use the cloud proxy with the TCP protocol.",
    "parameters": [
      {
        "proxyType": "The type of the cloud proxy. See CLOUD_PROXY_TYPE. This parameter is mandatory. The SDK reports an error if you do not pass in a value."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid.\n -7: The SDK is not initialized.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setcolorenhanceoptions",
    "name": "SetColorEnhanceOptions",
    "description": "Sets color enhancement.\n\nThe video images captured by the camera can have color distortion. The color enhancement feature intelligently adjusts video characteristics such as saturation and contrast to enhance the video color richness and color reproduction, making the video more vivid. You can call this method to enable the color enhancement feature and set the options of the color enhancement effect.\n Call this method after calling EnableVideo.\n The color enhancement feature has certain performance requirements on devices. With color enhancement turned on, Agora recommends that you change the color enhancement level to one that consumes less performance or turn off color enhancement if your device is experiencing severe heat problems.\n This method relies on the image enhancement dynamic library libagora_clear_vision_extension.dll. If the dynamic library is deleted, the function cannot be enabled normally.",
    "parameters": [
      {
        "type": "The type of the media source to which the filter effect is applied. See MEDIA_SOURCE_TYPE. In this method, this parameter supports only the following two settings:\n Use the default value PRIMARY_CAMERA_SOURCE if you use camera to capture local video.\n Set this parameter to CUSTOM_VIDEO_SOURCE if you use custom video source."
      },
      {
        "enabled": "Whether to enable color enhancement: true Enable color enhancement. false : (Default) Disable color enhancement."
      },
      {
        "options": "The color enhancement options. See ColorEnhanceOptions."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setdefaultaudioroutetospeakerphone",
    "name": "SetDefaultAudioRouteToSpeakerphone",
    "description": "Sets the default audio playback route.\n\nThis method is for Android and iOS only. Most mobile phones have two audio routes: an earpiece at the top, and a speakerphone at the bottom. The earpiece plays at a lower volume, and the speakerphone at a higher volume. When setting the default audio route, you determine whether audio playback comes through the earpiece or speakerphone when no external audio device is connected. In different scenarios, the default audio routing of the system is also different. See the following:\n Voice call: Earpiece.\n Audio broadcast: Speakerphone.\n Video call: Speakerphone.\n Video broadcast: Speakerphone. You can call this method to change the default audio route. After calling this method to set the default audio route, the actual audio route of the system will change with the connection of external audio devices (wired headphones or Bluetooth headphones).",
    "parameters": [
      {
        "defaultToSpeaker": "Whether to set the speakerphone as the default audio route: true : Set the speakerphone as the default audio route. false : Set the earpiece as the default audio route."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setdirectcdnstreamingaudioconfiguration",
    "name": "SetDirectCdnStreamingAudioConfiguration",
    "description": "Sets the audio profile of the audio streams directly pushed to the CDN by the host.",
    "parameters": [
      {
        "profile": "The audio profile, including the sampling rate, bitrate, encoding mode, and the number of channels. See AUDIO_PROFILE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setdirectcdnstreamingvideoconfiguration",
    "name": "SetDirectCdnStreamingVideoConfiguration",
    "description": "Sets the video profile of the media streams directly pushed to the CDN by the host.\n\nThis method only affects video streams captured by cameras or screens, or from custom video capture sources. That is, when you set publishCameraTrack or publishCustomVideoTrack in DirectCdnStreamingMediaOptions as true to capture videos, you can call this method to set the video profiles. If your local camera does not support the video resolution you set,the SDK automatically adjusts the video resolution to a value that is closest to your settings for capture, encoding or streaming, with the same aspect ratio as the resolution you set. You can get the actual resolution of the video streams through the OnDirectCdnStreamingStats callback.",
    "parameters": [
      {
        "config": "Video profile. See VideoEncoderConfiguration. During CDN live streaming, Agora only supports setting ORIENTATION_MODE as ORIENTATION_MODE_FIXED_LANDSCAPE or ORIENTATION_MODE_FIXED_PORTRAIT."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setdualstreammode",
    "name": "SetDualStreamMode [1/2]",
    "description": "Sets the dual-stream mode on the sender side.\n\nThe SDK defaults to enabling low-quality video stream adaptive mode (AUTO_SIMULCAST_STREAM) on the sender side, which means the sender does not actively send low-quality video stream. The receiving end with the role of the host can initiate a low-quality video stream request by calling SetRemoteVideoStreamType, and upon receiving the request, the sending end automatically starts sending low-quality stream.\n If you want to modify this behavior, you can call this method and set mode to DISABLE_SIMULCAST_STREAM (never send low-quality video streams) or ENABLE_SIMULCAST_STREAM (always send low-quality video streams).\n If you want to restore the default behavior after making changes, you can call this method again with mode set to AUTO_SIMULCAST_STREAM. The difference and connection between this method and EnableDualStreamMode [1/2] is as follows:\n When calling this method and setting mode to DISABLE_SIMULCAST_STREAM, it has the same effect as EnableDualStreamMode [1/2] (false).\n When calling this method and setting mode to ENABLE_SIMULCAST_STREAM, it has the same effect as EnableDualStreamMode [1/2] (true).\n Both methods can be called before and after joining a channel. If both methods are used, the settings in the method called later takes precedence.",
    "parameters": [
      {
        "mode": "The mode in which the video stream is sent. See SIMULCAST_STREAM_MODE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setdualstreammode2",
    "name": "SetDualStreamMode [2/2]",
    "description": "Sets dual-stream mode configuration on the sender side.\n\nThe SDK defaults to enabling low-quality video stream adaptive mode (AUTO_SIMULCAST_STREAM) on the sender side, which means the sender does not actively send low-quality video stream. The receiving end with the role of the host can initiate a low-quality video stream request by calling SetRemoteVideoStreamType, and upon receiving the request, the sending end automatically starts sending low-quality stream.\n If you want to modify this behavior, you can call this method and set mode to DISABLE_SIMULCAST_STREAM (never send low-quality video streams) or ENABLE_SIMULCAST_STREAM (always send low-quality video streams).\n If you want to restore the default behavior after making changes, you can call this method again with mode set to AUTO_SIMULCAST_STREAM. The difference between this method and SetDualStreamMode [1/2] is that this method can also configure the low-quality video stream, and the SDK sends the stream according to the configuration in streamConfig. The difference and connection between this method and EnableDualStreamMode [2/2] is as follows:\n When calling this method and setting mode to DISABLE_SIMULCAST_STREAM, it has the same effect as calling EnableDualStreamMode [2/2] and setting enabled to false.\n When calling this method and setting mode to ENABLE_SIMULCAST_STREAM, it has the same effect as calling EnableDualStreamMode [2/2] and setting enabled to true.\n Both methods can be called before and after joining a channel. If both methods are used, the settings in the method called later takes precedence.",
    "parameters": [
      {
        "streamConfig": "The configuration of the low-quality video stream. See SimulcastStreamConfig. When setting mode to DISABLE_SIMULCAST_STREAM, setting streamConfig will not take effect."
      },
      {
        "mode": "The mode in which the video stream is sent. See SIMULCAST_STREAM_MODE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setearmonitoringaudioframeparameters",
    "name": "SetEarMonitoringAudioFrameParameters",
    "description": "Sets the format of the in-ear monitoring raw audio data.\n\nThis method is used to set the in-ear monitoring audio data format reported by the OnEarMonitoringAudioFrame callback.\n Before calling this method, you need to call EnableInEarMonitoring, and set includeAudioFilters to EAR_MONITORING_FILTER_BUILT_IN_AUDIO_FILTERS or EAR_MONITORING_FILTER_NOISE_SUPPRESSION.\n The SDK calculates the sampling interval based on the samplesPerCall, sampleRate and channel parameters set in this method. Sample interval (sec) = samplePerCall /(sampleRate × channel). Ensure that the sample interval ≥ 0.01 (s). The SDK triggers the OnEarMonitoringAudioFrame callback according to the sampling interval.",
    "parameters": [
      {
        "sampleRate": "The sample rate of the audio data reported in the OnEarMonitoringAudioFrame callback, which can be set as 8,000, 16,000, 32,000, 44,100, or 48,000 Hz."
      },
      {
        "channel": "The number of audio channels reported in the OnEarMonitoringAudioFrame callback.\n 1: Mono.\n 2: Stereo."
      },
      {
        "mode": "The use mode of the audio frame. See RAW_AUDIO_FRAME_OP_MODE_TYPE."
      },
      {
        "samplesPerCall": "The number of data samples reported in the OnEarMonitoringAudioFrame callback, such as 1,024 for the Media Push."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_seteffectposition",
    "name": "SetEffectPosition",
    "description": "Sets the playback position of an audio effect file.\n\nAfter a successful setting, the local audio effect file starts playing at the specified position. Call this method after playEffect.",
    "parameters": [
      {
        "soundId": "The audio effect ID. The ID of each audio effect file is unique."
      },
      {
        "pos": "The playback position (ms) of the audio effect file."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_seteffectsvolume",
    "name": "SetEffectsVolume",
    "description": "Sets the volume of the audio effects.",
    "parameters": [
      {
        "volume": "The playback volume. The value range is [0, 100]. The default value is 100, which represents the original volume."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setenablespeakerphone",
    "name": "SetEnableSpeakerphone",
    "description": "Enables/Disables the audio route to the speakerphone.\n\nFor the default audio route in different scenarios, see. This method is for Android and iOS only.",
    "parameters": [
      {
        "speakerOn": "Sets whether to enable the speakerphone or earpiece: true : Enable device state monitoring. The audio route is the speakerphone. false : Disable device state monitoring. The audio route is the earpiece."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setextensionproperty",
    "name": "SetExtensionProperty",
    "description": "Sets the properties of the extension.\n\nAfter enabling the extension, you can call this method to set the properties of the extension.",
    "parameters": [
      {
        "type": "Source type of the extension. See MEDIA_SOURCE_TYPE."
      },
      {
        "provider": "The name of the extension provider."
      },
      {
        "extension": "The name of the extension."
      },
      {
        "key": "The key of the extension."
      },
      {
        "value": "The value of the extension key."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setextensionproviderproperty",
    "name": "SetExtensionProviderProperty",
    "description": "Sets the properties of the extension provider.\n\nYou can call this method to set the attributes of the extension provider and initialize the relevant parameters according to the type of the provider.",
    "parameters": [
      {
        "value": "The value of the extension key."
      },
      {
        "key": "The key of the extension."
      },
      {
        "provider": "The name of the extension provider."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setexternalmediaprojection",
    "name": "SetExternalMediaProjection",
    "description": "Configures MediaProjection outside of the SDK to capture screen video streams.\n\nThis method is for Android only. After successfully calling this method, the external MediaProjection you set will replace the MediaProjection requested by the SDK to capture the screen video stream. When the screen sharing is stopped or IRtcEngine is destroyed, the SDK will automatically release the MediaProjection.",
    "parameters": [
      {
        "mediaProjection": "An object used to capture screen video streams."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setexternalremoteeglcontext",
    "name": "SetExternalRemoteEglContext",
    "description": "Sets the EGL context for rendering remote video streams.\n\nThis method can replace the default remote EGL context within the SDK, making it easier to manage the EGL context. When the engine is destroyed, the SDK will automatically release the EGL context. This method is for Android only.",
    "parameters": [
      {
        "eglContext": "The EGL context for rendering remote video streams."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setfiltereffectoptions",
    "name": "SetFilterEffectOptions",
    "description": "Sets the filter effect options and specifies the media source.",
    "parameters": [
      {
        "enabled": "Whether to enable the filter effect: true : Yes. false : (Default) No."
      },
      {
        "options": "The filter effect options. See FilterEffectOptions."
      },
      {
        "type": "The type of the media source to which the filter effect is applied. See MEDIA_SOURCE_TYPE. In this method, this parameter supports only the following two settings:\n Use the default value PRIMARY_CAMERA_SOURCE if you use camera to capture local video.\n Set this parameter to CUSTOM_VIDEO_SOURCE if you use custom video source."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setheadphoneeqparameters",
    "name": "SetHeadphoneEQParameters",
    "description": "Sets the low- and high-frequency parameters of the headphone equalizer.\n\nIn a spatial audio effect scenario, if the preset headphone equalization effect is not achieved after calling the SetHeadphoneEQPreset method, you can further adjust the headphone equalization effect by calling this method.",
    "parameters": [
      {
        "lowGain": "The low-frequency parameters of the headphone equalizer. The value range is [-10,10]. The larger the value, the deeper the sound."
      },
      {
        "highGain": "The high-frequency parameters of the headphone equalizer. The value range is [-10,10]. The larger the value, the sharper the sound."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setheadphoneeqpreset",
    "name": "SetHeadphoneEQPreset",
    "description": "Sets the preset headphone equalization effect.\n\nThis method is mainly used in spatial audio effect scenarios. You can select the preset headphone equalizer to listen to the audio to achieve the expected audio experience. If the headphones you use already have a good equalization effect, you may not get a significant improvement when you call this method, and could even diminish the experience.",
    "parameters": [
      {
        "preset": "The preset headphone equalization effect. See HEADPHONE_EQUALIZER_PRESET."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setinearmonitoringvolume",
    "name": "SetInEarMonitoringVolume",
    "description": "Sets the volume of the in-ear monitor.",
    "parameters": [
      {
        "volume": "The volume of the user. The value range is [0,400].\n 0: Mute.\n 100: (Default) The original volume.\n 400: Four times the original volume (amplifying the audio signals by four times)."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: Invalid parameter settings, such as in-ear monitoring volume exceeding the valid range (< 0 or > 400).",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setlivetranscoding",
    "name": "",
    "description": "Sets the transcoding configurations for media push.\n\nDeprecated: This method is deprecated. Use StartRtmpStreamWithTranscoding or UpdateRtmpTranscoding instead according to your needs. This method sets the video layout and audio settings for media push. The SDK triggers the OnTranscodingUpdated callback when you call this method to update the transcoding settings.\n This method takes effect only when you are a host in live interactive streaming.\n Ensure that the Media Push function is enabled.\n If you call this method to set the transcoding configuration for the first time, the SDK does not trigger the OnTranscodingUpdated callback.\n Call this method after joining a channel.",
    "parameters": [
      {
        "transcoding": "The transcoding configurations for the media push. See LiveTranscoding."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_irtcengine_setlocalaccesspoint",
    "name": "SetLocalAccessPoint",
    "description": "Configures the connection with the access module of the Agora network private media server.\n\nAfter successfully deploying the Agora private media server and integrating the RTC 4.x SDK on the intranet terminal, you can call this method to specify the Local Access Point and assign the access module to the SDK.",
    "parameters": [
      {
        "config": "The configurations of the Local Access Point. See LocalAccessPointConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_irtcengine_setlocalrendermode",
    "name": "SetLocalRenderMode [1/2]",
    "description": "Sets the local video display mode.\n\nDeprecated: This method is deprecated. Use SetLocalRenderMode [2/2] instead. Call this method to set the local video display mode. This method can be called multiple times during a call to change the display mode.",
    "parameters": [
      {
        "renderMode": "The local video display mode. See RENDER_MODE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setlocalrendermode2",
    "name": "SetLocalRenderMode [2/2]",
    "description": "Updates the display mode of the local video view.\n\nAfter initializing the local video view, you can call this method to update its rendering and mirror modes. It affects only the video view that the local user sees and does not impact the publishing of the local video.",
    "parameters": [
      {
        "renderMode": "The local video display mode. See RENDER_MODE_TYPE."
      },
      {
        "mirrorMode": "The mirror mode of the local video view. See VIDEO_MIRROR_MODE_TYPE. This parameter is only effective when rendering custom videos. If you want to mirror the video view, set the scaleX of the GameObject attached to the video view as -1 or +1. If you use a front camera, the SDK enables the mirror mode by default; if you use a rear camera, the SDK disables the mirror mode by default."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setlocalrendertargetfps",
    "name": "SetLocalRenderTargetFps",
    "description": "Sets the maximum frame rate for rendering local video.",
    "parameters": [
      {
        "sourceType": "The type of the video source. See VIDEO_SOURCE_TYPE."
      },
      {
        "targetFps": "The capture frame rate (fps) of the local video. Sopported values are: 1, 7, 10, 15, 24, 30, 60. Set this parameter to a value lower than the actual video frame rate; otherwise, the settings do not take effect."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setlocalvideodatasourceposition",
    "name": "SetLocalVideoDataSourcePosition",
    "description": "Sets the observation position of the local video frame.",
    "parameters": [
      {
        "position": "The observation position of the video frame. See VIDEO_MODULE_POSITION.\n This method currently only supports setting the observation position to POSITION_POST_CAPTURER or POSITION_PRE_ENCODER.\n The video frames obtained at POSITION_POST_CAPTURER are not cropped and have a high frame rate, while the video frames obtained at POSITION_PRE_ENCODER are cropped before being sent, with a frame rate lower than or equal to the frame rate of the camera capture."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setlocalvideomirrormode",
    "name": "SetLocalVideoMirrorMode",
    "description": "Sets the local video mirror mode.\n\nDeprecated: This method is deprecated. Use SetLocalRenderMode [2/2] instead.",
    "parameters": [
      {
        "mirrorMode": "The local video mirror mode. See VIDEO_MIRROR_MODE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setlocalvoiceequalization",
    "name": "SetLocalVoiceEqualization",
    "description": "Sets the local voice equalization effect.",
    "parameters": [
      {
        "bandFrequency": "The band frequency. The value ranges between 0 and 9; representing the respective 10-band center frequencies of the voice effects, including 31, 62, 125, 250, 500, 1k, 2k, 4k, 8k, and 16k Hz. See AUDIO_EQUALIZATION_BAND_FREQUENCY."
      },
      {
        "bandGain": "The gain of each band in dB. The value ranges between -15 and 15. The default value is 0."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setlocalvoiceformant",
    "name": "SetLocalVoiceFormant",
    "description": "Set the formant ratio to change the timbre of human voice.\n\nFormant ratio affects the timbre of voice. The smaller the value, the deeper the sound will be, and the larger, the sharper. After you set the formant ratio, all users in the channel can hear the changed voice. If you want to change the timbre and pitch of voice at the same time, Agora recommends using this method together with SetLocalVoicePitch.",
    "parameters": [
      {
        "formantRatio": "The formant ratio. The value range is [-1.0, 1.0]. The default value is 0.0, which means do not change the timbre of the voice. Agora recommends setting this value within the range of [-0.4, 0.6]. Otherwise, the voice may be seriously distorted."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setlocalvoicepitch",
    "name": "SetLocalVoicePitch",
    "description": "Changes the voice pitch of the local speaker.",
    "parameters": [
      {
        "pitch": "The local voice pitch. The value range is [0.5,2.0]. The lower the value, the lower the pitch. The default value is 1.0 (no change to the pitch)."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setlocalvoicereverb",
    "name": "SetLocalVoiceReverb",
    "description": "Sets the local voice reverberation.\n\nThe SDK provides an easier-to-use method, SetAudioEffectPreset, to directly implement preset reverb effects for such as pop, R&B, and KTV. You can call this method either before or after joining a channel.",
    "parameters": [
      {
        "reverbKey": "The reverberation key. Agora provides five reverberation keys, see AUDIO_REVERB_TYPE."
      },
      {
        "value": "The value of the reverberation key."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setlogfile",
    "name": "SetLogFile",
    "description": "Sets the log file.\n\nDeprecated: This method is deprecated. Set the log file path by configuring the context parameter when calling Initialize. Specifies an SDK output log file. The log file records all log data for the SDK’s operation.",
    "parameters": [
      {
        "filePath": "The complete path of the log files. These log files are encoded in UTF-8."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setlogfilesize",
    "name": "SetLogFileSize",
    "description": "Sets the log file size.\n\nDeprecated: Use the logConfig parameter in Initialize instead. By default, the SDK generates five SDK log files and five API call log files with the following rules:\n The SDK log files are: agorasdk.log, agorasdk.1.log, agorasdk.2.log, agorasdk.3.log, and agorasdk.4.log.\n The API call log files are: agoraapi.log, agoraapi.1.log, agoraapi.2.log, agoraapi.3.log, and agoraapi.4.log.\n The default size of each SDK log file and API log file is 2,048 KB. These log files are encoded in UTF-8.\n The SDK writes the latest logs in agorasdk.log or agoraapi.log.\n When agorasdk.log is full, the SDK processes the log files in the following order:\n Delete the agorasdk.4.log file (if any).\n Rename agorasdk.3.log to agorasdk.4.log.\n Rename agorasdk.2.log to agorasdk.3.log.\n Rename agorasdk.1.log to agorasdk.2.log.\n Create a new agorasdk.log file.\n The overwrite rules for the agoraapi.log file are the same as for agorasdk.log. This method is used to set the size of the agorasdk.log file only and does not effect the agoraapi.log file.",
    "parameters": [
      {
        "fileSizeInKBytes": "The size (KB) of an agorasdk.log file. The value range is [128,20480]. The default value is 2,048 KB. If you set fileSizeInKByte smaller than 128 KB, the SDK automatically adjusts it to 128 KB; if you set fileSizeInKByte greater than 20,480 KB, the SDK automatically adjusts it to 20,480 KB."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setlogfilter",
    "name": "SetLogFilter",
    "description": "Sets the log output level of the SDK.\n\nDeprecated: Use logConfig in Initialize instead.",
    "parameters": [
      {
        "filter": "The output log level of the SDK."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setloglevel",
    "name": "SetLogLevel",
    "description": "Sets the output log level of the SDK.\n\nDeprecated: This method is deprecated. Set the log file level by configuring the context parameter when calling Initialize. Choose a level to see the logs preceding that level.",
    "parameters": [
      {
        "level": "The log level. See LOG_LEVEL."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setlowlightenhanceoptions",
    "name": "SetLowlightEnhanceOptions",
    "description": "Sets low-light enhancement.\n\nYou can call this method to enable the color enhancement feature and set the options of the color enhancement effect.",
    "parameters": [
      {
        "enabled": "Whether to enable low-light enhancement: true : Enable low-light enhancement. false : (Default) Disable low-light enhancement."
      },
      {
        "options": "The low-light enhancement options. See LowlightEnhanceOptions."
      },
      {
        "type": "The type of the media source to which the filter effect is applied. See MEDIA_SOURCE_TYPE. In this method, this parameter supports only the following two settings:\n Use the default value PRIMARY_CAMERA_SOURCE if you use camera to capture local video.\n Set this parameter to CUSTOM_VIDEO_SOURCE if you use custom video source."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setmaxmetadatasize",
    "name": "SetMaxMetadataSize",
    "description": "Sets the maximum size of the media metadata.\n\nAfter calling RegisterMediaMetadataObserver, you can call this method to set the maximum size of the media metadata.",
    "parameters": [
      {
        "size": "The maximum size of media metadata."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setmixedaudioframeparameters",
    "name": "SetMixedAudioFrameParameters",
    "description": "Set the format of the raw audio data after mixing for audio capture and playback.\n\nThe SDK calculates the sampling interval based on the samplesPerCall, sampleRate and channel parameters set in this method. Sample interval (sec) = samplePerCall /(sampleRate × channel). Ensure that the sample interval ≥ 0.01 (s). The SDK triggers the OnMixedAudioFrame callback according to the sampling interval.",
    "parameters": [
      {
        "samplesPerCall": "The number of data samples, such as 1024 for the Media Push."
      },
      {
        "channel": "The number of audio channels. You can set the value as 1 or 2.\n 1: Mono.\n 2: Stereo."
      },
      {
        "sampleRate": "The sample rate returned in the callback, which can be set as 8000, 16000, 32000, 44100, or 48000 Hz."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setparameters",
    "name": "SetParameters [1/2]",
    "description": "Provides technical preview functionalities or special customizations by configuring the SDK with JSON options.",
    "parameters": [
      {},
      {
        "key": "The key of the option."
      },
      {
        "value": "The value of the key."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setplaybackaudioframebeforemixingparameters",
    "name": "SetPlaybackAudioFrameBeforeMixingParameters",
    "description": "Sets the format of the raw audio playback data before mixing.\n\nThe SDK triggers the OnPlaybackAudioFrameBeforeMixing callback according to the sampling interval.",
    "parameters": [
      {
        "channel": "The number of audio channels. You can set the value as 1 or 2.\n 1: Mono.\n 2: Stereo."
      },
      {
        "sampleRate": "The sample rate returned in the callback, which can be set as 8000, 16000, 32000, 44100, or 48000 Hz."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setplaybackaudioframeparameters",
    "name": "SetPlaybackAudioFrameParameters",
    "description": "Sets the format of the raw audio playback data.\n\nThe SDK calculates the sampling interval based on the samplesPerCall, sampleRate and channel parameters set in this method. Sample interval (sec) = samplePerCall /(sampleRate × channel). Ensure that the sample interval ≥ 0.01 (s). The SDK triggers the OnPlaybackAudioFrame callback according to the sampling interval.",
    "parameters": [
      {
        "samplesPerCall": "The number of data samples, such as 1024 for the Media Push."
      },
      {
        "mode": "The use mode of the audio frame. See RAW_AUDIO_FRAME_OP_MODE_TYPE."
      },
      {
        "channel": "The number of audio channels. You can set the value as 1 or 2.\n 1: Mono.\n 2: Stereo."
      },
      {
        "sampleRate": "The sample rate returned in the callback, which can be set as 8000, 16000, 32000, 44100, or 48000 Hz."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setrecordingaudioframeparameters",
    "name": "SetRecordingAudioFrameParameters",
    "description": "Sets the format of the captured raw audio data.\n\nThe SDK calculates the sampling interval based on the samplesPerCall, sampleRate and channel parameters set in this method. Sample interval (sec) = samplePerCall /(sampleRate × channel). Ensure that the sample interval ≥ 0.01 (s). The SDK triggers the OnRecordAudioFrame callback according to the sampling interval.",
    "parameters": [
      {
        "sampleRate": "The sample rate returned in the callback, which can be set as 8000, 16000, 32000, 44100, or 48000 Hz."
      },
      {
        "channel": "The number of audio channels. You can set the value as 1 or 2.\n 1: Mono.\n 2: Stereo."
      },
      {
        "mode": "The use mode of the audio frame. See RAW_AUDIO_FRAME_OP_MODE_TYPE."
      },
      {
        "samplesPerCall": "The number of data samples, such as 1024 for the Media Push."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setremotedefaultvideostreamtype",
    "name": "SetRemoteDefaultVideoStreamType",
    "description": "Sets the default video stream type to subscribe to.\n\nThe SDK will dynamically adjust the size of the corresponding video stream based on the size of the video window to save bandwidth and computing resources. The default aspect ratio of the low-quality video stream is the same as that of the high-quality video stream. According to the current aspect ratio of the high-quality video stream, the system will automatically allocate the resolution, frame rate, and bitrate of the low-quality video stream. Depending on the default behavior of the sender and the specific settings when calling SetDualStreamMode [2/2], the scenarios for the receiver calling this method are as follows:\n The SDK enables low-quality video stream adaptive mode (AUTO_SIMULCAST_STREAM) on the sender side by default, meaning only the high-quality video stream is transmitted. Only the receiver with the role of the host can call this method to initiate a low-quality video stream request. Once the sender receives the request, it starts automatically sending the low-quality video stream. At this point, all users in the channel can call this method to switch to low-quality video stream subscription mode.\n If the sender calls SetDualStreamMode [2/2] and sets mode to DISABLE_SIMULCAST_STREAM (never send low-quality video stream), then calling this method will have no effect.\n If the sender calls SetDualStreamMode [2/2] and sets mode to ENABLE_SIMULCAST_STREAM (always send low-quality video stream), both the host and audience receivers can call this method to switch to low-quality video stream subscription mode.",
    "parameters": [
      {
        "streamType": "The default video-stream type. See VIDEO_STREAM_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setremoterendermode2",
    "name": "SetRemoteRenderMode",
    "description": "Updates the display mode of the video view of a remote user.\n\nAfter initializing the video view of a remote user, you can call this method to update its rendering and mirror modes. This method affects only the video view that the local user sees.\n During a call, you can call this method as many times as necessary to update the display mode of the video view of a remote user.",
    "parameters": [
      {
        "uid": "The user ID of the remote user."
      },
      {
        "renderMode": "The rendering mode of the remote user view. For details, see RENDER_MODE_TYPE."
      },
      {
        "mirrorMode": "The mirror mode of the remote user view. See VIDEO_MIRROR_MODE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setremoterendertargetfps",
    "name": "SetRemoteRenderTargetFps",
    "description": "Sets the maximum frame rate for rendering remote video.",
    "parameters": [
      {
        "targetFps": "The capture frame rate (fps) of the local video. Sopported values are: 1, 7, 10, 15, 24, 30, 60. Set this parameter to a value lower than the actual video frame rate; otherwise, the settings do not take effect."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setremotesubscribefallbackoption",
    "name": "SetRemoteSubscribeFallbackOption",
    "description": "Sets the fallback option for the subscribed video stream based on the network conditions.\n\nAn unstable network affects the audio and video quality in a video call or interactive live video streaming. If option is set as STREAM_FALLBACK_OPTION_VIDEO_STREAM_LOW or STREAM_FALLBACK_OPTION_AUDIO_ONLY, the SDK automatically switches the video from a high-quality stream to a low-quality stream or disables the video when the downlink network conditions cannot support both audio and video to guarantee the quality of the audio. Meanwhile, the SDK continuously monitors network quality and resumes subscribing to audio and video streams when the network quality improves. When the subscribed video stream falls back to an audio-only stream, or recovers from an audio-only stream to an audio-video stream, the SDK triggers the OnRemoteSubscribeFallbackToAudioOnly callback.",
    "parameters": [
      {
        "option": "Fallback options for the subscribed stream. See STREAM_FALLBACK_OPTIONS."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setremoteuserspatialaudioparams",
    "name": "SetRemoteUserSpatialAudioParams",
    "description": "Sets the spatial audio effect parameters of the remote user.\n\nCall this method after EnableSpatialAudio. After successfully setting the spatial audio effect parameters of the remote user, the local user can hear the remote user with a sense of space.",
    "parameters": [
      {
        "uid": "The user ID. This parameter must be the same as the user ID passed in when the user joined the channel."
      },
      {
        "param": "The spatial audio parameters. See SpatialAudioParams."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setremotevideostreamtype",
    "name": "SetRemoteVideoStreamType",
    "description": "Sets the video stream type to subscribe to.\n\nDepending on the default behavior of the sender and the specific settings when calling SetDualStreamMode [2/2], the scenarios for the receiver calling this method are as follows:\n The SDK enables low-quality video stream adaptive mode (AUTO_SIMULCAST_STREAM) on the sender side by default, meaning only the high-quality video stream is transmitted. Only the receiver with the role of the host can call this method to initiate a low-quality video stream request. Once the sender receives the request, it starts automatically sending the low-quality video stream. At this point, all users in the channel can call this method to switch to low-quality video stream subscription mode.\n If the sender calls SetDualStreamMode [2/2] and sets mode to DISABLE_SIMULCAST_STREAM (never send low-quality video stream), then calling this method will have no effect.\n If the sender calls SetDualStreamMode [2/2] and sets mode to ENABLE_SIMULCAST_STREAM (always send low-quality video stream), both the host and audience receivers can call this method to switch to low-quality video stream subscription mode. The SDK will dynamically adjust the size of the corresponding video stream based on the size of the video window to save bandwidth and computing resources. The default aspect ratio of the low-quality video stream is the same as that of the high-quality video stream. According to the current aspect ratio of the high-quality video stream, the system will automatically allocate the resolution, frame rate, and bitrate of the low-quality video stream.\n You can call this method either before or after joining a channel.\n If you call both this method and SetRemoteDefaultVideoStreamType, the setting of this method takes effect.",
    "parameters": [
      {
        "uid": "The user ID."
      },
      {
        "streamType": "The video stream type, see VIDEO_STREAM_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setremotevideosubscriptionoptions",
    "name": "SetRemoteVideoSubscriptionOptions",
    "description": "Options for subscribing to remote video streams.\n\nWhen a remote user has enabled dual-stream mode, you can call this method to choose the option for subscribing to the video streams sent by the remote user. The default subscription behavior of the SDK for remote video streams depends on the type of registered video observer:\n If the IVideoFrameObserver observer is registered, the default is to subscribe to both raw data and encoded data.\n If the IVideoEncodedFrameObserver observer is registered, the default is to subscribe only to the encoded data.\n If both types of observers are registered, the default behavior follows the last registered video observer. For example, if the last registered observer is the IVideoFrameObserver observer, the default is to subscribe to both raw data and encoded data. If you want to modify the default behavior, or set different subscription options for different uids, you can call this method to set it.",
    "parameters": [
      {
        "uid": "The user ID of the remote user."
      },
      {
        "options": "The video subscription options. See VideoSubscriptionOptions."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setremotevoiceposition",
    "name": "SetRemoteVoicePosition",
    "description": "Sets the 2D position (the position on the horizontal plane) of the remote user's voice.\n\nThis method sets the 2D position and volume of a remote user, so that the local user can easily hear and identify the remote user's position. When the local user calls this method to set the voice position of a remote user, the voice difference between the left and right channels allows the local user to track the real-time position of the remote user, creating a sense of space. This method applies to massive multiplayer online games, such as Battle Royale games.\n For this method to work, enable stereo panning for remote users by calling the EnableSoundPositionIndication method before joining a channel.\n For the best voice positioning, Agora recommends using a wired headset.\n Call this method after joining a channel.",
    "parameters": [
      {
        "uid": "The user ID of the remote user."
      },
      {
        "pan": "The voice position of the remote user. The value ranges from -1.0 to 1.0:\n 0.0: (Default) The remote voice comes from the front.\n -1.0: The remote voice comes from the left.\n 1.0: The remote voice comes from the right."
      },
      {
        "gain": "The volume of the remote user. The value ranges from 0.0 to 100.0. The default value is 100.0 (the original volume of the remote user). The smaller the value, the lower the volume."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setrouteincommunicationmode",
    "name": "SetRouteInCommunicationMode",
    "description": "Selects the audio playback route in communication audio mode.\n\nThis method is used to switch the audio route from Bluetooth headphones to earpiece, wired headphones or speakers in communication audio mode (). This method is for Android only.",
    "parameters": [
      {
        "route": "The audio playback route you want to use:\n -1: The default audio route.\n 0: Headphones with microphone.\n 1: Handset.\n 2: Headphones without microphone.\n 3: Device's built-in speaker.\n 4: (Not supported yet) External speakers.\n 5: Bluetooth headphones.\n 6: USB device."
      }
    ],
    "returns": "Without practical meaning.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setscreencapturecontenthint",
    "name": "SetScreenCaptureContentHint",
    "description": "Sets the content hint for screen sharing.\n\nA content hint suggests the type of the content being shared, so that the SDK applies different optimization algorithms to different types of content. If you don't call this method, the default content hint is CONTENT_HINT_NONE. You can call this method either before or after you start screen sharing.",
    "parameters": [
      {
        "contentHint": "The content hint for screen sharing. See VIDEO_CONTENT_HINT."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid.\n -8: The screen sharing state is invalid. Probably because you have shared other screens or windows. Try calling StopScreenCapture [1/2] to stop the current sharing and start sharing the screen again.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setscreencapturescenario",
    "name": "SetScreenCaptureScenario",
    "description": "Sets the screen sharing scenario.\n\nWhen you start screen sharing or window sharing, you can call this method to set the screen sharing scenario. The SDK adjusts the video quality and experience of the sharing according to the scenario. Agora recommends that you call this method before joining a channel.",
    "parameters": [
      {
        "screenScenario": "The screen sharing scenario. See SCREEN_SCENARIO_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setsubscribeaudioallowlist",
    "name": "SetSubscribeAudioAllowlist",
    "description": "Sets the allowlist of subscriptions for audio streams.\n\nYou can call this method to specify the audio streams of a user that you want to subscribe to.\n If a user is added in the allowlist and blocklist at the same time, only the blocklist takes effect.\n You can call this method either before or after joining a channel.\n The allowlist is not affected by the setting in MuteRemoteAudioStream, MuteAllRemoteAudioStreams and autoSubscribeAudio in ChannelMediaOptions.\n Once the allowlist of subscriptions is set, it is effective even if you leave the current channel and rejoin the channel.",
    "parameters": [
      {
        "uidList": "The user ID list of users that you want to subscribe to. If you want to specify the audio streams of a user for subscription, add the user ID in this list. If you want to remove a user from the allowlist, you need to call the SetSubscribeAudioAllowlist method to update the user ID list; this means you only add the uid of users that you want to subscribe to in the new user ID list."
      },
      {
        "uidNumber": "The number of users in the user ID list."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setsubscribeaudioblocklist",
    "name": "SetSubscribeAudioBlocklist",
    "description": "Set the blocklist of subscriptions for audio streams.\n\nYou can call this method to specify the audio streams of a user that you do not want to subscribe to.\n You can call this method either before or after joining a channel.\n The blocklist is not affected by the setting in MuteRemoteAudioStream, MuteAllRemoteAudioStreams, and autoSubscribeAudio in ChannelMediaOptions.\n Once the blocklist of subscriptions is set, it is effective even if you leave the current channel and rejoin the channel.\n If a user is added in the allowlist and blocklist at the same time, only the blocklist takes effect.",
    "parameters": [
      {
        "uidList": "The user ID list of users that you do not want to subscribe to. If you want to specify the audio streams of a user that you do not want to subscribe to, add the user ID in this list. If you want to remove a user from the blocklist, you need to call the SetSubscribeAudioBlocklist method to update the user ID list; this means you only add the uid of users that you do not want to subscribe to in the new user ID list."
      },
      {
        "uidNumber": "The number of users in the user ID list."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setsubscribevideoallowlist",
    "name": "SetSubscribeVideoAllowlist",
    "description": "Set the allowlist of subscriptions for video streams.\n\nYou can call this method to specify the video streams of a user that you want to subscribe to.\n If a user is added in the allowlist and blocklist at the same time, only the blocklist takes effect.\n Once the allowlist of subscriptions is set, it is effective even if you leave the current channel and rejoin the channel.\n You can call this method either before or after joining a channel.\n The allowlist is not affected by the setting in MuteRemoteVideoStream, MuteAllRemoteVideoStreams and autoSubscribeAudio in ChannelMediaOptions.",
    "parameters": [
      {
        "uidNumber": "The number of users in the user ID list."
      },
      {
        "uidList": "The user ID list of users that you want to subscribe to. If you want to specify the video streams of a user for subscription, add the user ID of that user in this list. If you want to remove a user from the allowlist, you need to call the SetSubscribeVideoAllowlist method to update the user ID list; this means you only add the uid of users that you want to subscribe to in the new user ID list."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setsubscribevideoblocklist",
    "name": "SetSubscribeVideoBlocklist",
    "description": "Set the blocklist of subscriptions for video streams.\n\nYou can call this method to specify the video streams of a user that you do not want to subscribe to.\n If a user is added in the allowlist and blocklist at the same time, only the blocklist takes effect.\n Once the blocklist of subscriptions is set, it is effective even if you leave the current channel and rejoin the channel.\n You can call this method either before or after joining a channel.\n The blocklist is not affected by the setting in MuteRemoteVideoStream, MuteAllRemoteVideoStreams and autoSubscribeAudio in ChannelMediaOptions.",
    "parameters": [
      {
        "uidNumber": "The number of users in the user ID list."
      },
      {
        "uidList": "The user ID list of users that you do not want to subscribe to. If you want to specify the video streams of a user that you do not want to subscribe to, add the user ID of that user in this list. If you want to remove a user from the blocklist, you need to call the SetSubscribeVideoBlocklist method to update the user ID list; this means you only add the uid of users that you do not want to subscribe to in the new user ID list."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setuplocalvideo",
    "name": "SetupLocalVideo",
    "description": "Initializes the local video view.\n\nThis method initializes the video view of a local stream on the local device. It only affects the video seen by the local user and does not impact the publishing of the local video. Call this method to bind the local video stream to a video view (view) and to set the rendering and mirror modes of the video view. The binding remains valid after leaving the channel. To stop rendering or unbind the local video from the view, set view as NULL.\n If you need to implement native window rendering, use this method; if you only need to render video images in your Unity project, use the methods in the VideoSurface class instead.\n To update only the rendering or mirror mode of the local video view during a call, call SetLocalRenderMode [2/2] instead.",
    "parameters": [
      {
        "canvas": "The local video view and settings. See VideoCanvas."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setupremotevideo",
    "name": "SetupRemoteVideo",
    "description": "Initializes the video view of a remote user.\n\nThis method initializes the video view of a remote stream on the local device. It affects only the video view that the local user sees. Call this method to bind the remote video stream to a video view and to set the rendering and mirror modes of the video view. You need to specify the ID of the remote user in this method. If the remote user ID is unknown to the application, set it after the app receives the OnUserJoined callback. To unbind the remote user from the view, set the view parameter to NULL. Once the remote user leaves the channel, the SDK unbinds the remote user. In the scenarios of custom layout for mixed videos on the mobile end, you can call this method and set a separate view for rendering each sub-video stream of the mixed video stream.\n If you need to implement native window rendering, use this method; if you only need to render video images in your Unity project, use the methods in the VideoSurface class instead.\n To update the rendering or mirror mode of the remote video view during a call, use the SetRemoteRenderMode method.\n When using the recording service, the app does not need to bind a view, as it does not send a video stream. If your app does not recognize the recording service, bind the remote user to the view when the SDK triggers the OnFirstRemoteVideoDecoded callback.",
    "parameters": [
      {
        "canvas": "The remote video view and settings. See VideoCanvas."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setvideodenoiseroptions",
    "name": "SetVideoDenoiserOptions",
    "description": "Sets video noise reduction.\n\nYou can call this method to enable the video noise reduction feature and set the options of the video noise reduction effect. If the noise reduction implemented by this method does not meet your needs, Agora recommends that you call the SetBeautyEffectOptions method to enable the beauty and skin smoothing function to achieve better video noise reduction effects. The recommended BeautyOptions settings for intense noise reduction effect are as follows: lighteningContrastLevel LIGHTENING_CONTRAST_NORMAL lighteningLevel : 0.0 smoothnessLevel : 0.5 rednessLevel : 0.0 sharpnessLevel : 0.1",
    "parameters": [
      {
        "type": "The type of the media source to which the filter effect is applied. See MEDIA_SOURCE_TYPE. In this method, this parameter supports only the following two settings:\n Use the default value PRIMARY_CAMERA_SOURCE if you use camera to capture local video.\n Set this parameter to CUSTOM_VIDEO_SOURCE if you use custom video source."
      },
      {
        "enabled": "Whether to enable video noise reduction: true : Enable video noise reduction. false : (Default) Disable video noise reduction."
      },
      {
        "options": "The video noise reduction options. See VideoDenoiserOptions."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setvideoencoderconfiguration",
    "name": "SetVideoEncoderConfiguration",
    "description": "Sets the video encoder configuration.\n\nSets the encoder configuration for the local video. Each configuration profile corresponds to a set of video parameters, including the resolution, frame rate, and bitrate.",
    "parameters": [
      {
        "config": "Video profile. See VideoEncoderConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setvideoscenario",
    "name": "SetVideoScenario",
    "description": "Sets video application scenarios.\n\nAfter successfully calling this method, the SDK will automatically enable the best practice strategies and adjust key performance metrics based on the specified scenario, to optimize the video experience. Call this method before joining a channel.",
    "parameters": [
      {
        "scenarioType": "The type of video application scenario. See VIDEO_APPLICATION_SCENARIO_TYPE. APPLICATION_SCENARIO_MEETING (1) is suitable for meeting scenarios. The SDK automatically enables the following strategies:\n In meeting scenarios where low-quality video streams are required to have a high bitrate, the SDK automatically enables multiple technologies used to deal with network congestions, to enhance the performance of the low-quality streams and to ensure the smooth reception by subscribers.\n The SDK monitors the number of subscribers to the high-quality video stream in real time and dynamically adjusts its configuration based on the number of subscribers.\n If nobody subscribers to the high-quality stream, the SDK automatically reduces its bitrate and frame rate to save upstream bandwidth.\n If someone subscribes to the high-quality stream, the SDK resets the high-quality stream to the VideoEncoderConfiguration configuration used in the most recent calling of SetVideoEncoderConfiguration. If no configuration has been set by the user previously, the following values are used:\n Resolution: (Windows and macOS) 1280 × 720; (Android and iOS) 960 × 540\n Frame rate: 15 fps\n Bitrate: (Windows and macOS) 1600 Kbps; (Android and iOS) 1000 Kbps\n The SDK monitors the number of subscribers to the low-quality video stream in real time and dynamically enables or disables it based on the number of subscribers. If the user has called SetDualStreamMode [2/2] to set that never send low-quality video stream (DISABLE_SIMULCAST_STREAM), the dynamic adjustment of the low-quality stream in meeting scenarios will not take effect.\n If nobody subscribes to the low-quality stream, the SDK automatically disables it to save upstream bandwidth.\n If someone subscribes to the low-quality stream, the SDK enables the low-quality stream and resets it to the SimulcastStreamConfig configuration used in the most recent calling of SetDualStreamMode [2/2]. If no configuration has been set by the user previously, the following values are used:\n Resolution: 480 × 272\n Frame rate: 15 fps\n Bitrate: 500 Kbps APPLICATION_SCENARIO_1V1 (2) This is applicable to the scenario. To meet the requirements for low latency and high-quality video in this scenario, the SDK optimizes its strategies, improving performance in terms of video quality, first frame rendering, latency on mid-to-low-end devices, and smoothness under weak network conditions. APPLICATION_SCENARIO_LIVESHOW (3) This is applicable to the scenario. In this scenario, fast video rendering and high image quality are crucial. The SDK implements several performance optimizations, including automatically enabling accelerated audio and video frame rendering to minimize first-frame latency (no need to call EnableInstantMediaRendering), and B-frame encoding to achieve better image quality and bandwidth efficiency. The SDK also provides enhanced video quality and smooth playback, even in poor network conditions or on lower-end devices."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).\n -4: Video application scenarios are not supported. Possible reasons include that you use the Voice SDK instead of the Video SDK.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setvoicebeautifierparameters",
    "name": "SetVoiceBeautifierParameters",
    "description": "Sets parameters for the preset voice beautifier effects.\n\nTo achieve better vocal effects, it is recommended that you call the following APIs before calling this method:\n Call SetAudioScenario to set the audio scenario to high-quality audio scenario, namely AUDIO_SCENARIO_GAME_STREAMING (3).\n Call SetAudioProfile [2/2] to set the profile parameter to AUDIO_PROFILE_MUSIC_HIGH_QUALITY (4) or AUDIO_PROFILE_MUSIC_HIGH_QUALITY_STEREO (5). Call this method to set a gender characteristic and a reverberation effect for the singing beautifier effect. This method sets parameters for the local user who sends an audio stream. After setting the audio parameters, all users in the channel can hear the effect.\n Do not set the profile parameter in SetAudioProfile [2/2] to AUDIO_PROFILE_SPEECH_STANDARD (1) or AUDIO_PROFILE_IOT (6), or the method does not take effect.\n You can call this method either before or after joining a channel.\n This method has the best effect on human voice processing, and Agora does not recommend calling this method to process audio data containing music.\n After calling SetVoiceBeautifierParameters, Agora does not recommend calling the following methods, otherwise the effect set by SetVoiceBeautifierParameters will be overwritten: SetAudioEffectPreset SetAudioEffectParameters SetVoiceBeautifierPreset SetLocalVoicePitch SetLocalVoiceEqualization SetLocalVoiceReverb SetVoiceConversionPreset\n This method relies on the voice beautifier dynamic library libagora_audio_beauty_extension.dll. If the dynamic library is deleted, the function cannot be enabled normally.",
    "parameters": [
      {
        "preset": "The option for the preset audio effect: SINGING_BEAUTIFIER : The singing beautifier effect."
      },
      {
        "param1": "The gender characteristics options for the singing voice: 1 : A male-sounding voice. 2 : A female-sounding voice."
      },
      {
        "param2": "The reverberation effect options for the singing voice: 1 : The reverberation effect sounds like singing in a small room. 2 : The reverberation effect sounds like singing in a large room. 3 : The reverberation effect sounds like singing in a hall."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setvoicebeautifierpreset",
    "name": "SetVoiceBeautifierPreset",
    "description": "Sets a preset voice beautifier effect.\n\nCall this method to set a preset voice beautifier effect for the local user who sends an audio stream. After setting a voice beautifier effect, all users in the channel can hear the effect. You can set different voice beautifier effects for different scenarios.",
    "parameters": [
      {
        "preset": "The preset voice beautifier effect options: VOICE_BEAUTIFIER_PRESET."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setvoiceconversionpreset",
    "name": "SetVoiceConversionPreset",
    "description": "Sets a preset voice beautifier effect.\n\nCall this method to set a preset voice changing effect for the local user who publishes an audio stream in a channel. After setting the voice changing effect, all users in the channel can hear the effect. You can set different voice changing effects for the user depending on different scenarios.",
    "parameters": [
      {
        "preset": "The options for the preset voice beautifier effects: VOICE_CONVERSION_PRESET."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_setvolumeofeffect",
    "name": "SetVolumeOfEffect",
    "description": "Gets the volume of a specified audio effect file.",
    "parameters": [
      {
        "soundId": "The ID of the audio effect. The ID of each audio effect file is unique."
      },
      {
        "volume": "The playback volume. The value range is [0, 100]. The default value is 100, which represents the original volume."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startaudiomixing",
    "name": "StartAudioMixing [1/2]",
    "description": "Starts playing the music file.\n\nFor the audio file formats supported by this method, see What formats of audio files does the Agora RTC SDK support. If the local music file does not exist, the SDK does not support the file format, or the the SDK cannot access the music file URL, the SDK reports AUDIO_MIXING_REASON_CAN_NOT_OPEN.",
    "parameters": [
      {
        "cycle": "The number of times the music file plays.\n >0: The number of times for playback. For example, 1 represents playing 1 time.\n -1: Play the audio file in an infinite loop."
      },
      {
        "loopback": "Whether to only play music files on the local client: true : Only play music files on the local client so that only the local user can hear the music. false : Publish music files to remote clients so that both the local user and remote users can hear the music."
      },
      {
        "filePath": "The file path. The SDK supports URLs and absolute path of local files. The absolute path needs to be accurate to the file name and extension. Supported audio formats include MP3, AAC, M4A, MP4, WAV, and 3GP. If you have preloaded an audio effect into memory by calling PreloadEffect, ensure that the value of this parameter is the same as that of filePath in PreloadEffect."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).\n -2: The parameter is invalid.\n -3: The SDK is not ready.\n The audio module is disabled.\n The program is not complete.\n The initialization of IRtcEngine fails. Reinitialize the IRtcEngine.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startaudiomixing2",
    "name": "StartAudioMixing [2/2]",
    "description": "Starts playing the music file.\n\nFor the audio file formats supported by this method, see What formats of audio files does the Agora RTC SDK support. If the local music file does not exist, the SDK does not support the file format, or the the SDK cannot access the music file URL, the SDK reports AUDIO_MIXING_REASON_CAN_NOT_OPEN.",
    "parameters": [
      {
        "filePath": "File path:\n Android: The file path, which needs to be accurate to the file name and suffix. Agora supports URL addresses, absolute paths, or file paths that start with /assets/. You might encounter permission issues if you use an absolute path to access a local file, so Agora recommends using a URI address instead. For example : content://com.android.providers.media.documents/document/audio%3A14441\n Windows: The absolute path or URL address (including the suffixes of the filename) of the audio effect file. For example : C:\\music\\audio.mp4.\n iOS or macOS: The absolute path or URL address (including the suffixes of the filename) of the audio effect file. For example: /var/mobile/Containers/Data/audio.mp4."
      },
      {
        "loopback": "Whether to only play music files on the local client: true : Only play music files on the local client so that only the local user can hear the music. false : Publish music files to remote clients so that both the local user and remote users can hear the music."
      },
      {
        "cycle": "The number of times the music file plays.\n >0: The number of times for playback. For example, 1 represents playing 1 time.\n -1: Play the audio file in an infinite loop."
      },
      {
        "startPos": "The playback position (ms) of the music file."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).\n -2: The parameter is invalid.\n -3: The SDK is not ready.\n The audio module is disabled.\n The program is not complete.\n The initialization of IRtcEngine fails. Reinitialize the IRtcEngine.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startaudiorecording",
    "name": "StartAudioRecording [1/3]",
    "description": "Starts audio recording on the client.\n\nThe sample rate of recording is 32 kHz by default and cannot be modified. The Agora SDK allows recording during a call. This method records the audio of all the users in the channel and generates an audio recording file. Supported formats of the recording file are as follows:.wav : Large file size with high fidelity..aac : Small file size with low fidelity. Ensure that the directory for the recording file exists and is writable. This method should be called after the JoinChannel [2/2] method. The recording automatically stops when you call the LeaveChannel [2/2] method.",
    "parameters": [
      {
        "filePath": "The absolute path (including the filename extensions) of the recording file. For example: C:\\music\\audio.aac. Ensure that the directory for the log files exists and is writable."
      },
      {
        "quality": "Audio recording quality. See AUDIO_RECORDING_QUALITY_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startaudiorecording2",
    "name": "StartAudioRecording [2/3]",
    "description": "Starts audio recording on the client and sets the sample rate of recording.\n\nThe Agora SDK allows recording during a call. After successfully calling this method, you can record the audio of all the users in the channel and get an audio recording file. Supported formats of audio files are as follows:\n .wav: Large file size with high fidelity.\n .aac: Small file size with low fidelity.\n Ensure that the directory you use to save the recording file exists and is writable.\n This method should be called after the JoinChannel [2/2] method. The recording automatically stops when you call the LeaveChannel [2/2] method.\n For better recording effects, set quality to AUDIO_RECORDING_QUALITY_MEDIUM or AUDIO_RECORDING_QUALITY_HIGH when sampleRate is 44.1 kHz or 48 kHz.",
    "parameters": [
      {
        "filePath": "The absolute path (including the filename extensions) of the recording file. For example: C:\\music\\audio.aac. Ensure that the directory for the log files exists and is writable."
      },
      {
        "sampleRate": "The sample rate (kHz) of the recording file. Supported values are as follows:\n 16000\n (Default) 32000\n 44100\n 48000"
      },
      {
        "quality": "Recording quality. See AUDIO_RECORDING_QUALITY_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startaudiorecording3",
    "name": "StartAudioRecording [3/3]",
    "description": "Starts audio recording on the client and sets recording configurations.\n\nThe Agora SDK allows recording during a call. After successfully calling this method, you can record the audio of users in the channel and get an audio recording file. Supported formats of audio files are as follows:\n WAV: High-fidelity files with typically larger file sizes. For example, if the sample rate is 32,000 Hz, the file size for 10-minute recording is approximately 73 MB.\n AAC: Low-fidelity files with typically smaller file sizes. For example, if the sample rate is 32,000 Hz and the recording quality is AUDIO_RECORDING_QUALITY_MEDIUM, the file size for 10-minute recording is approximately 2 MB. Once the user leaves the channel, the recording automatically stops.",
    "parameters": [
      {
        "config": "Recording configurations. See AudioFileRecordingConfig."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startcameracapture",
    "name": "StartCameraCapture",
    "description": "Starts camera capture.\n\nYou can call this method to start capturing video from one or more cameras by specifying sourceType. On the iOS platform, if you want to enable multi-camera capture, you need to call EnableMultiCamera and set enabled to true before calling this method.",
    "parameters": [
      {
        "sourceType": "The type of the video source. See VIDEO_SOURCE_TYPE.\n On iOS devices, you can capture video from up to 2 cameras, provided the device has multiple cameras or supports external cameras.\n On Android devices, you can capture video from up to 4 cameras, provided the device has multiple cameras or supports external cameras.\n On the desktop platforms, you can capture video from up to 4 cameras."
      },
      {
        "config": "The configuration of the video capture. See CameraCapturerConfiguration. On the iOS platform, this parameter has no practical function. Use the config parameter in EnableMultiCamera instead to set the video capture configuration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startdirectcdnstreaming",
    "name": "StartDirectCdnStreaming",
    "description": "Starts pushing media streams to the CDN directly.\n\nAogra does not support pushing media streams to one URL repeatedly. Media options Agora does not support setting the value of publishCameraTrack and publishCustomVideoTrack as true, or the value of publishMicrophoneTrack and publishCustomAudioTrack as true at the same time. When choosing media setting options (DirectCdnStreamingMediaOptions), you can refer to the following examples: If you want to push audio and video streams captured by the host from a custom source, the media setting options should be set as follows: publishCustomAudioTrack is set as true and call the PushAudioFrame method publishCustomVideoTrack is set as true and call the PushVideoFrame method publishCameraTrack is set as false (the default value) publishMicrophoneTrack is set as false (the default value) As of v4.2.0, Agora SDK supports audio-only live streaming. You can set publishCustomAudioTrack or publishMicrophoneTrack in DirectCdnStreamingMediaOptions as true and call PushAudioFrame to push audio streams. Agora only supports pushing one audio and video streams or one audio streams to CDN.",
    "parameters": [
      {
        "publishUrl": "The CDN live streaming URL."
      },
      {
        "options": "The media setting options for the host. See DirectCdnStreamingMediaOptions."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startechotest3",
    "name": "StartEchoTest",
    "description": "Starts an audio device loopback test.\n\nTo test whether the user's local sending and receiving streams are normal, you can call this method to perform an audio and video call loop test, which tests whether the audio and video devices and the user's upstream and downstream networks are working properly. After starting the test, the user needs to make a sound or face the camera. The audio or video is output after about two seconds. If the audio playback is normal, the audio device and the user's upstream and downstream networks are working properly; if the video playback is normal, the video device and the user's upstream and downstream networks are working properly.",
    "parameters": [
      {
        "config": "The configuration of the audio and video call loop test. See EchoTestConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startlastmileprobetest",
    "name": "StartLastmileProbeTest",
    "description": "Starts the last mile network probe test.\n\nThis method starts the last-mile network probe test before joining a channel to get the uplink and downlink last mile network statistics, including the bandwidth, packet loss, jitter, and round-trip time (RTT).",
    "parameters": [
      {
        "config": "The configurations of the last-mile network probe test. See LastmileProbeConfig."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startlocalaudiomixer",
    "name": "StartLocalAudioMixer",
    "description": "Starts local audio mixing.\n\nThis method supports merging multiple audio streams into one audio stream locally. For example, merging the audio streams captured from the local microphone, and that from the media player, the sound card, and the remote users into one audio stream, and then publish the merged audio stream to the channel.\n If you want to mix the locally captured audio streams, you can set publishMixedAudioTrack in ChannelMediaOptions to true, and then publish the mixed audio stream to the channel.\n If you want to mix the remote audio stream, ensure that the remote audio stream has been published in the channel and you have subcribed to the audio stream that you need to mix.",
    "parameters": [
      {
        "config": "The configurations for mixing the lcoal audio. See LocalAudioMixerConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startlocalvideotranscoder",
    "name": "StartLocalVideoTranscoder",
    "description": "Starts the local video mixing.\n\nAfter calling this method, you can merge multiple video streams into one video stream locally. For example, you can merge the video streams captured by the camera, screen sharing, media player, remote video, video files, images, etc. into one video stream, and then publish the mixed video stream to the channel.",
    "parameters": [
      {
        "config": "Configuration of the local video mixing, see LocalTranscoderConfiguration.\n The maximum resolution of each video stream participating in the local video mixing is 4096 × 2160. If this limit is exceeded, video mixing does not take effect.\n The maximum resolution of the mixed video stream is 4096 × 2160."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startmediarenderingtracing",
    "name": "StartMediaRenderingTracing",
    "description": "Enables tracing the video frame rendering process.\n\nThe SDK starts tracing the rendering status of the video frames in the channel from the moment this method is successfully called and reports information about the event through the OnVideoRenderingTracingResult callback.\n By default, the SDK starts tracing the video rendering event automatically when the local user successfully joins the channel. You can call this method at an appropriate time according to the actual application scenario to customize the tracing process.\n After the local user leaves the current channel, the SDK automatically resets the time point to the next time when the user successfully joins the channel.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.\n -7: The method is called before IRtcEngine is initialized.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startorupdatechannelmediarelay",
    "name": "StartOrUpdateChannelMediaRelay",
    "description": "Starts relaying media streams across channels or updates channels for media relay.\n\nThe first successful call to this method starts relaying media streams from the source channel to the destination channels. To relay the media stream to other channels, or exit one of the current media relays, you can call this method again to update the destination channels. This feature supports relaying media streams to a maximum of six destination channels. After a successful method call, the SDK triggers the OnChannelMediaRelayStateChanged callback, and this callback returns the state of the media stream relay. Common states are as follows:\n If the OnChannelMediaRelayStateChanged callback returns RELAY_STATE_RUNNING (2) and RELAY_OK (0), it means that the SDK starts relaying media streams from the source channel to the destination channel.\n If the OnChannelMediaRelayStateChanged callback returns RELAY_STATE_FAILURE (3), an exception occurs during the media stream relay.\n Call this method after joining the channel.\n This method takes effect only when you are a host in a live streaming channel.\n The relaying media streams across channels function needs to be enabled by contacting.\n Agora does not support string user accounts in this API.",
    "parameters": [
      {
        "configuration": "The configuration of the media stream relay. See ChannelMediaRelayConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).\n -2: The parameter is invalid.\n -8: Internal state error. Probably because the user is not a broadcaster.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startpreview",
    "name": "StartPreview [1/2]",
    "description": "Enables the local video preview.\n\nYou can call this method to enable local video preview.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startpreview2",
    "name": "StartPreview [2/2]",
    "description": "Enables the local video preview and specifies the video source for the preview.\n\nThis method is used to start local video preview and specify the video source that appears in the preview screen.",
    "parameters": [
      {
        "sourceType": "The type of the video source. See VIDEO_SOURCE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startrhythmplayer",
    "name": "StartRhythmPlayer",
    "description": "Enables the virtual metronome.\n\nAfter enabling the virtual metronome, the SDK plays the specified audio effect file from the beginning, and controls the playback duration of each file according to beatsPerMinute you set in AgoraRhythmPlayerConfig. For example, if you set beatsPerMinute as 60, the SDK plays one beat every second. If the file duration exceeds the beat duration, the SDK only plays the audio within the beat duration.\n By default, the sound of the virtual metronome is published in the channel. If you want the sound to be heard by the remote users, you can set publishRhythmPlayerTrack in ChannelMediaOptions as true.",
    "parameters": [
      {
        "sound1": "The absolute path or URL address (including the filename extensions) of the file for the downbeat. For example, C:\\music\\audio.mp4. For the audio file formats supported by this method, see What formats of audio files does the Agora RTC SDK support."
      },
      {
        "sound2": "The absolute path or URL address (including the filename extensions) of the file for the upbeats. For example, C:\\music\\audio.mp4. For the audio file formats supported by this method, see What formats of audio files does the Agora RTC SDK support."
      },
      {
        "config": "The metronome configuration. See AgoraRhythmPlayerConfig."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -22: Cannot find audio effect files. Please set the correct paths for sound1 and sound2.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startrtmpstreamwithouttranscoding",
    "name": "StartRtmpStreamWithoutTranscoding",
    "description": "Starts pushing media streams to a CDN without transcoding.\n\nCall this method after joining a channel.\n Only hosts in the LIVE_BROADCASTING profile can call this method.\n If you want to retry pushing streams after a failed push, make sure to call StopRtmpStream first, then call this method to retry pushing streams; otherwise, the SDK returns the same error code as the last failed push. Agora recommends that you use the server-side Media Push function. You can call this method to push an audio or video stream to the specified CDN address. This method can push media streams to only one CDN address at a time, so if you need to push streams to multiple addresses, call this method multiple times. After you call this method, the SDK triggers the OnRtmpStreamingStateChanged callback on the local client to report the state of the streaming.",
    "parameters": [
      {
        "url": "The address of Media Push. The format is RTMP or RTMPS. The character length cannot exceed 1024 bytes. Special characters such as Chinese characters are not supported."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The URL or configuration of transcoding is invalid; check your URL and transcoding configurations.\n -7: The SDK is not initialized before calling this method.\n -19: The Media Push URL is already in use; use another URL instead.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startrtmpstreamwithtranscoding",
    "name": "StartRtmpStreamWithTranscoding",
    "description": "Starts Media Push and sets the transcoding configuration.\n\nAgora recommends that you use the server-side Media Push function. You can call this method to push a live audio-and-video stream to the specified CDN address and set the transcoding configuration. This method can push media streams to only one CDN address at a time, so if you need to push streams to multiple addresses, call this method multiple times. Under one Agora project, the maximum number of concurrent tasks to push media streams is 200 by default. If you need a higher quota, contact. After you call this method, the SDK triggers the OnRtmpStreamingStateChanged callback on the local client to report the state of the streaming.\n Call this method after joining a channel.\n Only hosts in the LIVE_BROADCASTING profile can call this method.\n If you want to retry pushing streams after a failed push, make sure to call StopRtmpStream first, then call this method to retry pushing streams; otherwise, the SDK returns the same error code as the last failed push.",
    "parameters": [
      {
        "url": "The address of Media Push. The format is RTMP or RTMPS. The character length cannot exceed 1024 bytes. Special characters such as Chinese characters are not supported."
      },
      {
        "transcoding": "The transcoding configuration for Media Push. See LiveTranscoding."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The URL or configuration of transcoding is invalid; check your URL and transcoding configurations.\n -7: The SDK is not initialized before calling this method.\n -19: The Media Push URL is already in use; use another URL instead.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startscreencapture",
    "name": "StartScreenCapture [1/2]",
    "description": "Starts screen capture.\n\nThis method is for Android and iOS only.\n The billing for the screen sharing stream is based on the dimensions in ScreenVideoParameters :\n When you do not pass in a value, Agora bills you at 1280 × 720.\n When you pass in a value, Agora bills you at that value.",
    "parameters": [
      {
        "captureParams": "The screen sharing encoding parameters. See ScreenCaptureParameters2."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2 (iOS platform): Empty parameter.\n -2 (Android platform): The system version is too low. Ensure that the Android API level is not lower than 21.\n -3 (Android platform): Unable to capture system audio. Ensure that the Android API level is not lower than 29.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startscreencapture2",
    "name": "StartScreenCapture [2/2]",
    "description": "Starts screen capture from the specified video source.\n\nThis method applies to the macOS and Windows only.",
    "parameters": [
      {
        "sourceType": "The type of the video source. See VIDEO_SOURCE_TYPE. On the macOS platform, this parameter can only be set to VIDEO_SOURCE_SCREEN (2)."
      },
      {
        "config": "The configuration of the captured screen. See ScreenCaptureConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startscreencapturebydisplayid",
    "name": "StartScreenCaptureByDisplayId",
    "description": "Captures the screen by specifying the display ID.\n\nCaptures the video stream of a screen or a part of the screen area. This method is for Windows and macOS only.",
    "parameters": [
      {
        "displayId": "The display ID of the screen to be shared. For the Windows platform, if you need to simultaneously share two screens (main screen and secondary screen), you can set displayId to -1 when calling this method."
      },
      {
        "regionRect": "(Optional) Sets the relative location of the region to the screen. Pass in nil to share the entire screen. See Rectangle."
      },
      {
        "captureParams": "Screen sharing configurations. The default video dimension is 1920 x 1080, that is, 2,073,600 pixels. Agora uses the value of this parameter to calculate the charges. See ScreenCaptureParameters. The video properties of the screen sharing stream only need to be set through this parameter, and are unrelated to SetVideoEncoderConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid.\n -8: The screen sharing state is invalid. Probably because you have shared other screens or windows. Try calling StopScreenCapture [1/2] to stop the current sharing and start sharing the screen again.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startscreencapturebyscreenrect",
    "name": "StartScreenCaptureByScreenRect",
    "description": "Captures the whole or part of a screen by specifying the screen rect.\n\nYou can call this method either before or after joining the channel, with the following differences:\n Call this method before joining a channel, and then call JoinChannel [2/2] to join a channel and set publishScreenTrack or publishSecondaryScreenTrack to true to start screen sharing.\n Call this method after joining a channel, and then call UpdateChannelMediaOptions to join a channel and set publishScreenTrack or publishSecondaryScreenTrack to true to start screen sharing. Deprecated: This method is deprecated. Use StartScreenCaptureByDisplayId instead. Agora strongly recommends using StartScreenCaptureByDisplayId if you need to start screen sharing on a device connected to another display. This method shares a screen or part of the screen. You need to specify the area of the screen to be shared. This method applies to Windows only.",
    "parameters": [
      {
        "screenRect": "Sets the relative location of the screen to the virtual screen."
      },
      {
        "regionRect": "(Optional) Sets the relative location of the region to the screen. If you do not set this parameter, the SDK shares the whole screen. See Rectangle. If the specified region overruns the screen, the SDK shares only the region within it; if you set width or height as 0, the SDK shares the whole screen."
      },
      {
        "captureParams": "The screen sharing encoding parameters. The default video resolution is 1920 × 1080, that is, 2,073,600 pixels. Agora uses the value of this parameter to calculate the charges. See ScreenCaptureParameters."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid.\n -8: The screen sharing state is invalid. Probably because you have shared other screens or windows. Try calling StopScreenCapture [1/2] to stop the current sharing and start sharing the screen again.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_startscreencapturebywindowid",
    "name": "StartScreenCaptureByWindowId",
    "description": "Captures the whole or part of a window by specifying the window ID.\n\nThis method captures a window or part of the window. You need to specify the ID of the window to be captured. This method applies to the macOS and Windows only. This method supports window sharing of UWP (Universal Windows Platform) applications. Agora tests the mainstream UWP applications by using the lastest SDK, see details as follows:",
    "parameters": [
      {
        "windowId": "The ID of the window to be shared."
      },
      {
        "regionRect": "(Optional) Sets the relative location of the region to the screen. If you do not set this parameter, the SDK shares the whole screen. See Rectangle. If the specified region overruns the window, the SDK shares only the region within it; if you set width or height as 0, the SDK shares the whole window."
      },
      {
        "captureParams": "Screen sharing configurations. The default video resolution is 1920 × 1080, that is, 2,073,600 pixels. Agora uses the value of this parameter to calculate the charges. See ScreenCaptureParameters."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid.\n -8: The screen sharing state is invalid. Probably because you have shared other screens or windows. Try calling StopScreenCapture [1/2] to stop the current sharing and start sharing the screen again.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stopalleffects",
    "name": "StopAllEffects",
    "description": "Stops playing all audio effects.\n\nWhen you no longer need to play the audio effect, you can call this method to stop the playback. If you only need to pause the playback, call PauseAllEffects.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stopaudiomixing",
    "name": "StopAudioMixing",
    "description": "Stops playing the music file.\n\nAfter calling StartAudioMixing [2/2] to play a music file, you can call this method to stop the playing. If you only need to pause the playback, call PauseAudioMixing.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stopaudiorecording",
    "name": "StopAudioRecording",
    "description": "Stops the audio recording on the client.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stopcameracapture",
    "name": "StopCameraCapture",
    "description": "Stops camera capture.\n\nAfter calling StartCameraCapture to start capturing video through one or more cameras, you can call this method and set the sourceType parameter to stop the capture from the specified cameras. On the iOS platform, if you want to disable multi-camera capture, you need to call EnableMultiCamera after calling this method and set enabled to false. If you are using the local video mixing function, calling this method can cause the local video mixing to be interrupted.",
    "parameters": [
      {
        "sourceType": "The type of the video source. See VIDEO_SOURCE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stopchannelmediarelay",
    "name": "StopChannelMediaRelay",
    "description": "Stops the media stream relay. Once the relay stops, the host quits all the target channels.\n\nAfter a successful method call, the SDK triggers the OnChannelMediaRelayStateChanged callback. If the callback reports RELAY_STATE_IDLE (0) and RELAY_OK (0), the host successfully stops the relay. If the method call fails, the SDK triggers the OnChannelMediaRelayStateChanged callback with the RELAY_ERROR_SERVER_NO_RESPONSE (2) or RELAY_ERROR_SERVER_CONNECTION_LOST (8) status code. You can call the LeaveChannel [2/2] method to leave the channel, and the media stream relay automatically stops.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.\n -5: The method call was rejected. There is no ongoing channel media relay.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stopdirectcdnstreaming",
    "name": "StopDirectCdnStreaming",
    "description": "Stops pushing media streams to the CDN directly.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stopechotest",
    "name": "StopEchoTest",
    "description": "Stops the audio call test.\n\nAfter calling StartEchoTest, you must call this method to end the test; otherwise, the user cannot perform the next audio and video call loop test and cannot join the channel.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.\n -5(ERR_REFUSED): Failed to stop the echo test. The echo test may not be running.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stopeffect",
    "name": "StopEffect",
    "description": "Stops playing a specified audio effect.\n\nWhen you no longer need to play the audio effect, you can call this method to stop the playback. If you only need to pause the playback, call PauseEffect.",
    "parameters": [
      {
        "soundId": "The ID of the audio effect. Each audio effect has a unique ID."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stoplastmileprobetest",
    "name": "StopLastmileProbeTest",
    "description": "Stops the last mile network probe test.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stoplocalaudiomixer",
    "name": "StopLocalAudioMixer",
    "description": "Stops the local audio mixing.\n\nAfter calling StartLocalAudioMixer, call this method if you want to stop the local audio mixing.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stoplocalvideotranscoder",
    "name": "StopLocalVideoTranscoder",
    "description": "Stops the local video mixing.\n\nAfter calling StartLocalVideoTranscoder, call this method if you want to stop the local video mixing.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stoppreview",
    "name": "StopPreview [1/2]",
    "description": "Stops the local video preview.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stoppreview2",
    "name": "StopPreview [2/2]",
    "description": "Stops the local video preview.",
    "parameters": [
      {
        "sourceType": "The type of the video source. See VIDEO_SOURCE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stoprhythmplayer",
    "name": "StopRhythmPlayer",
    "description": "Disables the virtual metronome.\n\nAfter calling StartRhythmPlayer, you can call this method to disable the virtual metronome. This method is for Android and iOS only.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stoprtmpstream",
    "name": "StopRtmpStream",
    "description": "Stops pushing media streams to a CDN.\n\nAgora recommends that you use the server-side Media Push function. You can call this method to stop the live stream on the specified CDN address. This method can stop pushing media streams to only one CDN address at a time, so if you need to stop pushing streams to multiple addresses, call this method multiple times. After you call this method, the SDK triggers the OnRtmpStreamingStateChanged callback on the local client to report the state of the streaming.",
    "parameters": [
      {
        "url": "The address of Media Push. The format is RTMP or RTMPS. The character length cannot exceed 1024 bytes. Special characters such as Chinese characters are not supported."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stopscreencapture",
    "name": "StopScreenCapture [1/2]",
    "description": "Stops screen capture.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_stopscreencapture2",
    "name": "StopScreenCapture [2/2]",
    "description": "Stops screen capture from the specified video source.\n\nThis method applies to the macOS and Windows only.",
    "parameters": [
      {
        "sourceType": "The type of the video source. See VIDEO_SOURCE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_switchcamera",
    "name": "SwitchCamera",
    "description": "Switches between front and rear cameras.\n\nYou can call this method to dynamically switch cameras based on the actual camera availability during the app's runtime, without having to restart the video stream or reconfigure the video source. This method is for Android and iOS only.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_takesnapshot",
    "name": "TakeSnapshot [1/2]",
    "description": "Takes a snapshot of a video stream.\n\nThis method takes a snapshot of a video stream from the specified user, generates a JPG image, and saves it to the specified path.",
    "parameters": [
      {
        "uid": "The user ID. Set uid as 0 if you want to take a snapshot of the local user's video."
      },
      {
        "filePath": "The local path (including filename extensions) of the snapshot. For example:\n Windows: C:\\Users\\<user_name>\\AppData\\Local\\Agora\\<process_name>\\example.jpg\n iOS: /App Sandbox/Library/Caches/example.jpg\n macOS: ～/Library/Logs/example.jpg\n Android: /storage/emulated/0/Android/data/<package name>/files/example.jpg Ensure that the path you specify exists and is writable."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_takesnapshot2",
    "name": "TakeSnapshot [2/2]",
    "description": "Takes a screenshot of the video at the specified observation point.\n\nThis method takes a snapshot of a video stream from the specified user, generates a JPG image, and saves it to the specified path.",
    "parameters": [
      {
        "uid": "The user ID. Set uid as 0 if you want to take a snapshot of the local user's video."
      },
      {
        "config": "The configuration of the snaptshot. See SnapshotConfig."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_unloadalleffects",
    "name": "UnloadAllEffects",
    "description": "Releases a specified preloaded audio effect from the memory.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_unloadeffect",
    "name": "UnloadEffect",
    "description": "Releases a specified preloaded audio effect from the memory.\n\nAfter loading the audio effect file into memory using PreloadEffect, if you need to release the audio effect file, call this method.",
    "parameters": [
      {
        "soundId": "The ID of the audio effect. Each audio effect has a unique ID."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_unregisteraudioencodedframeobserver",
    "name": "UnRegisterAudioEncodedFrameObserver",
    "description": "Unregisters the encoded audio frame observer.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_unregisteraudioframeobserver",
    "name": "UnRegisterAudioFrameObserver",
    "description": "Unregisters an audio frame observer.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_unregisteraudiospectrumobserver",
    "name": "UnregisterAudioSpectrumObserver",
    "description": "Unregisters the audio spectrum observer.\n\nAfter calling RegisterAudioSpectrumObserver, if you want to disable audio spectrum monitoring, you can call this method. You can call this method either before or after joining a channel.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_unregistermediametadataobserver",
    "name": "UnregisterMediaMetadataObserver",
    "description": "Unregisters the specified metadata observer.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_updatechannelmediaoptions",
    "name": "UpdateChannelMediaOptions",
    "description": "Updates the channel media options after joining the channel.",
    "parameters": [
      {
        "options": "The channel media options. See ChannelMediaOptions."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The value of a member in ChannelMediaOptions is invalid. For example, the token or the user ID is invalid. You need to fill in a valid parameter.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.\n -8: The internal state of the IRtcEngine object is wrong. The possible reason is that the user is not in the channel. Agora recommends that you use the OnConnectionStateChanged callback to see whether the user is in the channel. If you receive the CONNECTION_STATE_DISCONNECTED (1) or CONNECTION_STATE_FAILED (5) state, the user is not in the channel. You need to call JoinChannel [2/2] to join a channel before calling this method.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_updatedirectcdnstreamingmediaoptions",
    "name": "updateDirectCdnStreamingMediaOptions",
    "description": "",
    "parameters": [
      {
        "null": ""
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "api_irtcengine_updatelocalaudiomixerconfiguration",
    "name": "UpdateLocalAudioMixerConfiguration",
    "description": "Updates the configurations for mixing audio streams locally.\n\nAfter calling StartLocalAudioMixer, call this method if you want to update the local audio mixing configuration.",
    "parameters": [
      {
        "config": "The configurations for mixing the lcoal audio. See LocalAudioMixerConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_updatelocaltranscoderconfiguration",
    "name": "UpdateLocalTranscoderConfiguration",
    "description": "Updates the local video mixing configuration.\n\nAfter calling StartLocalVideoTranscoder, call this method if you want to update the local video mixing configuration. If you want to update the video source type used for local video mixing, such as adding a second camera or screen to capture video, you need to call this method after StartCameraCapture or StartScreenCapture [2/2].",
    "parameters": [
      {
        "config": "Configuration of the local video mixing, see LocalTranscoderConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_updatepreloadchanneltoken",
    "name": "UpdatePreloadChannelToken",
    "description": "Updates the wildcard token for preloading channels.\n\nYou need to maintain the life cycle of the wildcard token by yourself. When the token expires, you need to generate a new wildcard token and then call this method to pass in the new token.",
    "parameters": [
      {
        "token": "The new token."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid. For example, the token is invalid. You need to pass in a valid parameter and join the channel again.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_updatertmptranscoding",
    "name": "UpdateRtmpTranscoding",
    "description": "Updates the transcoding configuration.\n\nAgora recommends that you use the server-side Media Push function. After you start pushing media streams to CDN with transcoding, you can dynamically update the transcoding configuration according to the scenario. The SDK triggers the OnTranscodingUpdated callback after the transcoding configuration is updated.",
    "parameters": [
      {
        "transcoding": "The transcoding configuration for Media Push. See LiveTranscoding."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_updatescreencapture",
    "name": "UpdateScreenCapture",
    "description": "Updates the screen capturing parameters.\n\nIf the system audio is not captured when screen sharing is enabled, and then you want to update the parameter configuration and publish the system audio, you can refer to the following steps:\n Call this method, and set captureAudio to true.\n Call UpdateChannelMediaOptions, and set publishScreenCaptureAudio to true to publish the audio captured by the screen.\n This method is for Android and iOS only.\n On the iOS platform, screen sharing is only available on iOS 12.0 and later.",
    "parameters": [
      {
        "captureParams": "The screen sharing encoding parameters. See ScreenCaptureParameters2."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid.\n -8: The screen sharing state is invalid. Probably because you have shared other screens or windows. Try calling StopScreenCapture [1/2] to stop the current sharing and start sharing the screen again.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_updatescreencaptureparameters",
    "name": "UpdateScreenCaptureParameters",
    "description": "Updates the screen capturing parameters.\n\nThis method is for Windows and macOS only.\n Call this method after starting screen sharing or window sharing.",
    "parameters": [
      {
        "captureParams": "The screen sharing encoding parameters. See ScreenCaptureParameters. The video properties of the screen sharing stream only need to be set through this parameter, and are unrelated to SetVideoEncoderConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid.\n -8: The screen sharing state is invalid. Probably because you have shared other screens or windows. Try calling StopScreenCapture [1/2] to stop the current sharing and start sharing the screen again.",
    "is_hide": false
  },
  {
    "id": "api_irtcengine_updatescreencaptureregion",
    "name": "UpdateScreenCaptureRegion",
    "description": "Updates the screen capturing region.\n\nCall this method after starting screen sharing or window sharing.",
    "parameters": [
      {
        "regionRect": "The relative location of the screen-share area to the screen or window. If you do not set this parameter, the SDK shares the whole screen or window. See Rectangle. If the specified region overruns the screen or window, the SDK shares only the region within it; if you set width or height as 0, the SDK shares the whole screen or window."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid.\n -8: The screen sharing state is invalid. Probably because you have shared other screens or windows. Try calling StopScreenCapture [1/2] to stop the current sharing and start sharing the screen again.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_addvideowatermarkex",
    "name": "AddVideoWatermarkEx",
    "description": "Adds a watermark image to the local video.\n\nThis method adds a PNG watermark image to the local video in the live streaming. Once the watermark image is added, all the audience in the channel (CDN audience included), and the capturing device can see and capture it. The Agora SDK supports adding only one watermark image onto a local video or CDN live stream. The newly added watermark image replaces the previous one. The watermark coordinates are dependent on the settings in the SetVideoEncoderConfigurationEx method:\n If the orientation mode of the encoding video (ORIENTATION_MODE) is fixed landscape mode or the adaptive landscape mode, the watermark uses the landscape orientation.\n If the orientation mode of the encoding video (ORIENTATION_MODE) is fixed portrait mode or the adaptive portrait mode, the watermark uses the portrait orientation.\n When setting the watermark position, the region must be less than the dimensions set in the SetVideoEncoderConfigurationEx method; otherwise, the watermark image will be cropped.\n Ensure that you have called EnableVideo before calling this method.\n This method supports adding a watermark image in the PNG file format only. Supported pixel formats of the PNG image are RGBA, RGB, Palette, Gray, and Alpha_gray.\n If the dimensions of the PNG image differ from your settings in this method, the image will be cropped or zoomed to conform to your settings.\n If you have enabled the local video preview by calling the StartPreview [2/2] method, you can use the visibleInPreview member to set whether or not the watermark is visible in the preview.\n If you have enabled the mirror mode for the local video, the watermark on the local video is also mirrored. To avoid mirroring the watermark, Agora recommends that you do not use the mirror and watermark functions for the local video at the same time. You can implement the watermark function in your application layer.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "options": "The options of the watermark image to be added. See WatermarkOptions."
      },
      {
        "watermarkUrl": "The local file path of the watermark image to be added. This method supports adding a watermark image from the local absolute or relative file path."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_adjustuserplaybacksignalvolumeex",
    "name": "AdjustUserPlaybackSignalVolumeEx",
    "description": "Adjusts the playback signal volume of a specified remote user.\n\nYou can call this method to adjust the playback volume of a specified remote user. To adjust the playback volume of different remote users, call the method as many times, once for each remote user.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "volume": "The volume of the user. The value range is [0,400].\n 0: Mute.\n 100: (Default) The original volume.\n 400: Four times the original volume (amplifying the audio signals by four times)."
      },
      {
        "uid": "The user ID of the remote user."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_clearvideowatermarkex",
    "name": "ClearVideoWatermarkEx",
    "description": "Removes the watermark image from the video stream.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_createdatastreamex",
    "name": "CreateDataStreamEx [1/2]",
    "description": "Creates a data stream.\n\nYou can call this method to create a data stream and improve the reliability and ordering of data transmission. Deprecated: This method is deprecated. Use CreateDataStreamEx [2/2] instead.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "ordered": "Sets whether the recipients receive the data stream in the sent order: true : The recipients receive the data in the sent order. false : The recipients do not receive the data in the sent order."
      },
      {
        "reliable": "Sets whether the recipients are guaranteed to receive the data stream within five seconds: true : The recipients receive the data from the sender within five seconds. If the recipient does not receive the data within five seconds, the SDK triggers the OnStreamMessageError callback and returns an error code. false : There is no guarantee that the recipients receive the data stream within five seconds and no error message is reported for any delay or missing data stream. Please ensure that reliable and ordered are either both set to true or both set to false."
      },
      {
        "streamId": "An output parameter; the ID of the data stream created."
      }
    ],
    "returns": "0: The data stream is successfully created.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_createdatastreamex2",
    "name": "CreateDataStreamEx [2/2]",
    "description": "Creates a data stream.\n\nCompared to CreateDataStreamEx [1/2], this method does not guarantee the reliability of data transmission. If a data packet is not received five seconds after it was sent, the SDK directly discards the data.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "config": "The configurations for the data stream. See DataStreamConfig."
      },
      {
        "streamId": "An output parameter; the ID of the data stream created."
      }
    ],
    "returns": "0: The data stream is successfully created.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_enableaudiovolumeindicationex",
    "name": "EnableAudioVolumeIndicationEx",
    "description": "Enables the reporting of users' volume indication.\n\nThis method enables the SDK to regularly report the volume information to the app of the local user who sends a stream and remote users (three users at most) whose instantaneous volumes are the highest.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "reportVad": "true : Enables the voice activity detection of the local user. Once it is enabled, the vad parameter of the OnAudioVolumeIndication callback reports the voice activity status of the local user. false : (Default) Disables the voice activity detection of the local user. Once it is disabled, the vad parameter of the OnAudioVolumeIndication callback does not report the voice activity status of the local user, except for the scenario where the engine automatically detects the voice activity of the local user."
      },
      {
        "smooth": "The smoothing factor that sets the sensitivity of the audio volume indicator. The value ranges between 0 and 10. The recommended value is 3. The greater the value, the more sensitive the indicator."
      },
      {
        "interval": "Sets the time interval between two consecutive volume indications:\n ≤ 0: Disables the volume indication.\n > 0: Time interval (ms) between two consecutive volume indications. Ensure this parameter is set to a value greater than 10, otherwise you will not receive the OnAudioVolumeIndication callback. Agora recommends that this value is set as greater than 100."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_enablecontentinspectex",
    "name": "EnableContentInspectEx",
    "description": "Enables or disables video screenshot and upload.\n\nThis method can take screenshots for multiple video streams and upload them. When video screenshot and upload function is enabled, the SDK takes screenshots and uploads videos sent by local users based on the type and frequency of the module you set in ContentInspectConfig. After video screenshot and upload, the Agora server sends the callback notification to your app server in HTTPS requests and sends all screenshots to the third-party cloud storage service.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "config": "Screenshot and upload configuration. See ContentInspectConfig."
      },
      {
        "enabled": "Whether to enalbe video screenshot and upload: true : Enables video screenshot and upload. false : Disables video screenshot and upload."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_enabledualstreammodeex",
    "name": "EnableDualStreamModeEx",
    "description": "Enables or disables dual-stream mode on the sender side.\n\nAfter you enable dual-stream mode, you can call SetRemoteVideoStreamType to choose to receive either the high-quality video stream or the low-quality video stream on the subscriber side. You can call this method to enable or disable the dual-stream mode on the publisher side. Dual streams are a pairing of a high-quality video stream and a low-quality video stream:\n High-quality video stream: High bitrate, high resolution.\n Low-quality video stream: Low bitrate, low resolution. Deprecated: This method is deprecated as of v4.2.0. Use SetDualStreamModeEx instead. This method is applicable to all types of streams from the sender, including but not limited to video streams collected from cameras, screen sharing streams, and custom-collected video streams.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "streamConfig": "The configuration of the low-quality video stream. See SimulcastStreamConfig. When setting mode to DISABLE_SIMULCAST_STREAM, setting streamConfig will not take effect."
      },
      {
        "enabled": "Whether to enable dual-stream mode: true : Enable dual-stream mode. false : (Default) Disable dual-stream mode."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_enableencryptionex",
    "name": "EnableEncryptionEx",
    "description": "Enables or disables the built-in encryption.\n\nAfter the user leaves the channel, the SDK automatically disables the built-in encryption. To enable the built-in encryption, call this method before the user joins the channel again.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "config": "Built-in encryption configurations. See EncryptionConfig."
      },
      {
        "enabled": "Whether to enable built-in encryption: true : Enable the built-in encryption. false : (Default) Disable the built-in encryption."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_enableloopbackrecordingex",
    "name": "EnableLoopbackRecordingEx",
    "description": "Enables loopback audio capturing.\n\nIf you enable loopback audio capturing, the output of the sound card is mixed into the audio stream sent to the other end.\n This method applies to the macOS and Windows only.\n macOS does not support loopback audio capture of the default sound card. If you need to use this function, use a virtual sound card and pass its name to the deviceName parameter. Agora recommends using AgoraALD as the virtual sound card for audio capturing.\n This method only supports using one sound card for audio capturing.",
    "parameters": [
      {
        "deviceName": "macOS: The device name of the virtual sound card. The default value is set to NULL, which means using AgoraALD for loopback audio capturing.\n Windows: The device name of the sound card. The default is set to NULL, which means the SDK uses the sound card of your device for loopback audio capturing."
      },
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "enabled": "Sets whether to enable loopback audio capture: true : Enable loopback audio capturing. false : (Default) Disable loopback audio capturing."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_getcallidex",
    "name": "GetCallIdEx",
    "description": "Gets the call ID with the connection ID.\n\nWhen a user joins a channel on a client, a callId is generated to identify the call from the client. You can call this method to get callId, and pass it in when calling methods such as Rate and Complain.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "callId": "Output parameter, the current call ID."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_getconnectionstateex",
    "name": "GetConnectionStateEx",
    "description": "Gets the current connection state of the SDK.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "The current connection state. See CONNECTION_STATE_TYPE.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_joinchannelex",
    "name": "JoinChannelEx",
    "description": "Joins a channel.\n\nYou can call this method multiple times to join more than one channel. If you want to join the same channel from different devices, ensure that the user IDs are different for all devices.",
    "parameters": [
      {
        "options": "The channel media options. See ChannelMediaOptions."
      },
      {
        "token": "The token generated on your server for authentication.\n (Recommended) If your project has enabled the security mode (using APP ID and Token for authentication), this parameter is required.\n If you have only enabled the testing mode (using APP ID for authentication), this parameter is optional. You will automatically exit the channel 24 hours after successfully joining in.\n If you need to join different channels at the same time or switch between channels, Agora recommends using a wildcard token so that you don't need to apply for a new token every time joining a channel."
      },
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The parameter is invalid. For example, the token is invalid, the uid parameter is not set to an integer, or the value of a member in ChannelMediaOptions is invalid. You need to pass in a valid parameter and join the channel again.\n -3: Fails to initialize the IRtcEngine object. You need to reinitialize the IRtcEngine object.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.\n -8: The internal state of the IRtcEngine object is wrong. The typical cause is that after calling StartEchoTest to start a call loop test, you call this method to join the channel without calling StopEchoTest to stop the test. You need to call StopEchoTest before calling this method.\n -17: The request to join the channel is rejected. The typical cause is that the user is already in the channel. Agora recommends that you use the OnConnectionStateChanged callback to see whether the user is in the channel. Do not call this method to join the channel unless you receive the CONNECTION_STATE_DISCONNECTED (1) state.\n -102: The channel name is invalid. You need to pass in a valid channel name in channelId to rejoin the channel.\n -121: The user ID is invalid. You need to pass in a valid user ID in uid to rejoin the channel.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_leavechannelex",
    "name": "LeaveChannelEx [1/2]",
    "description": "Leaves a channel.\n\nAfter calling this method, the SDK terminates the audio and video interaction, leaves the current channel, and releases all resources related to the session. After calling JoinChannelEx to join a channel, you must call this method or LeaveChannelEx [2/2] to end the call, otherwise, the next call cannot be started.\n This method call is asynchronous. When this method returns, it does not necessarily mean that the user has left the channel.\n If you call LeaveChannel [1/2] or LeaveChannel [2/2], you will leave all the channels you have joined by calling JoinChannel [1/2], JoinChannel [2/2], or JoinChannelEx.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_leavechannelex2",
    "name": "LeaveChannelEx [2/2]",
    "description": "Sets channel options and leaves the channel.\n\nAfter calling this method, the SDK terminates the audio and video interaction, leaves the current channel, and releases all resources related to the session. After calling JoinChannelEx to join a channel, you must call this method or LeaveChannelEx [1/2] to end the call, otherwise, the next call cannot be started.\n This method call is asynchronous. When this method returns, it does not necessarily mean that the user has left the channel.\n If you call LeaveChannel [1/2] or LeaveChannel [2/2], you will leave all the channels you have joined by calling JoinChannel [1/2], JoinChannel [2/2], or JoinChannelEx.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "options": "The options for leaving the channel. See LeaveChannelOptions. This parameter only supports the stopMicrophoneRecording member in the LeaveChannelOptions settings; setting other members does not take effect."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_muteallremoteaudiostreamsex",
    "name": "MuteAllRemoteAudioStreamsEx",
    "description": "Stops or resumes subscribing to the audio streams of all remote users.\n\nAfter successfully calling this method, the local user stops or resumes subscribing to the audio streams of all remote users, including the ones join the channel subsequent to this call.\n Call this method after joining a channel.\n If you do not want to subscribe the audio streams of remote users before joining a channel, you can set autoSubscribeAudio as false when calling JoinChannel [2/2].",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "mute": "Whether to stop subscribing to the audio streams of all remote users: true : Stops subscribing to the audio streams of all remote users. false : (Default) Subscribes to the audio streams of all remote users by default."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_muteallremotevideostreamsex",
    "name": "MuteAllRemoteVideoStreamsEx",
    "description": "Stops or resumes subscribing to the video streams of all remote users.\n\nAfter successfully calling this method, the local user stops or resumes subscribing to the video streams of all remote users, including all subsequent users.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "mute": "Whether to stop subscribing to the video streams of all remote users. true : Stop subscribing to the video streams of all remote users. false : (Default) Subscribe to the video streams of all remote users by default."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_mutelocalaudiostreamex",
    "name": "MuteLocalAudioStreamEx",
    "description": "Stops or resumes publishing the local audio stream.\n\nThis method does not affect any ongoing audio recording, because it does not disable the audio capture device. A successful call of this method triggers the OnUserMuteAudio and OnRemoteAudioStateChanged callbacks on the remote client.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "mute": "Whether to stop publishing the local audio stream: true : Stops publishing the local audio stream. false : (Default) Resumes publishing the local audio stream."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_mutelocalvideostreamex",
    "name": "MuteLocalVideoStreamEx",
    "description": "Stops or resumes publishing the local video stream.\n\nA successful call of this method triggers the OnUserMuteVideo callback on the remote client.\n This method does not affect any ongoing video recording, because it does not disable the camera.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "mute": "Whether to stop publishing the local video stream. true : Stop publishing the local video stream. false : (Default) Publish the local video stream."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_muteremoteaudiostreamex",
    "name": "MuteRemoteAudioStreamEx",
    "description": "Stops or resumes receiving the audio stream of a specified user.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "uid": "The ID of the specified user."
      },
      {
        "mute": "Whether to stop receiving the audio stream of the specified user: true : Stop receiving the audio stream of the specified user. false : (Default) Resume receiving the audio stream of the specified user."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_muteremotevideostreamex",
    "name": "MuteRemoteVideoStreamEx",
    "description": "Stops or resumes receiving the video stream of a specified user.\n\nThis method is used to stop or resume receiving the video stream of a specified user. You can call this method before or after joining a channel. If a user leaves a channel, the settings in this method become invalid.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "uid": "The user ID of the remote user."
      },
      {
        "mute": "Whether to stop receiving the video stream of the specified user: true : Stop receiving the video stream of the specified user. false : (Default) Resume receiving the video stream of the specified user."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_pauseallchannelmediarelayex",
    "name": "PauseAllChannelMediaRelayEx",
    "description": "Pauses the media stream relay to all target channels.\n\nAfter the cross-channel media stream relay starts, you can call this method to pause relaying media streams to all target channels; after the pause, if you want to resume the relay, call ResumeAllChannelMediaRelay. Call this method after StartOrUpdateChannelMediaRelayEx.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -5: The method call was rejected. There is no ongoing channel media relay.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_pushencodedvideoimageex",
    "name": "PushEncodedVideoImage [2/2]",
    "description": "Pushes the external encoded video frame to the SDK.\n\nAfter calling SetExternalVideoSource to enable external video source and set the sourceType parameter to ENCODED_VIDEO_FRAME, you can call this method to push the encoded external video frame to the SDK.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "videoEncodedFrameInfo": "Information about externally encoded video frames. See EncodedVideoFrameInfo."
      },
      {
        "length": "Length of the externally encoded video frames."
      },
      {
        "imageBuffer": "The buffer of the external encoded video frame."
      }
    ],
    "returns": "0: Pushes the external encoded video frame to the SDK successfully.\n < 0: Fails to push external encoded video frames to the SDK.",
    "is_hide": true
  },
  {
    "id": "api_irtcengineex_resumeallchannelmediarelayex",
    "name": "ResumeAllChannelMediaRelayEx",
    "description": "Resumes the media stream relay to all target channels.\n\nAfter calling the PauseAllChannelMediaRelayEx method, you can call this method to resume relaying media streams to all destination channels. Call this method after PauseAllChannelMediaRelayEx.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -5: The method call was rejected. There is no paused channel media relay.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_sendcustomreportmessageex",
    "name": "SendCustomReportMessageEx",
    "description": "Agora supports reporting and analyzing customized messages.\n\nAgora supports reporting and analyzing customized messages. This function is in the beta stage with a free trial. The ability provided in its beta test version is reporting a maximum of 10 message pieces within 6 seconds, with each message piece not exceeding 256 bytes and each string not exceeding 100 bytes. To try out this function, contact and discuss the format of customized messages with us.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_sendstreammessageex",
    "name": "SendStreamMessageEx",
    "description": "Sends data stream messages.\n\nA successful method call triggers the OnStreamMessage callback on the remote client, from which the remote user gets the stream message. A failed method call triggers the OnStreamMessageError callback on the remote client. The SDK has the following restrictions on this method:\n Each client within the channel can have up to 5 data channels simultaneously, with a total shared packet bitrate limit of 30 KB/s for all data channels.\n Each data channel can send up to 60 packets per second, with each packet being a maximum of 1 KB. After calling CreateDataStreamEx [2/2], you can call this method to send data stream messages to all users in the channel.\n Call this method after JoinChannelEx.\n Ensure that you call CreateDataStreamEx [2/2] to create a data channel before calling this method.\n This method applies only to the COMMUNICATION profile or to the hosts in the LIVE_BROADCASTING profile. If an audience in the LIVE_BROADCASTING profile calls this method, the audience may be switched to a host.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "streamId": "The data stream ID. You can get the data stream ID by calling CreateDataStreamEx [2/2]."
      },
      {
        "data": "The message to be sent."
      },
      {
        "length": "The length of the data."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_setdualstreammodeex",
    "name": "SetDualStreamModeEx",
    "description": "Sets the dual-stream mode on the sender side.\n\nThe SDK defaults to enabling low-quality video stream adaptive mode (AUTO_SIMULCAST_STREAM) on the sender side, which means the sender does not actively send low-quality video stream. The receiving end with the role of the host can initiate a low-quality video stream request by calling SetRemoteVideoStreamTypeEx, and upon receiving the request, the sending end automatically starts sending low-quality stream.\n If you want to modify this behavior, you can call this method and set mode to DISABLE_SIMULCAST_STREAM (never send low-quality video streams) or ENABLE_SIMULCAST_STREAM (always send low-quality video streams).\n If you want to restore the default behavior after making changes, you can call this method again with mode set to AUTO_SIMULCAST_STREAM. The difference and connection between this method and EnableDualStreamModeEx is as follows:\n When calling this method and setting mode to DISABLE_SIMULCAST_STREAM, it has the same effect as EnableDualStreamModeEx (false).\n When calling this method and setting mode to ENABLE_SIMULCAST_STREAM, it has the same effect as EnableDualStreamModeEx (true).\n Both methods can be called before and after joining a channel. If both methods are used, the settings in the method called later takes precedence.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "streamConfig": "The configuration of the low-quality video stream. See SimulcastStreamConfig. When setting mode to DISABLE_SIMULCAST_STREAM, setting streamConfig will not take effect."
      },
      {
        "mode": "The mode in which the video stream is sent. See SIMULCAST_STREAM_MODE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_setremoterendermodeex",
    "name": "SetRemoteRenderModeEx",
    "description": "Sets the video display mode of a specified remote user.\n\nAfter initializing the video view of a remote user, you can call this method to update its rendering and mirror modes. This method affects only the video view that the local user sees.\n During a call, you can call this method as many times as necessary to update the display mode of the video view of a remote user.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "uid": "The user ID of the remote user."
      },
      {
        "renderMode": "The video display mode of the remote user. See RENDER_MODE_TYPE."
      },
      {
        "mirrorMode": "The mirror mode of the remote user view. See VIDEO_MIRROR_MODE_TYPE."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_setremotevideostreamtypeex",
    "name": "SetRemoteVideoStreamTypeEx",
    "description": "Sets the video stream type to subscribe to.\n\nThe SDK will dynamically adjust the size of the corresponding video stream based on the size of the video window to save bandwidth and computing resources. The default aspect ratio of the low-quality video stream is the same as that of the high-quality video stream. According to the current aspect ratio of the high-quality video stream, the system will automatically allocate the resolution, frame rate, and bitrate of the low-quality video stream. Depending on the default behavior of the sender and the specific settings when calling SetDualStreamMode [2/2], the scenarios for the receiver calling this method are as follows:\n The SDK enables low-quality video stream adaptive mode (AUTO_SIMULCAST_STREAM) on the sender side by default, meaning only the high-quality video stream is transmitted. Only the receiver with the role of the host can call this method to initiate a low-quality video stream request. Once the sender receives the request, it starts automatically sending the low-quality video stream. At this point, all users in the channel can call this method to switch to low-quality video stream subscription mode.\n If the sender calls SetDualStreamMode [2/2] and sets mode to DISABLE_SIMULCAST_STREAM (never send low-quality video stream), then calling this method will have no effect.\n If the sender calls SetDualStreamMode [2/2] and sets mode to ENABLE_SIMULCAST_STREAM (always send low-quality video stream), both the host and audience receivers can call this method to switch to low-quality video stream subscription mode.\n If the publisher has already called SetDualStreamModeEx and set mode to DISABLE_SIMULCAST_STREAM (never send low-quality video stream), calling this method will not take effect, you should call SetDualStreamModeEx again on the sending end and adjust the settings.\n Calling this method on the receiving end of the audience role will not take effect.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "streamType": "The video stream type, see VIDEO_STREAM_TYPE."
      },
      {
        "uid": "The user ID."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_setremotevideosubscriptionoptionsex",
    "name": "SetRemoteVideoSubscriptionOptionsEx",
    "description": "Options for subscribing to remote video streams.\n\nWhen a remote user has enabled dual-stream mode, you can call this method to choose the option for subscribing to the video streams sent by the remote user.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "options": "The video subscription options. See VideoSubscriptionOptions."
      },
      {
        "uid": "The user ID of the remote user."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_setremotevoicepositionex",
    "name": "SetRemoteVoicePositionEx",
    "description": "Sets the 2D position (the position on the horizontal plane) of the remote user's voice.\n\nThis method sets the voice position and volume of a remote user. When the local user calls this method to set the voice position of a remote user, the voice difference between the left and right channels allows the local user to track the real-time position of the remote user, creating a sense of space. This method applies to massive multiplayer online games, such as Battle Royale games.\n For the best voice positioning, Agora recommends using a wired headset.\n Call this method after joining a channel.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "uid": "The user ID of the remote user."
      },
      {
        "pan": "The voice position of the remote user. The value ranges from -1.0 to 1.0:\n -1.0: The remote voice comes from the left.\n 0.0: (Default) The remote voice comes from the front.\n 1.0: The remote voice comes from the right."
      },
      {
        "gain": "The volume of the remote user. The value ranges from 0.0 to 100.0. The default value is 100.0 (the original volume of the remote user). The smaller the value, the lower the volume."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_setsubscribeaudioallowlistex",
    "name": "SetSubscribeAudioAllowlistEx",
    "description": "Sets the allowlist of subscriptions for audio streams.\n\nYou can call this method to specify the audio streams of a user that you want to subscribe to.\n If a user is added in the allowlist and blocklist at the same time, only the blocklist takes effect.\n You can call this method either before or after joining a channel.\n The allowlist is not affected by the setting in MuteRemoteAudioStream, MuteAllRemoteAudioStreams and autoSubscribeAudio in ChannelMediaOptions.\n Once the allowlist of subscriptions is set, it is effective even if you leave the current channel and rejoin the channel.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "uidNumber": "The number of users in the user ID list."
      },
      {
        "uidList": "The user ID list of users that you want to subscribe to. If you want to specify the audio streams of a user for subscription, add the user ID in this list. If you want to remove a user from the allowlist, you need to call the SetSubscribeAudioAllowlist method to update the user ID list; this means you only add the uid of users that you want to subscribe to in the new user ID list."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_setsubscribeaudioblocklistex",
    "name": "SetSubscribeAudioBlocklistEx",
    "description": "Set the blocklist of subscriptions for audio streams.\n\nYou can call this method to specify the audio streams of a user that you do not want to subscribe to.\n You can call this method either before or after joining a channel.\n The blocklist is not affected by the setting in MuteRemoteAudioStream, MuteAllRemoteAudioStreams, and autoSubscribeAudio in ChannelMediaOptions.\n Once the blocklist of subscriptions is set, it is effective even if you leave the current channel and rejoin the channel.\n If a user is added in the allowlist and blocklist at the same time, only the blocklist takes effect.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "uidNumber": "The number of users in the user ID list."
      },
      {
        "uidList": "The user ID list of users that you do not want to subscribe to. If you want to specify the audio streams of a user that you do not want to subscribe to, add the user ID in this list. If you want to remove a user from the blocklist, you need to call the SetSubscribeAudioBlocklist method to update the user ID list; this means you only add the uid of users that you do not want to subscribe to in the new user ID list."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_setsubscribevideoallowlistex",
    "name": "SetSubscribeVideoAllowlistEx",
    "description": "Set the allowlist of subscriptions for video streams.\n\nYou can call this method to specify the video streams of a user that you want to subscribe to.\n If a user is added in the allowlist and blocklist at the same time, only the blocklist takes effect.\n Once the allowlist of subscriptions is set, it is effective even if you leave the current channel and rejoin the channel.\n You can call this method either before or after joining a channel.\n The allowlist is not affected by the setting in MuteRemoteVideoStream, MuteAllRemoteVideoStreams and autoSubscribeAudio in ChannelMediaOptions.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "uidNumber": "The number of users in the user ID list."
      },
      {
        "uidList": "The user ID list of users that you want to subscribe to. If you want to specify the video streams of a user for subscription, add the user ID of that user in this list. If you want to remove a user from the allowlist, you need to call the SetSubscribeVideoAllowlist method to update the user ID list; this means you only add the uid of users that you want to subscribe to in the new user ID list."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_setsubscribevideoblocklistex",
    "name": "SetSubscribeVideoBlocklistEx",
    "description": "Set the blocklist of subscriptions for video streams.\n\nYou can call this method to specify the video streams of a user that you do not want to subscribe to.\n If a user is added in the allowlist and blocklist at the same time, only the blocklist takes effect.\n Once the blocklist of subscriptions is set, it is effective even if you leave the current channel and rejoin the channel.\n You can call this method either before or after joining a channel.\n The blocklist is not affected by the setting in MuteRemoteVideoStream, MuteAllRemoteVideoStreams and autoSubscribeAudio in ChannelMediaOptions.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "uidNumber": "The number of users in the user ID list."
      },
      {
        "uidList": "The user ID list of users that you do not want to subscribe to. If you want to specify the video streams of a user that you do not want to subscribe to, add the user ID of that user in this list. If you want to remove a user from the blocklist, you need to call the SetSubscribeVideoBlocklist method to update the user ID list; this means you only add the uid of users that you do not want to subscribe to in the new user ID list."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_setupremotevideoex",
    "name": "SetupRemoteVideoEx",
    "description": "Initializes the video view of a remote user.\n\nThis method initializes the video view of a remote stream on the local device. It affects only the video view that the local user sees. Call this method to bind the remote video stream to a video view and to set the rendering and mirror modes of the video view. The application specifies the uid of the remote video in the VideoCanvas method before the remote user joins the channel. If the remote uid is unknown to the application, set it after the application receives the OnUserJoined callback. If the Video Recording function is enabled, the Video Recording Service joins the channel as a dummy client, causing other clients to also receive the onUserJoined callback. Do not bind the dummy client to the application view because the dummy client does not send any video streams. To unbind the remote user from the view, set the view parameter to NULL. Once the remote user leaves the channel, the SDK unbinds the remote user.\n To update the rendering or mirror mode of the remote video view during a call, use the SetRemoteRenderModeEx method.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "canvas": "The remote video view settings. See VideoCanvas."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_setvideoencoderconfigurationex",
    "name": "SetVideoEncoderConfigurationEx",
    "description": "Sets the video encoder configuration.\n\nSets the encoder configuration for the local video. Each configuration profile corresponds to a set of video parameters, including the resolution, frame rate, and bitrate.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "config": "Video profile. See VideoEncoderConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_startmediarenderingtracingex",
    "name": "StartMediaRenderingTracingEx",
    "description": "Enables tracing the video frame rendering process.\n\nBy default, the SDK starts tracing the video rendering event automatically when the local user successfully joins the channel. You can call this method at an appropriate time according to the actual application scenario to customize the tracing process.\n After the local user leaves the current channel, the SDK automatically resets the time point to the next time when the user successfully joins the channel. The SDK starts tracing the rendering status of the video frames in the channel from the moment this method is successfully called and reports information about the event through the OnVideoRenderingTracingResult callback.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_startorupdatechannelmediarelayex",
    "name": "StartOrUpdateChannelMediaRelayEx",
    "description": "Starts relaying media streams across channels or updates channels for media relay.\n\nThe first successful call to this method starts relaying media streams from the source channel to the destination channels. To relay the media stream to other channels, or exit one of the current media relays, you can call this method again to update the destination channels. This feature supports relaying media streams to a maximum of six destination channels. After a successful method call, the SDK triggers the OnChannelMediaRelayStateChanged callback, and this callback returns the state of the media stream relay. Common states are as follows:\n If the OnChannelMediaRelayStateChanged callback returns RELAY_STATE_RUNNING (2) and RELAY_OK (0), it means that the SDK starts relaying media streams from the source channel to the destination channel.\n If the OnChannelMediaRelayStateChanged callback returns RELAY_STATE_FAILURE (3), an exception occurs during the media stream relay.\n Call this method after joining the channel.\n This method takes effect only when you are a host in a live streaming channel.\n The relaying media streams across channels function needs to be enabled by contacting.\n Agora does not support string user accounts in this API.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "configuration": "The configuration of the media stream relay. See ChannelMediaRelayConfiguration."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).\n -2: The parameter is invalid.\n -8: Internal state error. Probably because the user is not a broadcaster.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_startrtmpstreamwithouttranscodingex",
    "name": "StartRtmpStreamWithoutTranscodingEx",
    "description": "Starts pushing media streams to a CDN without transcoding.\n\nCall this method after joining a channel.\n Only hosts in the LIVE_BROADCASTING profile can call this method.\n If you want to retry pushing streams after a failed push, make sure to call StopRtmpStream first, then call this method to retry pushing streams; otherwise, the SDK returns the same error code as the last failed push. Agora recommends that you use the server-side Media Push function. You can call this method to push an audio or video stream to the specified CDN address. This method can push media streams to only one CDN address at a time, so if you need to push streams to multiple addresses, call this method multiple times. After you call this method, the SDK triggers the OnRtmpStreamingStateChanged callback on the local client to report the state of the streaming.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "url": "The address of Media Push. The format is RTMP or RTMPS. The character length cannot exceed 1024 bytes. Special characters such as Chinese characters are not supported."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The URL or configuration of transcoding is invalid; check your URL and transcoding configurations.\n -7: The SDK is not initialized before calling this method.\n -19: The Media Push URL is already in use; use another URL instead.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_startrtmpstreamwithtranscodingex",
    "name": "StartRtmpStreamWithTranscodingEx",
    "description": "Starts Media Push and sets the transcoding configuration.\n\nAgora recommends that you use the server-side Media Push function. You can call this method to push a live audio-and-video stream to the specified CDN address and set the transcoding configuration. This method can push media streams to only one CDN address at a time, so if you need to push streams to multiple addresses, call this method multiple times. After you call this method, the SDK triggers the OnRtmpStreamingStateChanged callback on the local client to report the state of the streaming.\n Ensure that you enable the Media Push service before using this function.\n Call this method after joining a channel.\n Only hosts in the LIVE_BROADCASTING profile can call this method.\n If you want to retry pushing streams after a failed push, make sure to call StopRtmpStreamEx first, then call this method to retry pushing streams; otherwise, the SDK returns the same error code as the last failed push.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "transcoding": "The transcoding configuration for Media Push. See LiveTranscoding."
      },
      {
        "url": "The address of Media Push. The format is RTMP or RTMPS. The character length cannot exceed 1024 bytes. Special characters such as Chinese characters are not supported."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The URL or configuration of transcoding is invalid; check your URL and transcoding configurations.\n -7: The SDK is not initialized before calling this method.\n -19: The Media Push URL is already in use; use another URL instead.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_stopchannelmediarelayex",
    "name": "StopChannelMediaRelayEx",
    "description": "Stops the media stream relay. Once the relay stops, the host quits all the target channels.\n\nAfter a successful method call, the SDK triggers the OnChannelMediaRelayStateChanged callback. If the callback reports RELAY_STATE_IDLE (0) and RELAY_OK (0), the host successfully stops the relay. If the method call fails, the SDK triggers the OnChannelMediaRelayStateChanged callback with the RELAY_ERROR_SERVER_NO_RESPONSE (2) or RELAY_ERROR_SERVER_CONNECTION_LOST (8) status code. You can call the LeaveChannel [2/2] method to leave the channel, and the media stream relay automatically stops.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -5: The method call was rejected. There is no ongoing channel media relay.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_stoprtmpstreamex",
    "name": "StopRtmpStreamEx",
    "description": "Stops pushing media streams to a CDN.\n\nAgora recommends that you use the server-side Media Push function. You can call this method to stop the live stream on the specified CDN address. This method can stop pushing media streams to only one CDN address at a time, so if you need to stop pushing streams to multiple addresses, call this method multiple times. After you call this method, the SDK triggers the OnRtmpStreamingStateChanged callback on the local client to report the state of the streaming.",
    "parameters": [
      {
        "url": "The address of Media Push. The format is RTMP or RTMPS. The character length cannot exceed 1024 bytes. Special characters such as Chinese characters are not supported."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_takesnapshotex",
    "name": "TakeSnapshotEx [1/2]",
    "description": "Takes a snapshot of a video stream using connection ID.\n\nThis method takes a snapshot of a video stream from the specified user, generates a JPG image, and saves it to the specified path.",
    "parameters": [
      {
        "filePath": "The local path (including filename extensions) of the snapshot. For example:\n Windows: C:\\Users\\<user_name>\\AppData\\Local\\Agora\\<process_name>\\example.jpg\n iOS: /App Sandbox/Library/Caches/example.jpg\n macOS: ～/Library/Logs/example.jpg\n Android: /storage/emulated/0/Android/data/<package name>/files/example.jpg Ensure that the path you specify exists and is writable."
      },
      {
        "uid": "The user ID. Set uid as 0 if you want to take a snapshot of the local user's video."
      },
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_takesnapshotex2",
    "name": "TakeSnapshotEx [2/2]",
    "description": "Gets a video screenshot of the specified observation point using the connection ID.\n\nThis method takes a snapshot of a video stream from the specified user, generates a JPG image, and saves it to the specified path.",
    "parameters": [
      {
        "config": "The configuration of the snaptshot. See SnapshotConfig."
      },
      {
        "uid": "The user ID. Set uid as 0 if you want to take a snapshot of the local user's video."
      },
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_updatechannelmediaoptionsex",
    "name": "UpdateChannelMediaOptionsEx",
    "description": "Updates the channel media options after joining the channel.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "options": "The channel media options. See ChannelMediaOptions."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.\n -2: The value of a member in ChannelMediaOptions is invalid. For example, the token or the user ID is invalid. You need to fill in a valid parameter.\n -7: The IRtcEngine object has not been initialized. You need to initialize the IRtcEngine object before calling this method.\n -8: The internal state of the IRtcEngine object is wrong. The possible reason is that the user is not in the channel. Agora recommends that you use the OnConnectionStateChanged callback to see whether the user is in the channel. If you receive the CONNECTION_STATE_DISCONNECTED (1) or CONNECTION_STATE_FAILED (5) state, the user is not in the channel. You need to call JoinChannel [2/2] to join a channel before calling this method.",
    "is_hide": false
  },
  {
    "id": "api_irtcengineex_updatertmptranscodingex",
    "name": "UpdateRtmpTranscodingEx",
    "description": "Updates the transcoding configuration.\n\nAgora recommends that you use the server-side Media Push function. After you start pushing media streams to CDN with transcoding, you can dynamically update the transcoding configuration according to the scenario. The SDK triggers the OnTranscodingUpdated callback after the transcoding configuration is updated.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "transcoding": "The transcoding configuration for Media Push. See LiveTranscoding."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ivideodevicemanager_enumeratevideodevices",
    "name": "EnumerateVideoDevices",
    "description": "Enumerates the video devices.\n\nThis method is for Windows and macOS only.",
    "parameters": [],
    "returns": "Success: A DeviceInfo array including all video devices in the system.\n Failure: An empty array.",
    "is_hide": false
  },
  {
    "id": "api_ivideodevicemanager_getcapability",
    "name": "GetCapability",
    "description": "Gets the detailed video frame information of the video capture device in the specified video format.\n\nThis method is for Windows and macOS only. After calling NumberOfCapabilities to get the number of video formats supported by the video capture device, you can call this method to get the specific video frame information supported by the specified index number.",
    "parameters": [
      {
        "deviceIdUTF8": "The ID of the video capture device."
      },
      {
        "deviceCapabilityNumber": "The index number of the video format. If the return value of NumberOfCapabilities is i, the value range of this parameter is [0,i)."
      },
      {
        "capability": "An output parameter. Indicates the specific information of the specified video format, including width (px), height (px), and frame rate (fps). See VideoFormat."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ivideodevicemanager_getdevice",
    "name": "GetDevice",
    "description": "Retrieves the current video capture device.\n\nThis method is for Windows and macOS only.",
    "parameters": [
      {
        "deviceIdUTF8": "An output parameter. The device ID."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ivideodevicemanager_numberofcapabilities",
    "name": "NumberOfCapabilities",
    "description": "Gets the number of video formats supported by the specified video capture device.\n\nThis method is for Windows and macOS only. Video capture devices may support multiple video formats, and each format supports different combinations of video frame width, video frame height, and frame rate. You can call this method to get how many video formats the specified video capture device can support, and then call GetCapability to get the specific video frame information in the specified video format.",
    "parameters": [
      {
        "deviceIdUTF8": "The ID of the video capture device."
      }
    ],
    "returns": "> 0: Success. Returns the number of video formats supported by this device. For example: If the specified camera supports 10 different video formats, the return value is 10.\n ≤ 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ivideodevicemanager_setdevice",
    "name": "SetDevice",
    "description": "Specifies the video capture device with the device ID.\n\nPlugging or unplugging a device does not change its device ID.\n This method is for Windows and macOS only.",
    "parameters": [
      {
        "deviceIdUTF8": "The device ID. You can get the device ID by calling EnumerateVideoDevices."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "api_ivideodevicemanager_startdevicetest",
    "name": "StartDeviceTest",
    "description": "Starts the video capture device test.\n\nThis method tests whether the video-capture device is working properly. Before calling this method, ensure that you have already called the EnableVideo method, and the window handle (hwnd) parameter is valid.",
    "parameters": [
      {
        "hwnd": "The window handle of the view."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_ivideodevicemanager_stopdevicetest",
    "name": "StopDeviceTest",
    "description": "Stops the video capture device test.",
    "parameters": [],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": true
  },
  {
    "id": "api_videosurface_setenable",
    "name": "SetEnable",
    "description": "Sets whether to enable the video rendering.",
    "parameters": [
      {
        "enable": "Whether to enable the video rendering: true : (Default) Enable the video rendering. false : Disable the video rendering."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "api_videosurface_setforuser",
    "name": "SetForUser",
    "description": "Sets the local or remote video display.\n\nEnsure that you call this method in the main thread.\n Ensure that you call this method before binding VideoSurface.cs.",
    "parameters": [
      {
        "uid": "The ID of remote users, obtained through OnUserJoined. The default value is 0, which means you can see the local video."
      },
      {
        "channelId": "The ID of the channel."
      },
      {
        "source_type": "The type of the video source. See VIDEO_SOURCE_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_iaudioencodedframeobserver_onmixedaudioencodedframe",
    "name": "OnMixedAudioEncodedFrame",
    "description": "Gets the mixed and encoded audio data of the local and all remote users.\n\nAfter calling RegisterAudioEncodedFrameObserver and setting the audio profile as AUDIO_ENCODED_FRAME_OBSERVER_POSITION_MIXED, you can get the mixed and encoded audio data of the local and all remote users through this callback.",
    "parameters": [
      {
        "samplesPerSec": "Recording sample rate (Hz)."
      },
      {
        "channels": "The number of channels.\n 1: Mono.\n 2: Stereo. If the channel uses stereo, the data is interleaved."
      },
      {
        "samplesPerChannel": "The number of samples per channel in the audio frame."
      },
      {
        "frameBufferPtr": "The audio buffer."
      },
      {
        "length": "The data length (byte)."
      },
      {
        "audioEncodedFrameInfo": "Audio information after encoding. See EncodedAudioFrameInfo."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_iaudioencodedframeobserver_onplaybackaudioencodedframe",
    "name": "OnPlaybackAudioEncodedFrame",
    "description": "Gets the encoded audio data of all remote users.\n\nAfter calling RegisterAudioEncodedFrameObserver and setting the encoded audio as AUDIO_ENCODED_FRAME_OBSERVER_POSITION_PLAYBACK, you can get encoded audio data of all remote users through this callback.",
    "parameters": [
      {
        "samplesPerSec": "Recording sample rate (Hz)."
      },
      {
        "channels": "The number of channels.\n 1: Mono.\n 2: Stereo. If the channel uses stereo, the data is interleaved."
      },
      {
        "samplesPerChannel": "The number of samples per channel in the audio frame."
      },
      {
        "frameBufferPtr": "The audio buffer."
      },
      {
        "length": "The data length (byte)."
      },
      {
        "audioEncodedFrameInfo": "Audio information after encoding. See EncodedAudioFrameInfo."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_iaudioencodedframeobserver_onrecordaudioencodedframe",
    "name": "OnRecordAudioEncodedFrame",
    "description": "Gets the encoded audio data of the local user.\n\nAfter calling RegisterAudioEncodedFrameObserver and setting the encoded audio as AUDIO_ENCODED_FRAME_OBSERVER_POSITION_RECORD, you can get the encoded audio data of the local user from this callback.",
    "parameters": [
      {
        "channels": "The number of channels.\n 1: Mono.\n 2: Stereo. If the channel uses stereo, the data is interleaved."
      },
      {
        "frameBufferPtr": "The audio buffer."
      },
      {
        "length": "The data length (byte)."
      },
      {
        "audioEncodedFrameInfo": "Audio information after encoding. See EncodedAudioFrameInfo."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_iaudioframeobserver_ismultiplechannelframewanted",
    "name": "IsMultipleChannelFrameWanted",
    "description": "Determines whether to receive audio data from multiple channels.\n\nAfter you register the audio frame observer, the SDK triggers this callback every time it captures an audio frame. In the multi-channel scenario, if you want to get audio data from multiple channels, set the return value of this callback as true. After that, the SDK triggers the callback to send you the before-mixing audio frame from various channels. You can also get the channel ID of each audio frame.\n Once you set the return value of this callback as true, the SDK triggers only the callback to send the audio data. and OnPlaybackAudioFrameBeforeMixing is not to be triggered. In the multi-channel scenario, Agora recommends setting the return value as true.\n If you set the return value of this callback as false, the SDK triggers only the OnPlaybackAudioFrameBeforeMixing callback to send the audio data.",
    "parameters": [],
    "returns": "true : Receive audio data from multiple channels. false : Do not receive audio data from multiple channels.",
    "is_hide": true
  },
  {
    "id": "callback_iaudioframeobserver_onplaybackaudioframebeforemixing",
    "name": "OnPlaybackAudioFrameBeforeMixing",
    "description": "Retrieves the audio frame before mixing of subscribed remote users.",
    "parameters": [
      {
        "channel_id": "The channel ID."
      },
      {
        "uid": "The ID of subscribed remote users."
      },
      {
        "audio_Frame": "The raw audio data. See AudioFrame."
      }
    ],
    "returns": "Without practical meaning.",
    "is_hide": false
  },
  {
    "id": "callback_iaudioframeobserver_onplaybackaudioframebeforemixing2",
    "name": "OnPlaybackAudioFrameBeforeMixing",
    "description": "Retrieves the audio frame of a specified user before mixing.",
    "parameters": [
      {
        "channel_id": "The channel name that the audio frame came from."
      },
      {
        "uid": "The ID of the user sending the audio frame."
      },
      {
        "audio_Frame": "The raw audio data. See AudioFrame."
      }
    ],
    "returns": "Reserved for future use.",
    "is_hide": true
  },
  {
    "id": "callback_iaudioframeobserverbase_onearmonitoringaudioframe",
    "name": "OnEarMonitoringAudioFrame",
    "description": "Gets the in-ear monitoring audio frame.\n\nIn order to ensure that the obtained in-ear audio data meets the expectations, Agora recommends that you set the in-ear monitoring-ear audio data format as follows: After calling SetEarMonitoringAudioFrameParameters to set the audio data format and RegisterAudioFrameObserver to register the audio frame observer object, the SDK calculates the sampling interval according to the parameters set in the methods, and triggers the OnEarMonitoringAudioFrame callback according to the sampling interval.",
    "parameters": [
      {
        "audioFrame": "The raw audio data. See AudioFrame."
      }
    ],
    "returns": "Without practical meaning.",
    "is_hide": false
  },
  {
    "id": "callback_iaudioframeobserverbase_onmixedaudioframe",
    "name": "OnMixedAudioFrame",
    "description": "Retrieves the mixed captured and playback audio frame.\n\nTo ensure that the data format of mixed captured and playback audio frame meets the expectations, Agora recommends that you set the data format as follows: After calling SetMixedAudioFrameParameters to set the audio data format and RegisterAudioFrameObserver to register the audio frame observer object, the SDK calculates the sampling interval according to the parameters set in the methods, and triggers the OnMixedAudioFrame callback according to the sampling interval.",
    "parameters": [
      {
        "audio_Frame": "The raw audio data. See AudioFrame."
      },
      {
        "channelId": "The channel ID."
      }
    ],
    "returns": "Without practical meaning.",
    "is_hide": false
  },
  {
    "id": "callback_iaudioframeobserverbase_onplaybackaudioframe",
    "name": "OnPlaybackAudioFrame",
    "description": "Gets the raw audio frame for playback.\n\nTo ensure that the data format of audio frame for playback is as expected, Agora recommends that you set the audio data format as follows: After calling SetPlaybackAudioFrameParameters to set the audio data format and RegisterAudioFrameObserver to register the audio frame observer object, the SDK calculates the sampling interval according to the parameters set in the methods, and triggers the OnPlaybackAudioFrame callback according to the sampling interval.",
    "parameters": [
      {
        "audio_Frame": "The raw audio data. See AudioFrame."
      },
      {
        "channelId": "The channel ID."
      }
    ],
    "returns": "Without practical meaning.",
    "is_hide": false
  },
  {
    "id": "callback_iaudioframeobserverbase_onrecordaudioframe",
    "name": "OnRecordAudioFrame",
    "description": "Gets the captured audio frame.\n\nTo ensure that the data format of captured audio frame is as expected, Agora recommends that you set the audio data format as follows: After calling SetRecordingAudioFrameParameters to set the audio data format, call RegisterAudioFrameObserver to register the audio observer object, the SDK will calculate the sampling interval according to the parameters set in this method, and triggers the OnRecordAudioFrame callback according to the sampling interval.",
    "parameters": [
      {
        "audioFrame": "The raw audio data. See AudioFrame."
      },
      {
        "channelId": "The channel ID."
      }
    ],
    "returns": "Without practical meaning.",
    "is_hide": false
  },
  {
    "id": "callback_iaudiopcmframesink_onframe",
    "name": "OnFrame",
    "description": "Occurs each time the player receives an audio frame.\n\nAfter registering the audio frame observer, the callback occurs every time the player receives an audio frame, reporting the detailed information of the audio frame.",
    "parameters": [
      {
        "frame": "The audio frame information. See AudioPcmFrame."
      }
    ],
    "returns": "Without practical meaning.",
    "is_hide": false
  },
  {
    "id": "callback_iaudiospectrumobserver_onlocalaudiospectrum",
    "name": "OnLocalAudioSpectrum",
    "description": "Gets the statistics of a local audio spectrum.\n\nAfter successfully calling RegisterAudioSpectrumObserver to implement the OnLocalAudioSpectrum callback in IAudioSpectrumObserver and calling EnableAudioSpectrumMonitor to enable audio spectrum monitoring, the SDK triggers this callback as the time interval you set to report the received remote audio data spectrum before encoding.",
    "parameters": [
      {
        "data": "The audio spectrum data of the local user. See AudioSpectrumData."
      }
    ],
    "returns": "Whether the spectrum data is received: true : Spectrum data is received. false : No spectrum data is received.",
    "is_hide": false
  },
  {
    "id": "callback_iaudiospectrumobserver_onremoteaudiospectrum",
    "name": "OnRemoteAudioSpectrum",
    "description": "Gets the remote audio spectrum.\n\nAfter successfully calling RegisterAudioSpectrumObserver to implement the OnRemoteAudioSpectrum callback in the IAudioSpectrumObserver and calling EnableAudioSpectrumMonitor to enable audio spectrum monitoring, the SDK will trigger the callback as the time interval you set to report the received remote audio data spectrum.",
    "parameters": [
      {
        "spectrums": "The audio spectrum information of the remote user. See UserAudioSpectrumInfo. The number of arrays is the number of remote users monitored by the SDK. If the array is null, it means that no audio spectrum of remote users is detected."
      },
      {
        "spectrumNumber": "The number of remote users."
      }
    ],
    "returns": "Whether the spectrum data is received: true : Spectrum data is received. false : No spectrum data is received.",
    "is_hide": false
  },
  {
    "id": "callback_icloudspatialaudioeventhandler_onconnectionstatechange",
    "name": "onConnectionStateChange",
    "description": "Occurs when the connection state between the SDK and the Agora Spatial Audio Server changes.\n\nWhen the connection state between the SDK and the Agora Spatial Audio Server changes, the SDK triggers this callback to inform the user of the current connection state and the reason for the change.",
    "parameters": [
      {
        "state": "The connection state between the SDK and the Agora Spatial Audio Server. See SAE_CONNECTION_STATE_TYPE."
      },
      {
        "reason": "Occurs when the connection state between the SDK and the Agora Spatial Audio Server changes. See SAE_CONNECTION_CHANGED_REASON_TYPE."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_icloudspatialaudioeventhandler_onteammatejoined",
    "name": "onTeammateJoined",
    "description": "Occurs when the user joins the current team.\n\nWhen a remote user with the same team ID calls EnterRoom to enter the current room, the local user receives this callback.",
    "parameters": [
      {
        "uid": "The user ID of the remote user who joins the current team."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_icloudspatialaudioeventhandler_onteammateleft",
    "name": "onTeammateLeft",
    "description": "Occurs when the user leaves the current team.\n\nWhen a remote user in the current team calls ExitRoom to leave the current room, the local user receives this callback.",
    "parameters": [
      {
        "uid": "The user ID of the remote user who leaves the current team."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_icloudspatialaudioeventhandler_ontokenwillexpire",
    "name": "OnTokenWillExpire",
    "description": "Occurs when the RTM token expires.\n\nOnce the RTM token expires, the SDK triggers this callback to notify the app to renew the RTM token. When you receive this callback, you need to generate a new token on your server and call to pass the new token to the SDK.",
    "parameters": [],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_idirectcdnstreamingeventhandler_ondirectcdnstreamingstatechanged",
    "name": "OnDirectCdnStreamingStateChanged",
    "description": "Occurs when the CDN streaming state changes.\n\nWhen the host directly pushes streams to the CDN, if the streaming state changes, the SDK triggers this callback to report the changed streaming state, error codes, and other information. You can troubleshoot issues by referring to this callback.",
    "parameters": [
      {
        "state": "The current CDN streaming state. See DIRECT_CDN_STREAMING_STATE."
      },
      {
        "reason": "Reasons for changes in the status of CDN streaming. See DIRECT_CDN_STREAMING_REASON."
      },
      {
        "message": "The information about the changed streaming state."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_idirectcdnstreamingeventhandler_ondirectcdnstreamingstats",
    "name": "OnDirectCdnStreamingStats",
    "description": "Reports the CDN streaming statistics.\n\nWhen the host directly pushes media streams to the CDN, the SDK triggers this callback every one second.",
    "parameters": [
      {
        "stats": "The statistics of the current CDN streaming. See DirectCdnStreamingStats."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_ifaceinfoobserver_onfaceinfo",
    "name": "OnFaceInfo",
    "description": "Occurs when the facial information processed by speech driven extension is received.",
    "parameters": [
      {
        "outFaceInfo": "Output parameter, the JSON string of the facial information processed by the voice driver plugin, including the following fields:\n faces: Object sequence. The collection of facial information, with each face corresponding to an object.\n blendshapes: Object. The collection of face capture coefficients, named according to ARkit standards, with each key-value pair representing a blendshape coefficient. The blendshape coefficient is a floating point number with a range of [0.0, 1.0].\n rotation: Object sequence. The rotation of the head, which includes the following three key-value pairs, with values as floating point numbers ranging from -180.0 to 180.0:\n pitch: Head pitch angle. A positve value means looking down, while a negative value means looking up.\n yaw: Head yaw angle. A positve value means turning left, while a negative value means turning right.\n roll: Head roll angle. A positve value means tilting to the right, while a negative value means tilting to the left.\n timestamp: String. The timestamp of the output result, in milliseconds. Here is an example of JSON:\n{ \"faces\":[{ \"blendshapes\":{ \"eyeBlinkLeft\":0.9, \"eyeLookDownLeft\":0.0, \"eyeLookInLeft\":0.0, \"eyeLookOutLeft\":0.0, \"eyeLookUpLeft\":0.0, \"eyeSquintLeft\":0.0, \"eyeWideLeft\":0.0, \"eyeBlinkRight\":0.0, \"eyeLookDownRight\":0.0, \"eyeLookInRight\":0.0, \"eyeLookOutRight\":0.0, \"eyeLookUpRight\":0.0, \"eyeSquintRight\":0.0, \"eyeWideRight\":0.0, \"jawForward\":0.0, \"jawLeft\":0.0, \"jawRight\":0.0, \"jawOpen\":0.0, \"mouthClose\":0.0, \"mouthFunnel\":0.0, \"mouthPucker\":0.0, \"mouthLeft\":0.0, \"mouthRight\":0.0, \"mouthSmileLeft\":0.0, \"mouthSmileRight\":0.0, \"mouthFrownLeft\":0.0, \"mouthFrownRight\":0.0, \"mouthDimpleLeft\":0.0, \"mouthDimpleRight\":0.0, \"mouthStretchLeft\":0.0, \"mouthStretchRight\":0.0, \"mouthRollLower\":0.0, \"mouthRollUpper\":0.0, \"mouthShrugLower\":0.0, \"mouthShrugUpper\":0.0, \"mouthPressLeft\":0.0, \"mouthPressRight\":0.0, \"mouthLowerDownLeft\":0.0, \"mouthLowerDownRight\":0.0, \"mouthUpperUpLeft\":0.0, \"mouthUpperUpRight\":0.0, \"browDownLeft\":0.0, \"browDownRight\":0.0, \"browInnerUp\":0.0, \"browOuterUpLeft\":0.0, \"browOuterUpRight\":0.0, \"cheekPuff\":0.0, \"cheekSquintLeft\":0.0, \"cheekSquintRight\":0.0, \"noseSneerLeft\":0.0, \"noseSneerRight\":0.0, \"tongueOut\":0.0 }, \"rotation\":{\"pitch\":30.0, \"yaw\":25.5, \"roll\":-15.5},\n }], \"timestamp\":\"654879876546\" }"
      }
    ],
    "returns": "true : Facial information JSON parsing successful. false : Facial information JSON parsing failed.",
    "is_hide": false
  },
  {
    "id": "callback_imediaplayercustomdataprovider_onreaddata",
    "name": "OnReadData",
    "description": "Occurs when the SDK reads the media resource data.\n\nWhen you call the OpenWithMediaSource method to open a media resource, the SDK triggers this callback and request you to pass in the buffer of the media resource data.",
    "parameters": [
      {
        "bufferPtr": "An input parameter. Data buffer (bytes). Write the bufferSize data reported by the SDK into this parameter."
      },
      {
        "bufferSize": "The length of the data buffer (bytes)."
      }
    ],
    "returns": "If the data is read successfully, pass in the length of the data (bytes) you actually read in the return value.\n If reading the data fails, pass in 0 in the return value.",
    "is_hide": false
  },
  {
    "id": "callback_imediaplayercustomdataprovider_onseek",
    "name": "OnSeek",
    "description": "Occurs when the SDK seeks the media resource data.\n\nWhen you call the OpenWithMediaSource or Open method to open a custom media resource, the SDK triggers this callback to request the specified location in the media resource.",
    "parameters": [
      {
        "offset": "An input parameter. The offset of the target position relative to the starting point, in bytes. The value can be positive or negative."
      },
      {
        "whence": "An input parameter. The starting point. You can set it as one of the following values:\n 0: The starting point is the head of the data, and the actual data offset after seeking is offset.\n 1: The starting point is the current position, and the actual data offset after seeking is the current position plus offset.\n 2: The starting point is the end of the data, and the actual data offset after seeking is the whole data length plus offset.\n 65536: Do not perform position seeking, return the file size. Agora recommends that you use this parameter value when playing pure audio files such as MP3 and WAV."
      }
    ],
    "returns": "When whence is 65536, the media file size is returned.\n When whence is 0, 1, or 2, the actual data offset after the seeking is returned.\n -1: Seeking failed.",
    "is_hide": false
  },
  {
    "id": "callback_imediaplayersourceobserver_onagoracdntokenwillexpire",
    "name": "OnAgoraCDNTokenWillExpire",
    "description": "Occurs when the token is about to expire.\n\nIf the ts is about to expire when you call the SwitchAgoraCDNLineByIndex method to switch the CDN route for playing the media resource, the SDK triggers this callback to remind you to renew the authentication information. You need to call the RenewAgoraCDNSrcToken method to pass in the updated authentication information to update the authentication information of the media resource URL. After updating the authentication information, you need to call SwitchAgoraCDNLineByIndex to complete the route switching.",
    "parameters": [],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_imediaplayersourceobserver_onaudiovolumeindication",
    "name": "OnAudioVolumeIndication",
    "description": "Reports the volume of the media player.\n\nThe SDK triggers this callback every 200 milliseconds to report the current volume of the media player.",
    "parameters": [
      {
        "volume": "The volume of the media player. The value ranges from 0 to 255."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_imediaplayersourceobserver_oncompleted",
    "name": "OnCompleted",
    "description": "Occurs when the media file is played once.",
    "parameters": [],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_imediaplayersourceobserver_onmetadata",
    "name": "OnMetaData",
    "description": "Occurs when the media metadata is received.\n\nThe callback occurs when the player receives the media metadata and reports the detailed information of the media metadata.",
    "parameters": [
      {
        "data": "The detailed data of the media metadata."
      },
      {
        "length": "The data length (bytes)."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_imediaplayersourceobserver_onplaybufferupdated",
    "name": "OnPlayBufferUpdated",
    "description": "Reports the playback duration that the buffered data can support.\n\nWhen playing online media resources, the SDK triggers this callback every two seconds to report the playback duration that the currently buffered data can support.\n When the playback duration supported by the buffered data is less than the threshold (0 by default), the SDK returns PLAYER_EVENT_BUFFER_LOW (6).\n When the playback duration supported by the buffered data is greater than the threshold (0 by default), the SDK returns PLAYER_EVENT_BUFFER_RECOVER (7).",
    "parameters": [
      {
        "playCachedBuffer": "The playback duration (ms) that the buffered data can support."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_imediaplayersourceobserver_onplayercachestats",
    "name": "OnPlayerCacheStats",
    "description": "Reports the statistics of the media file being cached.\n\nAfter you call the OpenWithMediaSource method and set enableCache as true, the SDK triggers this callback once per second to report the statistics of the media file being cached.",
    "parameters": [
      {
        "stats": "The statistics of the media file being cached. See CacheStatistics."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_imediaplayersourceobserver_onplayerevent",
    "name": "OnPlayerEvent",
    "description": "Reports the player events.\n\nAfter calling the Seek method, the SDK triggers the callback to report the results of the seek operation.",
    "parameters": [
      {
        "eventCode": "The player event. See MEDIA_PLAYER_EVENT."
      },
      {
        "elapsedTime": "The time (ms) when the event occurs."
      },
      {
        "message": "Information about the event."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_imediaplayersourceobserver_onplayerinfoupdated",
    "name": "OnPlayerInfoUpdated",
    "description": "Occurs when information related to the media player changes.\n\nWhen the information about the media player changes, the SDK triggers this callback. You can use this callback for troubleshooting.",
    "parameters": [
      {
        "info": "Information related to the media player. See PlayerUpdatedInfo."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_imediaplayersourceobserver_onplayerplaybackstats",
    "name": "OnPlayerPlaybackStats",
    "description": "The statistics of the media file being played.\n\nThe SDK triggers this callback once per second to report the statistics of the media file being played.",
    "parameters": [
      {
        "stats": "The statistics of the media file. See PlayerPlaybackStats."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_imediaplayersourceobserver_onplayersourcestatechanged",
    "name": "OnPlayerSourceStateChanged",
    "description": "Reports the changes of playback state.\n\nWhen the state of the media player changes, the SDK triggers this callback to report the current playback state.",
    "parameters": [
      {
        "state": "The playback state. See MEDIA_PLAYER_STATE."
      },
      {
        "reason": "The reason for the changes in the media player status. See MEDIA_PLAYER_REASON."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_imediaplayersourceobserver_onplayersrcinfochanged",
    "name": "OnPlayerSrcInfoChanged",
    "description": "Occurs when the video bitrate of the media resource changes.",
    "parameters": [
      {
        "from": "Information about the video bitrate of the media resource being played. See SrcInfo."
      },
      {
        "to": "Information about the changed video bitrate of media resource being played. See SrcInfo."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_imediaplayersourceobserver_onpositionchanged",
    "name": "OnPositionChanged",
    "description": "Reports the playback progress of the media file.\n\nWhen playing media files, the SDK triggers this callback every two second to report current playback progress.",
    "parameters": [
      {
        "position_ms": "The playback position (ms) of media files."
      },
      {
        "timeStampMs": "The NTP timestamp (ms) of the current playback progress."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_imediaplayersourceobserver_onpreloadevent",
    "name": "OnPreloadEvent",
    "description": "Reports the events of preloaded media resources.",
    "parameters": [
      {
        "src": "The URL of the media resource."
      },
      {
        "@event": "Events that occur when media resources are preloaded. See PLAYER_PRELOAD_EVENT."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_imediarecorderobserver_onrecorderinfoupdated",
    "name": "OnRecorderInfoUpdated",
    "description": "Occurs when the recording information is updated.\n\nAfter you successfully enable the audio and video recording, the SDK periodically triggers this callback based on the value of recorderInfoUpdateInterval set in MediaRecorderConfiguration. This callback reports the file name, duration, and size of the current recording file.",
    "parameters": [
      {
        "uid": "The user ID."
      },
      {
        "channelId": "The channel name."
      },
      {
        "info": "The information about the file that is recorded. See RecorderInfo."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_imediarecorderobserver_onrecorderstatechanged",
    "name": "OnRecorderStateChanged",
    "description": "Occurs when the recording state changes.\n\nWhen the recording state changes, the SDK triggers this callback to report the current recording state and the reason for the change.",
    "parameters": [
      {
        "channelId": "The channel name."
      },
      {
        "uid": "The user ID."
      },
      {
        "state": "The current recording state. See RecorderState."
      },
      {
        "error": "The reason for the state change. See RecorderReasonCode."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_imetadataobserver_getmaxmetadatasize",
    "name": "GetMaxMetadataSize",
    "description": "Occurs when the SDK requests the maximum size of the metadata.\n\nAfter successfully complete the registration by calling RegisterMediaMetadataObserver, the SDK triggers this callback once every video frame is sent. You need to specify the maximum size of the metadata in the return value of this callback.",
    "parameters": [],
    "returns": "The maximum size of the buffer of the metadata that you want to use. The highest value is 1024 bytes. Ensure that you set the return value.",
    "is_hide": true
  },
  {
    "id": "callback_imetadataobserver_onmetadatareceived",
    "name": "OnMetadataReceived",
    "description": "Occurs when the local user receives the metadata.",
    "parameters": [
      {
        "metadata": "The metadata received. See Metadata."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_imetadataobserver_onreadytosendmetadata",
    "name": "OnReadyToSendMetadata",
    "description": "Occurs when the SDK is ready to send metadata.\n\nThis callback is triggered when the SDK is ready to send metadata.",
    "parameters": [
      {
        "source_type": "Video data type. See VIDEO_SOURCE_TYPE."
      },
      {
        "metadata": "The metadata that the user wants to send. See Metadata."
      }
    ],
    "returns": "true : Send the video frame. false : Do not send the video frame.",
    "is_hide": false
  },
  {
    "id": "callback_imusiccontentcentereventhandler_onlyricresult",
    "name": "OnLyricResult",
    "description": "Reports the download URL for lyrics.\n\nWhen you call the GetLyric method to get the download URL for lyrics, the SDK triggers this callback.",
    "parameters": [
      {
        "reason": "音乐内容中心的请求状态码，详见 MusicContentCenterStateReason 。"
      },
      {
        "songCode": "The code of the music, which is an unique identifier of the music."
      },
      {
        "requestId": "The request ID. 本次请求的唯一标识。"
      },
      {
        "lyricUrl": "The download URL for lyrics."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_imusiccontentcentereventhandler_onmusicchartsresult",
    "name": "OnMusicChartsResult",
    "description": "获取音乐榜单回调。\n\n当你调用 GetMusicCharts 方法获取全部音乐榜单之后，SDK 会触发该回调。",
    "parameters": [
      {
        "requestId": "The request ID. 本次请求的唯一标识。"
      },
      {
        "reason": "音乐内容中心的请求状态码，详见 MusicContentCenterStateReason 。"
      },
      {
        "result": "MusicChartInfo 当前可播放的音乐榜单列表。"
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_imusiccontentcentereventhandler_onmusiccollectionresult",
    "name": "OnMusicCollectionResult",
    "description": "Reports the details of music collection.\n\n当你调 GetMusicCollectionByMusicChartId 方法来获取指定榜单的音乐资源列表或调用 SearchMusic 来搜索音乐资源时，SDK 会触发此回调报告榜单中音乐资源列表的详细信息。",
    "parameters": [
      {
        "reason": "音乐内容中心的请求状态码，详见 MusicContentCenterStateReason 。"
      },
      {
        "requestId": "The request ID. 本次请求的唯一标识。"
      },
      {
        "result": "MusicCollection The details of musci assets in the collection."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_imusiccontentcentereventhandler_onpreloadevent",
    "name": "OnPreLoadEvent",
    "description": "Reports the events of preloading music assets.\n\nWhen you call Preload [1/2] the method to preload music assets, the SDK will trigger this callback.",
    "parameters": [
      {
        "reason": "音乐内容中心的请求状态码，详见 MusicContentCenterStateReason 。"
      },
      {
        "requestId": "The request ID. 本次请求的唯一标识。"
      },
      {
        "songCode": "The code of the music, which is an unique identifier of the music."
      },
      {
        "percent": "The current loading progress of the music asset. The value range is [0, 100]."
      },
      {
        "lyricUrl": "The download URL for lyrics."
      },
      {
        "state": "The loading status of the current music asset. See PreloadState."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_imusiccontentcentereventhandler_onsongsimpleinforesult",
    "name": "OnSongSimpleInfoResult",
    "description": "音乐资源的详细信息回调。\n\n当你调用 GetSongSimpleInfo 获取某一音乐资源的详细信息后，SDK 会触发该回调。",
    "parameters": [
      {
        "reason": "音乐内容中心的请求状态码，详见 MusicContentCenterStateReason 。"
      },
      {
        "requestId": "The request ID. 本次请求的唯一标识。"
      },
      {
        "songCode": "The code of the music, which is an unique identifier of the music."
      },
      {
        "simpleInfo": "音乐资源的相关信息，包含下列内容：\n 副歌片段的开始和结束的时间（ms）\n 副歌片段的歌词下载地址\n 副歌片段时长（ms）\n 歌曲名称\n 歌手名"
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_irtcengineeventhandler_onactivespeaker",
    "name": "OnActiveSpeaker",
    "description": "Occurs when the most active remote speaker is detected.\n\nAfter a successful call of EnableAudioVolumeIndication, the SDK continuously detects which remote user has the loudest volume. During the current period, the remote user whose volume is detected as the loudest for the most times, is the most active user. When the number of users is no less than two and an active remote speaker exists, the SDK triggers this callback and reports the uid of the most active remote speaker.\n If the most active remote speaker is always the same user, the SDK triggers the OnActiveSpeaker callback only once.\n If the most active remote speaker changes to another user, the SDK triggers this callback again and reports the uid of the new active remote speaker.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "uid": "The user ID of the most active speaker."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onaudiodevicestatechanged",
    "name": "OnAudioDeviceStateChanged",
    "description": "Occurs when the audio device state changes.\n\nThis callback notifies the application that the system's audio device state is changed. For example, a headset is unplugged from the device. This method is for Windows and macOS only.",
    "parameters": [
      {
        "deviceId": "The device ID."
      },
      {
        "deviceType": "The device type. See MEDIA_DEVICE_TYPE."
      },
      {
        "deviceState": "The device state. See MEDIA_DEVICE_STATE_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onaudiodevicevolumechanged",
    "name": "OnAudioDeviceVolumeChanged",
    "description": "Reports the volume change of the audio device or app.\n\nOccurs when the volume on the playback device, audio capture device, or the volume of the app changes. This callback is for Windows and macOS only.",
    "parameters": [
      {
        "deviceType": "The device type. See MEDIA_DEVICE_TYPE."
      },
      {
        "volume": "The volume value. The range is [0, 255]."
      },
      {
        "muted": "Whether the audio device is muted: true : The audio device is muted. false : The audio device is not muted."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onaudioeffectfinished",
    "name": "OnAudioEffectFinished",
    "description": "Occurs when the playback of the local music file finishes.\n\nThis callback occurs when the local audio effect file finishes playing.",
    "parameters": [
      {
        "soundId": "The ID of the audio effect. The ID of each audio effect file is unique."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onaudiomixingfinished",
    "name": "OnAudioMixingFinished",
    "description": "Occurs when the playback of the local music file finishes.\n\nDeprecated: Use OnAudioMixingStateChanged instead. After you call StartAudioMixing [2/2] to play a local music file, this callback occurs when the playback finishes. If the call of StartAudioMixing [2/2] fails, the error code WARN_AUDIO_MIXING_OPEN_ERROR is returned.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onaudiomixingpositionchanged",
    "name": "OnAudioMixingPositionChanged",
    "description": "Reports the playback progress of a music file.\n\nAfter you called the StartAudioMixing [2/2] method to play a music file, the SDK triggers this callback every two seconds to report the playback progress.",
    "parameters": [
      {
        "position": "The playback progress (ms)."
      }
    ],
    "returns": "0: Success.\n < 0: Failure.",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onaudiomixingstatechanged",
    "name": "OnAudioMixingStateChanged",
    "description": "Occurs when the playback state of the music file changes.\n\nThis callback occurs when the playback state of the music file changes, and reports the current state and error code.",
    "parameters": [
      {
        "state": "The playback state of the music file. See AUDIO_MIXING_STATE_TYPE."
      },
      {
        "reason": "Error code. See AUDIO_MIXING_REASON_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onaudiopublishstatechanged",
    "name": "OnAudioPublishStateChanged",
    "description": "Occurs when the audio publishing state changes.",
    "parameters": [
      {
        "channel": "The channel name."
      },
      {
        "oldState": "The previous publishing state. See STREAM_PUBLISH_STATE."
      },
      {
        "newState": "The current publishing stat. See STREAM_PUBLISH_STATE."
      },
      {
        "elapseSinceLastState": "The time elapsed (ms) from the previous state to the current state."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onaudioquality",
    "name": "OnAudioQuality",
    "description": "Reports the statistics of the audio stream sent by each remote user.\n\nDeprecated: Use OnRemoteAudioStats instead. The SDK triggers this callback once every two seconds to report the audio quality of each remote user who is sending an audio stream. If a channel has multiple users sending audio streams, the SDK triggers this callback as many times.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The user ID of the remote user sending the audio stream."
      },
      {
        "quality": "Audio quality of the user. See QUALITY_TYPE."
      },
      {
        "delay": "The network delay (ms) from the sender to the receiver, including the delay caused by audio sampling pre-processing, network transmission, and network jitter buffering."
      },
      {
        "lost": "The packet loss rate (%) of the audio packet sent from the remote user to the receiver."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onaudioroutingchanged",
    "name": "OnAudioRoutingChanged",
    "description": "Occurs when the local audio route changes.\n\nThis method is for Android, iOS and macOS only.",
    "parameters": [
      {
        "routing": "The current audio routing. See AudioRoute."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onaudiosubscribestatechanged",
    "name": "OnAudioSubscribeStateChanged",
    "description": "Occurs when the audio subscribing state changes.",
    "parameters": [
      {
        "channel": "The channel name."
      },
      {
        "uid": "The user ID of the remote user."
      },
      {
        "oldState": "The previous subscribing status. See STREAM_SUBSCRIBE_STATE."
      },
      {
        "newState": "The current subscribing status. See STREAM_SUBSCRIBE_STATE."
      },
      {
        "elapseSinceLastState": "The time elapsed (ms) from the previous state to the current state."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onaudiovolumeindication",
    "name": "OnAudioVolumeIndication",
    "description": "Reports the volume information of users.\n\nBy default, this callback is disabled. You can enable it by calling EnableAudioVolumeIndication. Once this callback is enabled and users send streams in the channel, the SDK triggers the OnAudioVolumeIndication callback according to the time interval set in EnableAudioVolumeIndication. The SDK triggers two independent OnAudioVolumeIndication callbacks simultaneously, which separately report the volume information of the local user who sends a stream and the remote users (up to three) whose instantaneous volume is the highest. Once this callback is enabled, if the local user calls the MuteLocalAudioStream method to mute, the SDK continues to report the volume indication of the local user. If a remote user whose volume is one of the three highest in the channel stops publishing the audio stream for 20 seconds, the callback excludes this user's information; if all remote users stop publishing audio streams for 20 seconds, the SDK stops triggering the callback for remote users.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "speakers": "The volume information of the users. See AudioVolumeInfo. An empty speakers array in the callback indicates that no remote user is in the channel or is sending a stream."
      },
      {
        "speakerNumber": "The total number of users.\n In the callback for the local user, if the local user is sending streams, the value of speakerNumber is 1.\n In the callback for remote users, the value range of speakerNumber is [0,3]. If the number of remote users who send streams is greater than or equal to three, the value of speakerNumber is 3."
      },
      {
        "totalVolume": "The volume of the speaker. The value range is [0,255].\n In the callback for the local user, totalVolume is the volume of the local user who sends a stream. In the callback for remote users, totalVolume is the sum of the volume of all remote users (up to three) whose instantaneous volume is the highest."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_oncameraexposureareachanged",
    "name": "OnCameraExposureAreaChanged",
    "description": "Occurs when the camera exposure area changes.",
    "parameters": [
      {
        "x": "The x coordinate of the changed camera exposure area."
      },
      {
        "y": "The y coordinate of the changed camera exposure area."
      },
      {
        "width": "The width of the changed camera exposure area."
      },
      {
        "height": "The height of the changed exposure area."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_oncamerafocusareachanged",
    "name": "OnCameraFocusAreaChanged",
    "description": "Occurs when the camera focus area changes.\n\nThe SDK triggers this callback when the local user changes the camera focus position by calling SetCameraFocusPositionInPreview. This callback is for Android and iOS only.",
    "parameters": [
      {
        "x": "The x-coordinate of the changed camera focus area."
      },
      {
        "y": "The y-coordinate of the changed camera focus area."
      },
      {
        "width": "The width of the changed camera focus area."
      },
      {
        "height": "The height of the changed camera focus area."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_oncameraready",
    "name": "OnCameraReady",
    "description": "Occurs when the camera turns on and is ready to capture the video.\n\nDeprecated: Use LOCAL_VIDEO_STREAM_STATE_CAPTURING (1) in OnLocalVideoStateChanged instead. This callback indicates that the camera has been successfully turned on and you can start to capture video.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onchannelmediarelaystatechanged",
    "name": "OnChannelMediaRelayStateChanged",
    "description": "Occurs when the state of the media stream relay changes.\n\nThe SDK returns the state of the current media relay with any error message.",
    "parameters": [
      {
        "state": "The state code. See CHANNEL_MEDIA_RELAY_STATE."
      },
      {
        "code": "The error code of the channel media relay. See CHANNEL_MEDIA_RELAY_ERROR."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onclientrolechanged",
    "name": "OnClientRoleChanged",
    "description": "Occurs when the user role or the audience latency level changes.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "oldRole": "Role that the user switches from: CLIENT_ROLE_TYPE."
      },
      {
        "newRole": "Role that the user switches to: CLIENT_ROLE_TYPE."
      },
      {
        "newRoleOptions": "Properties of the role that the user switches to. See ClientRoleOptions."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onclientrolechangefailed",
    "name": "OnClientRoleChangeFailed",
    "description": "Occurs when switching a user role fails.\n\nThis callback informs you about the reason for failing to switching and your current user role.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "reason": "The reason for a user role switch failure. See CLIENT_ROLE_CHANGE_FAILED_REASON."
      },
      {
        "currentRole": "Current user role. See CLIENT_ROLE_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onconnectionbanned",
    "name": "OnConnectionBanned",
    "description": "Occurs when the connection is banned by the Agora server.\n\nDeprecated: Use OnConnectionStateChanged instead.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onconnectioninterrupted",
    "name": "OnConnectionInterrupted",
    "description": "Occurs when the connection between the SDK and the server is interrupted.\n\nDeprecated: Use OnConnectionStateChanged instead. The SDK triggers this callback when it loses connection with the server for more than four seconds after the connection is established. After triggering this callback, the SDK tries to reconnect to the server. You can use this callback to implement pop-up reminders. The differences between this callback and OnConnectionLost are as follow:\n The SDK triggers the OnConnectionInterrupted callback when it loses connection with the server for more than four seconds after it successfully joins the channel.\n The SDK triggers the OnConnectionLost callback when it loses connection with the server for more than 10 seconds, whether or not it joins the channel. If the SDK fails to rejoin the channel 20 minutes after being disconnected from Agora's edge server, the SDK stops rejoining the channel.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onconnectionlost",
    "name": "OnConnectionLost",
    "description": "Occurs when the SDK cannot reconnect to Agora's edge server 10 seconds after its connection to the server is interrupted.\n\nThe SDK triggers this callback when it cannot connect to the server 10 seconds after calling the JoinChannel [2/2] method, regardless of whether it is in the channel. If the SDK fails to rejoin the channel 20 minutes after being disconnected from Agora's edge server, the SDK stops rejoining the channel.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onconnectionstatechanged",
    "name": "OnConnectionStateChanged",
    "description": "Occurs when the network connection state changes.\n\nWhen the network connection state changes, the SDK triggers this callback and reports the current connection state and the reason for the change.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "state": "The current connection state. See CONNECTION_STATE_TYPE."
      },
      {
        "reason": "The reason for a connection state change. See CONNECTION_CHANGED_REASON_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_oncontentinspectresult",
    "name": "onContentInspectResult",
    "description": "Reports the result of video content moderation.\n\nAfter calling EnableContentInspect to enable the video content moderation, and setting the type parameter in ContentInspectConfig to, the SDK triggers the onContentInspectResult callback and reports the result of video content moderation.",
    "parameters": [
      {
        "result": "Content moderation results. See CONTENT_INSPECT_RESULT."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "callback_irtcengineeventhandler_onencryptionerror",
    "name": "OnEncryptionError",
    "description": "Reports the built-in encryption errors.\n\nWhen encryption is enabled by calling EnableEncryption, the SDK triggers this callback if an error occurs in encryption or decryption on the sender or the receiver side.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "errorType": "Details about the error type. See ENCRYPTION_ERROR_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onerror",
    "name": "OnError",
    "description": "Reports an error during SDK runtime.\n\nThis callback indicates that an error (concerning network or media) occurs during SDK runtime. In most cases, the SDK cannot fix the issue and resume running. The SDK requires the app to take action or informs the user about the issue.",
    "parameters": [
      {
        "err": "Error code. See ERROR_CODE_TYPE."
      },
      {
        "msg": "The error message."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onextensionerrorwithcontext",
    "name": "OnExtensionErrorWithContext",
    "description": "Occurs when the extension runs incorrectly.\n\nIn case of extension enabling failure or runtime errors, the extension triggers this callback and reports the error code along with the reasons.",
    "parameters": [
      {
        "context": "The context information of the extension, see ExtensionContext."
      },
      {
        "error": "Error code. For details, see the extension documentation provided by the extension provider."
      },
      {
        "message": "Reason. For details, see the extension documentation provided by the extension provider."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onextensioneventwithcontext",
    "name": "OnExtensionEventWithContext",
    "description": "The event callback of the extension.\n\nTo listen for events while the extension is running, you need to register this callback.",
    "parameters": [
      {
        "value": "The value of the extension key."
      },
      {
        "key": "The key of the extension."
      },
      {
        "context": "The context information of the extension, see ExtensionContext."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onextensionstartedwithcontext",
    "name": "OnExtensionStartedWithContext",
    "description": "Occurrs when the extension is enabled.\n\nThe callback is triggered after the extension is successfully enabled.",
    "parameters": [
      {
        "context": "The context information of the extension, see ExtensionContext."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onextensionstoppedwithcontext",
    "name": "OnExtensionStoppedWithContext",
    "description": "Occurs when the extension is disabled.\n\nThe callback is triggered after the extension is successfully disabled.",
    "parameters": [
      {
        "context": "The context information of the extension, see ExtensionContext."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onfacepositionchanged",
    "name": "OnFacePositionChanged",
    "description": "Reports the face detection result of the local user.\n\nOnce you enable face detection by calling EnableFaceDetection (true), you can get the following information on the local user in real-time:\n The width and height of the local video.\n The position of the human face in the local view.\n The distance between the human face and the screen. This value is based on the fitting calculation of the local video size and the position of the human face.\n This callback is for Android and iOS only.\n When it is detected that the face in front of the camera disappears, the callback will be triggered immediately. When no human face is detected, the frequency of this callback to be triggered wil be decreased to reduce power consumption on the local device.\n The SDK stops triggering this callback when a human face is in close proximity to the screen.",
    "parameters": [
      {
        "imageWidth": "The width (px) of the video image captured by the local camera."
      },
      {
        "imageHeight": "The height (px) of the video image captured by the local camera."
      },
      {
        "vecRectangle": "The information of the detected human face. See Rectangle."
      },
      {
        "vecDistance": "The distance between the human face and the device screen (cm)."
      },
      {
        "numFaces": "The number of faces detected. If the value is 0, it means that no human face is detected."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onfirstlocalaudioframepublished",
    "name": "OnFirstLocalAudioFramePublished",
    "description": "Occurs when the first audio frame is published.\n\nThe SDK triggers this callback under one of the following circumstances:\n The local client enables the audio module and calls JoinChannel [2/2] successfully.\n The local client calls MuteLocalAudioStream (true) and MuteLocalAudioStream (false) in sequence.\n The local client calls DisableAudio and EnableAudio in sequence.\n The local client calls PushAudioFrame to successfully push the audio frame to the SDK.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "elapsed": "Time elapsed (ms) from the local user calling JoinChannel [2/2] until the SDK triggers this callback."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onfirstlocalvideoframe",
    "name": "OnFirstLocalVideoFrame",
    "description": "Occurs when the first local video frame is displayed on the local video view.\n\nThe SDK triggers this callback when the first local video frame is displayed on the local video view.",
    "parameters": [
      {
        "source": "The type of the video source. See VIDEO_SOURCE_TYPE."
      },
      {
        "width": "The width (px) of the first local video frame."
      },
      {
        "height": "The height (px) of the first local video frame."
      },
      {
        "elapsed": "The time elapsed (ms) from the local user calling JoinChannel [1/2] or JoinChannel [2/2] to join the channel to when the SDK triggers this callback. If StartPreview [1/2] / StartPreview [2/2] is called before joining the channel, this parameter indicates the time elapsed from calling StartPreview [1/2] or StartPreview [2/2] to when this event occurred."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onfirstlocalvideoframepublished",
    "name": "OnFirstLocalVideoFramePublished",
    "description": "Occurs when the first video frame is published.\n\nThe SDK triggers this callback under one of the following circumstances:\n The local client enables the video module and calls JoinChannel [1/2] or JoinChannel [2/2] to join the channel successfully.\n The local client calls MuteLocalVideoStream (true) and MuteLocalVideoStream (false) in sequence.\n The local client calls DisableVideo and EnableVideo in sequence.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "elapsed": "Time elapsed (ms) from the local user calling JoinChannel [1/2] or JoinChannel [2/2] until this callback is triggered."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onfirstremoteaudiodecoded",
    "name": "OnFirstRemoteAudioDecoded",
    "description": "Occurs when the SDK decodes the first remote audio frame for playback.\n\nDeprecated: Use OnRemoteAudioStateChanged instead. The SDK triggers this callback under one of the following circumstances:\n The remote user joins the channel and sends the audio stream for the first time.\n The remote user's audio is offline and then goes online to re-send audio. It means the local user cannot receive audio in 15 seconds. Reasons for such an interruption include:\n The remote user leaves channel.\n The remote user drops offline.\n The remote user calls MuteLocalAudioStream to stop sending the audio stream.\n The remote user calls DisableAudio to disable audio.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "uid": "The user ID of the remote user."
      },
      {
        "elapsed": "The time elapsed (ms) from the local user calling JoinChannel [2/2] until the SDK triggers this callback."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onfirstremoteaudioframe",
    "name": "OnFirstRemoteAudioFrame",
    "description": "Occurs when the SDK receives the first audio frame from a specific remote user.\n\nDeprecated: Use OnRemoteAudioStateChanged instead.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "userId": "The user ID of the remote user."
      },
      {
        "elapsed": "The time elapsed (ms) from the local user calling JoinChannel [2/2] until the SDK triggers this callback."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onfirstremotevideodecoded",
    "name": "OnFirstRemoteVideoDecoded",
    "description": "Occurs when the first remote video frame is received and decoded.\n\nThe SDK triggers this callback under one of the following circumstances:\n The remote user joins the channel and sends the video stream.\n The remote user stops sending the video stream and re-sends it after 15 seconds. Reasons for such an interruption include:\n The remote user leaves the channel.\n The remote user drops offline.\n The remote user calls DisableVideo to disable video.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The user ID of the remote user sending the video stream."
      },
      {
        "width": "The width (px) of the video stream."
      },
      {
        "height": "The height (px) of the video stream."
      },
      {
        "elapsed": "The time elapsed (ms) from the local user calling JoinChannel [1/2] or JoinChannel [2/2] until the SDK triggers this callback."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onfirstremotevideoframe",
    "name": "OnFirstRemoteVideoFrame",
    "description": "Occurs when the renderer receives the first frame of the remote video.\n\nThis callback is only triggered when the video frame is rendered by the SDK; it will not be triggered if the user employs custom video rendering.You need to implement this independently using methods outside the SDK.",
    "parameters": [
      {
        "remoteUid": "The user ID of the remote user sending the video stream."
      },
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "width": "The width (px) of the video stream."
      },
      {
        "height": "The height (px) of the video stream."
      },
      {
        "elapsed": "The time elapsed (ms) from the local user calling JoinChannel [1/2] or JoinChannel [2/2] until the SDK triggers this callback."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onjoinchannelsuccess",
    "name": "OnJoinChannelSuccess",
    "description": "Occurs when a user joins a channel.\n\nThis callback notifies the application that a user joins a specified channel.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "elapsed": "The time elapsed (ms) from the local user calling JoinChannel [2/2] until the SDK triggers this callback."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onlastmileproberesult",
    "name": "OnLastmileProbeResult",
    "description": "Reports the last mile network probe result.\n\nThe SDK triggers this callback within 30 seconds after the app calls StartLastmileProbeTest.",
    "parameters": [
      {
        "result": "The uplink and downlink last-mile network probe test result. See LastmileProbeResult."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onlastmilequality",
    "name": "OnLastmileQuality",
    "description": "Reports the last-mile network quality of the local user.\n\nThis callback reports the last-mile network conditions of the local user before the user joins the channel. Last mile refers to the connection between the local device and Agora's edge server. Before the user joins the channel, this callback is triggered by the SDK once StartLastmileProbeTest is called and reports the last-mile network conditions of the local user.",
    "parameters": [
      {
        "quality": "The last-mile network quality. QUALITY_UNKNOWN (0): The quality is unknown. QUALITY_EXCELLENT (1): The quality is excellent. QUALITY_GOOD (2): The network quality seems excellent, but the bitrate can be slightly lower than excellent. QUALITY_POOR (3): Users can feel the communication is slightly impaired. QUALITY_BAD (4): Users cannot communicate smoothly. QUALITY_VBAD (5): The quality is so bad that users can barely communicate. QUALITY_DOWN (6): The network is down, and users cannot communicate at all. QUALITY_DETECTING (8): The last-mile probe test is in progress. See QUALITY_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onleavechannel",
    "name": "OnLeaveChannel",
    "description": "Occurs when a user leaves a channel.\n\nYou can obtain information such as the total duration of a call, and the data traffic that the SDK transmits and receives.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "stats": "Call statistics. See RtcStats."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onlocalaudiostatechanged",
    "name": "OnLocalAudioStateChanged",
    "description": "Occurs when the local audio stream state changes.\n\nWhen the state of the local audio stream changes (including the state of the audio capture and encoding), the SDK triggers this callback to report the current state. This callback indicates the state of the local audio stream, and allows you to troubleshoot issues when audio exceptions occur. When the state is LOCAL_AUDIO_STREAM_STATE_FAILED (3), you can view the error information in the error parameter.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "state": "The state of the local audio. See LOCAL_AUDIO_STREAM_STATE."
      },
      {
        "reason": "Reasons for local audio state changes. See LOCAL_AUDIO_STREAM_REASON."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onlocalaudiostats",
    "name": "OnLocalAudioStats",
    "description": "Reports the statistics of the local audio stream.\n\nThe SDK triggers this callback once every two seconds.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "stats": "Local audio statistics. See LocalAudioStats."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onlocaluserregistered",
    "name": "OnLocalUserRegistered",
    "description": "Occurs when the local user registers a user account.\n\nAfter the local user successfully calls RegisterLocalUserAccount to register the user account or calls JoinChannelWithUserAccount [2/2] to join a channel, the SDK triggers the callback and informs the local user's UID and User Account.",
    "parameters": [
      {
        "uid": "The ID of the local user."
      },
      {
        "userAccount": "The user account of the local user."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onlocalvideostatechanged",
    "name": "OnLocalVideoStateChanged",
    "description": "Occurs when the local video stream state changes.\n\nWhen the status of the local video changes, the SDK triggers this callback to report the current local video state and the reason for the state change.",
    "parameters": [
      {
        "source": "The type of the video source. See VIDEO_SOURCE_TYPE."
      },
      {
        "state": "The state of the local video, see LOCAL_VIDEO_STREAM_STATE."
      },
      {
        "reason": "The reasons for changes in local video state. See LOCAL_VIDEO_STREAM_REASON."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onlocalvideostats",
    "name": "OnLocalVideoStats",
    "description": "Reports the statistics of the local video stream.\n\nThe SDK triggers this callback once every two seconds to report the statistics of the local video stream.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "stats": "The statistics of the local video stream. See LocalVideoStats."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onlocalvideotranscodererror",
    "name": "OnLocalVideoTranscoderError",
    "description": "Occurs when there's an error during the local video mixing.\n\nWhen you fail to call StartLocalVideoTranscoder or UpdateLocalTranscoderConfiguration, the SDK triggers this callback to report the reason.",
    "parameters": [
      {
        "stream": "The video streams that cannot be mixed during video mixing. See TranscodingVideoStream."
      },
      {
        "error": "The reason for local video mixing error. See VIDEO_TRANSCODER_ERROR."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onnetworkquality",
    "name": "OnNetworkQuality",
    "description": "Reports the last mile network quality of each user in the channel.\n\nThis callback reports the last mile network conditions of each user in the channel. Last mile refers to the connection between the local device and Agora's edge server. The SDK triggers this callback once every two seconds. If a channel includes multiple users, the SDK triggers this callback as many times. This callback provides feedback on network quality through sending and receiving broadcast packets within the channel. Excessive broadcast packets can lead to broadcast storms. To prevent broadcast storms from causing a large amount of data transmission within the channel, this callback supports feedback on the network quality of up to 4 remote hosts simultaneously by default. txQuality is UNKNOWN when the user is not sending a stream; rxQuality is UNKNOWN when the user is not receiving a stream.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The user ID. The network quality of the user with this user ID is reported. If the uid is 0, the local network quality is reported."
      },
      {
        "txQuality": "Uplink network quality rating of the user in terms of the transmission bit rate, packet loss rate, average RTT (Round-Trip Time) and jitter of the uplink network. This parameter is a quality rating helping you understand how well the current uplink network conditions can support the selected video encoder configuration. For example, a 1000 Kbps uplink network may be adequate for video frames with a resolution of 640 × 480 and a frame rate of 15 fps in the LIVE_BROADCASTING profile, but might be inadequate for resolutions higher than 1280 × 720. See QUALITY_TYPE."
      },
      {
        "rxQuality": "Downlink network quality rating of the user in terms of packet loss rate, average RTT, and jitter of the downlink network. See QUALITY_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onnetworktypechanged",
    "name": "OnNetworkTypeChanged",
    "description": "Occurs when the local network type changes.\n\nThis callback occurs when the connection state of the local user changes. You can get the connection state and reason for the state change in this callback. When the network connection is interrupted, this callback indicates whether the interruption is caused by a network type change or poor network conditions.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "type": "The type of the local network connection. See NETWORK_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onpermissionerror",
    "name": "OnPermissionError",
    "description": "Occurs when the SDK cannot get the device permission.\n\nWhen the SDK fails to get the device permission, the SDK triggers this callback to report which device permission cannot be got.",
    "parameters": [
      {
        "permissionType": "The type of the device permission. See PERMISSION_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onproxyconnected",
    "name": "OnProxyConnected",
    "description": "Reports the proxy connection state.\n\nYou can use this callback to listen for the state of the SDK connecting to a proxy. For example, when a user calls SetCloudProxy and joins a channel successfully, the SDK triggers this callback to report the user ID, the proxy type connected, and the time elapsed fromthe user calling JoinChannel [2/2] until this callback is triggered.",
    "parameters": [
      {
        "channel": "The channel name."
      },
      {
        "uid": "The user ID."
      },
      {
        "proxyType": "The proxy type connected. See PROXY_TYPE."
      },
      {
        "localProxyIp": "Reserved for future use."
      },
      {
        "elapsed": "The time elapsed (ms) from the user calling JoinChannel [2/2] until this callback is triggered."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onrejoinchannelsuccess",
    "name": "OnRejoinChannelSuccess",
    "description": "Occurs when a user rejoins the channel.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "elapsed": "Time elapsed (ms) from the local user calling JoinChannel [2/2] until the SDK triggers this callback."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onremoteaudiostatechanged",
    "name": "OnRemoteAudioStateChanged",
    "description": "Occurs when the remote audio state changes.\n\nWhen the audio state of a remote user (in a voice/video call channel) or host (in a live streaming channel) changes, the SDK triggers this callback to report the current state of the remote audio stream. This callback does not work properly when the number of users (in the communication profile) or hosts (in the live streaming channel) in a channel exceeds 17.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The ID of the remote user whose audio state changes."
      },
      {
        "state": "The state of the remote audio. See REMOTE_AUDIO_STATE."
      },
      {
        "reason": "The reason of the remote audio state change. See REMOTE_AUDIO_STATE_REASON."
      },
      {
        "elapsed": "Time elapsed (ms) from the local user calling the JoinChannel [2/2] method until the SDK triggers this callback."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onremoteaudiostats",
    "name": "OnRemoteAudioStats",
    "description": "Reports the transport-layer statistics of each remote audio stream.\n\nThe SDK triggers this callback once every two seconds for each remote user who is sending audio streams. If a channel includes multiple remote users, the SDK triggers this callback as many times.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "stats": "The statistics of the received remote audio streams. See RemoteAudioStats."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onremoteaudiotransportstats",
    "name": "OnRemoteAudioTransportStats",
    "description": "Reports the transport-layer statistics of each remote audio stream.\n\nDeprecated: Use OnRemoteAudioStats instead. This callback reports the transport-layer statistics, such as the packet loss rate and network time delay after the local user receives an audio packet from a remote user. During a call, when the user receives the audio packet sent by the remote user, the callback is triggered every 2 seconds.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The ID of the remote user sending the audio streams."
      },
      {
        "delay": "The network delay (ms) from the remote user to the receiver."
      },
      {
        "lost": "The packet loss rate (%) of the audio packet sent from the remote user to the receiver."
      },
      {
        "rxKBitrate": "The bitrate of the received audio (Kbps)."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onremotesubscribefallbacktoaudioonly",
    "name": "OnRemoteSubscribeFallbackToAudioOnly",
    "description": "Occurs when the remote media stream falls back to the audio-only stream due to poor network conditions or switches back to the video stream after the network conditions improve.\n\nIf you call SetRemoteSubscribeFallbackOption and set option to STREAM_FALLBACK_OPTION_AUDIO_ONLY, the SDK triggers this callback in the following situations:\n The downstream network condition is poor, and the subscribed video stream is downgraded to audio-only stream.\n The downstream network condition has improved, and the subscribed stream has been restored to video stream. Once the remote media stream switches to the low-quality video stream due to weak network conditions, you can monitor the stream switch between a high-quality and low-quality stream in the OnRemoteVideoStats callback.",
    "parameters": [
      {
        "uid": "The user ID of the remote user."
      },
      {
        "isFallbackOrRecover": "true : The subscribed media stream falls back to audio-only due to poor network conditions. false : The subscribed media stream switches back to the video stream after the network conditions improve."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onremotevideostatechanged",
    "name": "OnRemoteVideoStateChanged",
    "description": "Occurs when the remote video stream state changes.\n\nThis callback does not work properly when the number of users (in the communication profile) or hosts (in the live streaming channel) in a channel exceeds 17.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The ID of the remote user whose video state changes."
      },
      {
        "state": "The state of the remote video. See REMOTE_VIDEO_STATE."
      },
      {
        "reason": "The reason for the remote video state change. See REMOTE_VIDEO_STATE_REASON."
      },
      {
        "elapsed": "Time elapsed (ms) from the local user calling the JoinChannel [2/2] method until the SDK triggers this callback."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onremotevideostats",
    "name": "OnRemoteVideoStats",
    "description": "Reports the statistics of the video stream sent by each remote users.\n\nReports the statistics of the video stream from the remote users. The SDK triggers this callback once every two seconds for each remote user. If a channel has multiple users/hosts sending video streams, the SDK triggers this callback as many times.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "stats": "Statistics of the remote video stream. See RemoteVideoStats."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onremotevideotransportstats",
    "name": "OnRemoteVideoTransportStats",
    "description": "Reports the transport-layer statistics of each remote video stream.\n\nDeprecated: This callback is deprecated. Use OnRemoteVideoStats instead. This callback reports the transport-layer statistics, such as the packet loss rate and network time delay after the local user receives a video packet from a remote user. During a call, when the user receives the video packet sent by the remote user/host, the callback is triggered every 2 seconds.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The ID of the remote user sending the video packets."
      },
      {
        "delay": "The network delay (ms) from the sender to the receiver."
      },
      {
        "lost": "The packet loss rate (%) of the video packet sent from the remote user."
      },
      {
        "rxKBitRate": "The bitrate of the received video (Kbps)."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onrequesttoken",
    "name": "OnRequestToken",
    "description": "Occurs when the token expires.\n\nThe SDK triggers this callback if the token expires. When receiving this callback, you need to generate a new token on your token server and you can renew your token through one of the following ways:\n In scenarios involving one channel:\n Call RenewToken to pass in the new token.\n Call LeaveChannel [2/2] to leave the current channel and then pass in the new token when you call JoinChannel [2/2] to join a channel.\n In scenarios involving mutiple channels: Call UpdateChannelMediaOptionsEx to pass in the new token.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onrhythmplayerstatechanged",
    "name": "OnRhythmPlayerStateChanged",
    "description": "Occurs when the state of virtual metronome changes.\n\nWhen the state of the virtual metronome changes, the SDK triggers this callback to report the current state of the virtual metronome. This callback indicates the state of the local audio stream and enables you to troubleshoot issues when audio exceptions occur. This callback is for Android and iOS only.",
    "parameters": [
      {
        "state": "For the current virtual metronome status, see RHYTHM_PLAYER_STATE_TYPE."
      },
      {
        "errorCode": "For the error codes and error messages related to virtual metronome errors, see RHYTHM_PLAYER_REASON."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onrtcstats",
    "name": "OnRtcStats",
    "description": "Reports the statistics about the current call.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "stats": "Statistics of the RTC engine. See RtcStats."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onrtmpstreamingevent",
    "name": "OnRtmpStreamingEvent",
    "description": "Reports events during the Media Push.",
    "parameters": [
      {
        "url": "The URL for Media Push."
      },
      {
        "eventCode": "The event code of Media Push. See RTMP_STREAMING_EVENT."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onrtmpstreamingstatechanged",
    "name": "OnRtmpStreamingStateChanged",
    "description": "Occurs when the state of Media Push changes.\n\nWhen the state of Media Push changes, the SDK triggers this callback and reports the URL address and the current state of the Media Push. This callback indicates the state of the Media Push. When exceptions occur, you can troubleshoot issues by referring to the detailed error descriptions in the error code parameter.",
    "parameters": [
      {
        "url": "The URL address where the state of the Media Push changes."
      },
      {
        "state": "The current state of the Media Push. See RTMP_STREAM_PUBLISH_STATE."
      },
      {
        "reason": "Reasons for the changes in the Media Push status. See RTMP_STREAM_PUBLISH_REASON."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onsnapshottaken",
    "name": "OnSnapshotTaken",
    "description": "Reports the result of taking a video snapshot.\n\nAfter a successful TakeSnapshot [1/2] method call, the SDK triggers this callback to report whether the snapshot is successfully taken as well as the details for the snapshot taken.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "uid": "The user ID. One uid of 0 indicates the local user."
      },
      {
        "filePath": "The local path of the snapshot."
      },
      {
        "width": "The width (px) of the snapshot."
      },
      {
        "height": "The height (px) of the snapshot."
      },
      {
        "errCode": "The message that confirms success or gives the reason why the snapshot is not successfully taken:\n 0: Success.\n < 0: Failure:\n -1: The SDK fails to write data to a file or encode a JPEG image.\n -2: The SDK does not find the video stream of the specified user within one second after the TakeSnapshot [1/2] method call succeeds. The possible reasons are: local capture stops, remote end stops publishing, or video data processing is blocked.\n -3: Calling the TakeSnapshot [1/2] method too frequently."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onstreammessage",
    "name": "OnStreamMessage",
    "description": "Occurs when the local user receives the data stream from the remote user.\n\nThe SDK triggers this callback when the local user receives the stream message that the remote user sends by calling the SendStreamMessage method.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The ID of the remote user sending the message."
      },
      {
        "streamId": "The stream ID of the received message."
      },
      {
        "data": "The data received."
      },
      {
        "length": "The data length (byte)."
      },
      {
        "sentTs": "The time when the data stream is sent."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onstreammessageerror",
    "name": "OnStreamMessageError",
    "description": "Occurs when the local user does not receive the data stream from the remote user.\n\nThe SDK triggers this callback when the local user fails to receive the stream message that the remote user sends by calling the SendStreamMessage method.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The ID of the remote user sending the message."
      },
      {
        "streamId": "The stream ID of the received message."
      },
      {
        "code": "Error code."
      },
      {
        "missed": "The number of lost messages."
      },
      {
        "cached": "Number of incoming cached messages when the data stream is interrupted."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_ontokenprivilegewillexpire",
    "name": "OnTokenPrivilegeWillExpire",
    "description": "Occurs when the token expires in 30 seconds.\n\nWhen receiving this callback, you need to generate a new token on your token server and you can renew your token through one of the following ways:\n In scenarios involving one channel:\n Call RenewToken to pass in the new token.\n Call LeaveChannel [2/2] to leave the current channel and then pass in the new token when you call JoinChannel [2/2] to join a channel.\n In scenarios involving mutiple channels: Call UpdateChannelMediaOptionsEx to pass in the new token.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "token": "The token that is about to expire."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_ontranscodedstreamlayoutinfo",
    "name": "OnTranscodedStreamLayoutInfo",
    "description": "Occurs when the local user receives a mixed video stream carrying layout information.\n\nWhen the local user receives a mixed video stream sent by the video mixing server for the first time, or when there is a change in the layout information of the mixed stream, the SDK triggers this callback, reporting the layout information of each sub-video stream within the mixed video stream. This callback is for Android and iOS only.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "uid": "User ID who published this mixed video stream."
      },
      {
        "width": "Width (px) of the mixed video stream."
      },
      {
        "height": "Heitht (px) of the mixed video stream."
      },
      {
        "layoutCount": "The number of layout information in the mixed video stream."
      },
      {
        "layoutlist": "Layout information of a specific sub-video stream within the mixed stream. See VideoLayout."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_ontranscodingupdated",
    "name": "OnTranscodingUpdated",
    "description": "Occurs when the publisher's transcoding is updated.\n\nWhen the LiveTranscoding class in the StartRtmpStreamWithTranscoding method updates, the SDK triggers the OnTranscodingUpdated callback to report the update information. If you call the StartRtmpStreamWithTranscoding method to set the LiveTranscoding class for the first time, the SDK does not trigger this callback.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onuplinknetworkinfoupdated",
    "name": "OnUplinkNetworkInfoUpdated",
    "description": "Occurs when the uplink network information changes.\n\nThe SDK triggers this callback when the uplink network information changes. This callback only applies to scenarios where you push externally encoded video data in H.264 format to the SDK.",
    "parameters": [
      {
        "info": "The uplink network information. See UplinkNetworkInfo."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onuserenablelocalvideo",
    "name": "OnUserEnableLocalVideo",
    "description": "Occurs when a specific remote user enables/disables the local video capturing function.\n\nDeprecated: This callback is deprecated, use the following enumerations in the OnRemoteVideoStateChanged callback: REMOTE_VIDEO_STATE_STOPPED (0) and REMOTE_VIDEO_STATE_REASON_REMOTE_MUTED (5). REMOTE_VIDEO_STATE_DECODING (2) and REMOTE_VIDEO_STATE_REASON_REMOTE_UNMUTED (6). The SDK triggers this callback when the remote user resumes or stops capturing the video stream by calling the EnableLocalVideo method.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The user ID of the remote user."
      },
      {
        "enabled": "Whether the specified remote user enables/disables local video capturing: true : The video module is enabled. Other users in the channel can see the video of this remote user. false : The video module is disabled. Other users in the channel can no longer receive the video stream from this remote user, while this remote user can still receive the video streams from other users."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onuserenablevideo",
    "name": "OnUserEnableVideo",
    "description": "Occurs when a remote user enables or disables the video module.\n\nOnce the video module is disabled, the user can only use a voice call. The user cannot send or receive any video. The SDK triggers this callback when a remote user enables or disables the video module by calling the EnableVideo or DisableVideo method.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The user ID of the remote user."
      },
      {
        "enabled": "true : The video module is enabled. false : The video module is disabled."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onuserinfoupdated",
    "name": "OnUserInfoUpdated",
    "description": "Occurs when the SDK gets the user ID and user account of the remote user.\n\nAfter a remote user joins the channel, the SDK gets the UID and user account of the remote user, caches them in a mapping table object, and triggers this callback on the local client.",
    "parameters": [
      {
        "uid": "The user ID of the remote user."
      },
      {
        "info": "The UserInfo object that contains the user ID and user account of the remote user. See UserInfo for details."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onuserjoined",
    "name": "OnUserJoined",
    "description": "Occurs when a remote user (in the communication profile)/ host (in the live streaming profile) joins the channel.\n\nIn a communication channel, this callback indicates that a remote user joins the channel. The SDK also triggers this callback to report the existing users in the channel when a user joins the channel.\n In a live-broadcast channel, this callback indicates that a host joins the channel. The SDK also triggers this callback to report the existing hosts in the channel when a host joins the channel. Agora recommends limiting the number of hosts to 17.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The ID of the user or host who joins the channel."
      },
      {
        "elapsed": "Time delay (ms) from the local user calling JoinChannel [2/2] until this callback is triggered."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onusermuteaudio",
    "name": "OnUserMuteAudio",
    "description": "Occurs when a remote user (in the communication profile) or a host (in the live streaming profile) stops/resumes sending the audio stream.\n\nThe SDK triggers this callback when the remote user stops or resumes sending the audio stream by calling the MuteLocalAudioStream method. This callback does not work properly when the number of users (in the communication profile) or hosts (in the live streaming channel) in a channel exceeds 17.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The user ID."
      },
      {
        "muted": "Whether the remote user's audio stream is muted: true : User's audio stream is muted. false : User's audio stream is unmuted."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onusermutevideo",
    "name": "OnUserMuteVideo",
    "description": "Occurs when a remote user stops or resumes publishing the video stream.\n\nWhen a remote user calls MuteLocalVideoStream to stop or resume publishing the video stream, the SDK triggers this callback to report to the local user the state of the streams published by the remote user. This callback can be inaccurate when the number of users (in the communication profile) or hosts (in the live streaming profile) in a channel exceeds 17.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The user ID of the remote user."
      },
      {
        "muted": "Whether the remote user stops publishing the video stream: true : The remote user stops publishing the video stream. false : The remote user resumes publishing the video stream."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onuseroffline",
    "name": "OnUserOffline",
    "description": "Occurs when a remote user (in the communication profile)/ host (in the live streaming profile) leaves the channel.\n\nThere are generally two reasons for users to become offline:\n Leave the channel: When a user/host leaves the channel, the user/host sends a goodbye message.\n Drop offline: When no data packet of the user or host is received for a certain period of time (20 seconds for the communication profile, and more for the live broadcast profile), the SDK assumes that the user/host drops offline. A poor network connection may lead to false detections. It is recommended to use the Agora RTM SDK for reliable offline detection.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "remoteUid": "The ID of the user who leaves the channel or goes offline."
      },
      {
        "reason": "Reasons why a remote user (in the communication profile) or host (in the live streaming profile) goes offline. See USER_OFFLINE_REASON_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onvideodevicestatechanged",
    "name": "OnVideoDeviceStateChanged",
    "description": "Occurs when the video device state changes.\n\nThis callback reports the change of system video devices, such as being unplugged or removed. On a Windows device with an external camera for video capturing, the video disables once the external camera is unplugged. This callback is for Windows and macOS only.",
    "parameters": [
      {
        "deviceId": "The device ID."
      },
      {
        "deviceType": "Media device types. See MEDIA_DEVICE_TYPE."
      },
      {
        "deviceState": "Media device states. See MEDIA_DEVICE_STATE_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onvideopublishstatechanged",
    "name": "OnVideoPublishStateChanged",
    "description": "Occurs when the video publishing state changes.",
    "parameters": [
      {
        "channel": "The channel name."
      },
      {
        "source": "The type of the video source. See VIDEO_SOURCE_TYPE."
      },
      {
        "oldState": "The previous publishing state. See STREAM_PUBLISH_STATE."
      },
      {
        "newState": "The current publishing stat. See STREAM_PUBLISH_STATE."
      },
      {
        "elapseSinceLastState": "The time elapsed (ms) from the previous state to the current state."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onvideorenderingtracingresult",
    "name": "OnVideoRenderingTracingResult",
    "description": "Video frame rendering event callback.\n\nAfter calling the StartMediaRenderingTracing method or joining a channel, the SDK triggers this callback to report the events of video frame rendering and the indicators during the rendering process. Developers can optimize the indicators to improve the efficiency of the first video frame rendering.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "uid": "The user ID."
      },
      {
        "currentEvent": "The current video frame rendering event. See MEDIA_TRACE_EVENT."
      },
      {
        "tracingInfo": "The indicators during the video frame rendering process. Developers need to reduce the value of indicators as much as possible in order to improve the efficiency of the first video frame rendering. See VideoRenderingTracingInfo."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onvideosizechanged",
    "name": "OnVideoSizeChanged",
    "description": "Occurs when the video size or rotation of a specified user changes.",
    "parameters": [
      {
        "connection": "The connection information. See RtcConnection."
      },
      {
        "sourceType": "The type of the video source. See VIDEO_SOURCE_TYPE."
      },
      {
        "uid": "The ID of the user whose video size or rotation changes. (The uid for the local user is 0. The video is the local user's video preview)."
      },
      {
        "width": "The width (pixels) of the video stream."
      },
      {
        "height": "The height (pixels) of the video stream."
      },
      {
        "rotation": "The rotation information. The value range is [0,360). On the iOS platform, the parameter value is always 0."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onvideostopped",
    "name": "OnVideoStopped",
    "description": "Occurs when the video stops playing.\n\nDeprecated: Use LOCAL_VIDEO_STREAM_STATE_STOPPED (0) in the OnLocalVideoStateChanged callback instead. The application can use this callback to change the configuration of the view (for example, displaying other pictures in the view) after the video stops playing.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_irtcengineeventhandler_onvideosubscribestatechanged",
    "name": "OnVideoSubscribeStateChanged",
    "description": "Occurs when the video subscribing state changes.",
    "parameters": [
      {
        "channel": "The channel name."
      },
      {
        "uid": "The user ID of the remote user."
      },
      {
        "oldState": "The previous subscribing status. See STREAM_SUBSCRIBE_STATE."
      },
      {
        "newState": "The current subscribing status. See STREAM_SUBSCRIBE_STATE."
      },
      {
        "elapseSinceLastState": "The time elapsed (ms) from the previous state to the current state."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "callback_ivideoencodedframeobserver_onencodedvideoframereceived",
    "name": "OnEncodedVideoFrameReceived",
    "description": "Reports that the receiver has received the to-be-decoded video frame sent by the remote end.\n\nIf you call the SetRemoteVideoSubscriptionOptions method and set encodedFrameOnly to true, the SDK triggers this callback locally to report the received encoded video frame information.",
    "parameters": [
      {
        "uid": "The user ID of the remote user."
      },
      {
        "imageBufferPtr": "The encoded video image buffer."
      },
      {
        "length": "The data length of the video image."
      },
      {
        "videoEncodedFrameInfo": "For the information of the encoded video frame, see EncodedVideoFrameInfo."
      }
    ],
    "returns": "Without practical meaning.",
    "is_hide": false
  },
  {
    "id": "callback_ivideoframeobserver_oncapturevideoframe",
    "name": "OnCaptureVideoFrame",
    "description": "Occurs each time the SDK receives a video frame captured by local devices.\n\nYou can get raw video data collected by the local device through this callback and preprocess it as needed. Once the preprocessing is complete, you can directly modify videoFrame in this callback, and set the return value to true to send the modified video data to the SDK.",
    "parameters": [
      {
        "sourceType": "Video source types, including cameras, screens, or media player. See VIDEO_SOURCE_TYPE."
      },
      {
        "videoFrame": "The video frame. See VideoFrame."
      }
    ],
    "returns": "true : Sets the SDK to receive the video frame. false : Sets the SDK to discard the video frame.",
    "is_hide": false
  },
  {
    "id": "callback_ivideoframeobserver_onpreencodevideoframe",
    "name": "OnPreEncodeVideoFrame",
    "description": "Occurs each time the SDK receives a video frame before encoding.\n\nAfter you successfully register the video frame observer, the SDK triggers this callback each time it receives a video frame. In this callback, you can get the video data before encoding and then process the data according to your particular scenarios. After processing, you can send the processed video data back to the SDK in this callback.\n It is recommended that you ensure the modified parameters in videoFrame are consistent with the actual situation of the video frames in the video frame buffer. Otherwise, it may cause unexpected rotation, distortion, and other issues in the local preview and remote video display.\n To get the video data captured from the second screen before encoding, you need to set POSITION_PRE_ENCODER (1 << 2) as a frame position through the position parameter of the RegisterVideoFrameObserver method.\n The video data that this callback gets has been preprocessed, with its content cropped and rotated, and the image enhanced.",
    "parameters": [
      {
        "videoFrame": "The video frame. See VideoFrame."
      },
      {
        "sourceType": "The type of the video source. See VIDEO_SOURCE_TYPE."
      }
    ],
    "returns": "true : Sets the SDK to receive the video frame. false : Sets the SDK to discard the video frame.",
    "is_hide": false
  },
  {
    "id": "callback_ivideoframeobserver_onrendervideoframe",
    "name": "OnRenderVideoFrame",
    "description": "Occurs each time the SDK receives a video frame sent by the remote user.\n\nAfter you successfully register the video frame observer, the SDK triggers this callback each time it receives a video frame. In this callback, you can get the video data sent from the remote end before rendering, and then process it according to the particular scenarios. If you use Unity for development, Agora only supports sending video data in YUV format to SDK. Ensure that you set mode as INTPTR when you call the RegisterVideoFrameObserver method to register a video frame observer.",
    "parameters": [
      {
        "videoFrame": "The video frame. See VideoFrame."
      },
      {
        "remoteUid": "The user ID of the remote user who sends the current video frame."
      },
      {
        "channelId": "The channel ID."
      }
    ],
    "returns": "true : Sets the SDK to receive the video frame. false : Sets the SDK to discard the video frame.",
    "is_hide": false
  },
  {
    "id": "callback_videosurface_ontexturesizemodify",
    "name": "OnTextureSizeModify",
    "description": "This callback is triggered when the width and height of Texture are changed.\n\nWhen the width and height of Texture are changed, the SDK triggers this callback.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_Localaccesspointconfiguration",
    "name": "LocalAccessPointConfiguration",
    "description": "The configurations of the Local Access Point.",
    "parameters": [
      {
        "ipList": "The list of IP addresses for the Local Access Point. ipList and domainList must be filled in at least one."
      },
      {
        "ipListSize": "The number of IP addresses of the Local Access Point. This value must be the same as the number of IP addresses that you specify for the ipList parameter."
      },
      {
        "domainList": "The list of domain names for the Local Access Point. The SDK will parse the IP addresses according to the provided domain names. The parse times out after 10 seconds. ipList and domainList must be filled in at least one. If you specify an IP address and a domain name at the same time, the SDK will combine the IP address parsed from the domain name and the IP address you specify, remove duplicates, and then connect to a random IP to achieve load balancing."
      },
      {
        "domainListSize": "The number of domain names for Local Access Point. This value must be the same as the number of domain names that you specifyfor the domainList parameter."
      },
      {
        "verifyDomainName": "The domain name used for Intranet certificate verification. If you pass an empty value, the SDK uses the default domain name secure-edge.local for certificate verification."
      },
      {
        "mode": "The connection mode. See LOCAL_PROXY_MODE."
      },
      {
        "advancedConfig": "The advanced options for the Local Access Point. See AdvancedConfigInfo."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_advancedaudiooptions",
    "name": "AdvancedAudioOptions",
    "description": "The advanced options for audio.",
    "parameters": [
      {
        "audioProcessingChannels": "The number of channels for audio preprocessing. See AUDIO_PROCESSING_CHANNELS."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_advancedconfiginfo",
    "name": "AdvancedConfigInfo",
    "description": "The advanced options for the Local Access Point.",
    "parameters": [
      {
        "logUploadServer": "Custom log upload server. By default, the SDK uploads logs to Agora log server. You can modify the server where logs are uploaded through this parameter. See LogUploadServerInfo."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_advanceoptions",
    "name": "AdvanceOptions",
    "description": "Advanced options for video encoding.",
    "parameters": [
      {
        "encodingPreference": "Video encoder preference. See ENCODING_PREFERENCE."
      },
      {
        "compressionPreference": "Compression preference for video encoding. See COMPRESSION_PREFERENCE."
      },
      {
        "encodeAlpha": "Whether to encode and send the Alpha data present in the video frame to the remote end: true : Encode and send Alpha data. false : (Default) Do not encode and send Alpha data."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_agorarhythmplayerconfig",
    "name": "AgoraRhythmPlayerConfig",
    "description": "The metronome configuration.",
    "parameters": [
      {
        "beatsPerMeasure": "The number of beats per measure, which ranges from 1 to 9. The default value is 4, which means that each measure contains one downbeat and three upbeats."
      },
      {
        "beatsPerMinute": "The beat speed (beats/minute), which ranges from 60 to 360. The default value is 60, which means that the metronome plays 60 beats in one minute."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_audioencodedframeobserverconfig",
    "name": "AudioEncodedFrameObserverConfig",
    "description": "Observer settings for the encoded audio.",
    "parameters": [
      {
        "postionType": "Audio profile. See AUDIO_ENCODED_FRAME_OBSERVER_POSITION."
      },
      {
        "encodingType": "Audio encoding type. See AUDIO_ENCODING_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_audioframe",
    "name": "AudioFrame",
    "description": "Raw audio data.",
    "parameters": [
      {
        "type": "The type of the audio frame. See AUDIO_FRAME_TYPE."
      },
      {
        "samplesPerChannel": "The number of samples per channel in the audio frame."
      },
      {
        "bytesPerSample": "The number of bytes per sample. For PCM, this parameter is generally set to 16 bits (2 bytes)."
      },
      {
        "channels": "The number of audio channels (the data are interleaved if it is stereo).\n 1: Mono.\n 2: Stereo."
      },
      {
        "samplesPerSec": "The number of samples per channel in the audio frame."
      },
      {
        "RawBuffer": "The data buffer of the audio frame. When the audio frame uses a stereo channel, the data buffer is interleaved. The size of the data buffer is as follows: buffer = samples × channels × bytesPerSample."
      },
      {
        "renderTimeMs": "The timestamp (ms) of the external audio frame. You can use this timestamp to restore the order of the captured audio frame, and synchronize audio and video frames in video scenarios, including scenarios where external video sources are used."
      },
      {
        "avsync_type": "Reserved for future use."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_audioparams",
    "name": "AudioParams",
    "description": "Audio data format.\n\nYou can pass the AudioParams object in the following APIs to set the audio data format for the corresponding callback: SetRecordingAudioFrameParameters : Sets the audio data format for the OnRecordAudioFrame callback. SetPlaybackAudioFrameParameters : Sets the audio data format for the OnPlaybackAudioFrame callback. SetMixedAudioFrameParameters : Sets the audio data format for the OnMixedAudioFrame callback. SetEarMonitoringAudioFrameParameters : Sets the audio data format for the OnEarMonitoringAudioFrame callback.\n The SDK calculates the sampling interval through the samplesPerCall, sampleRate, and channel parameters in AudioParams, and triggers the OnRecordAudioFrame, OnPlaybackAudioFrame, OnMixedAudioFrame, and OnEarMonitoringAudioFrame callbacks according to the sampling interval. Sample interval (sec) = samplePerCall /(sampleRate × channel).\n Ensure that the sample interval ≥ 0.01 (s).",
    "parameters": [
      {
        "sample_rate": "The audio sample rate (Hz), which can be set as one of the following values:\n 8000.\n (Default) 16000.\n 32000.\n 44100\n 48000"
      },
      {
        "channels": "The number of audio channels, which can be set as either of the following values:\n 1: (Default) Mono.\n 2: Stereo."
      },
      {
        "mode": "The use mode of the audio data. See RAW_AUDIO_FRAME_OP_MODE_TYPE."
      },
      {
        "samples_per_call": "The number of samples, such as 1024 for the media push."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_audiopcmframe",
    "name": "AudioPcmFrame",
    "description": "The parameters of the audio frame in PCM format.",
    "parameters": [
      {
        "capture_timestamp": "The timestamp (ms) of the audio frame."
      },
      {
        "samples_per_channel_": "The number of samples per channel in the audio frame."
      },
      {
        "sample_rate_hz_": "Audio sample rate (Hz)."
      },
      {
        "num_channels_": "The number of audio channels."
      },
      {
        "bytes_per_sample": "The number of bytes per sample."
      },
      {
        "data_": "The audio frame."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_audiorecordingconfiguration",
    "name": "AudioFileRecordingConfig",
    "description": "Recording configurations.",
    "parameters": [
      {
        "filePath": "The absolute path (including the filename extensions) of the recording file. For example: C:\\music\\audio.aac. Ensure that the directory for the log files exists and is writable."
      },
      {
        "encode": "Whether to encode the audio data: true : Encode audio data in AAC. false : (Default) Do not encode audio data, but save the recorded audio data directly."
      },
      {
        "sampleRate": "Recording sample rate (Hz).\n 16000\n (Default) 32000\n 44100\n 48000 If you set this parameter to 44100 or 48000, Agora recommends recording WAV files, or AAC files with quality set as AUDIO_RECORDING_QUALITY_MEDIUM or AUDIO_RECORDING_QUALITY_HIGH for better recording quality."
      },
      {
        "fileRecordingType": "The recording content. See AUDIO_FILE_RECORDING_TYPE."
      },
      {
        "quality": "Recording quality. See AUDIO_RECORDING_QUALITY_TYPE. This parameter applies to AAC files only."
      },
      {
        "recordingChannel": "The audio channel of recording: The parameter supports the following values:\n 1: (Default) Mono.\n 2: Stereo. The actual recorded audio channel is related to the audio channel that you capture.\n If the captured audio is mono and recordingChannel is 2, the recorded audio is the dual-channel data that is copied from mono data, not stereo.\n If the captured audio is dual channel and recordingChannel is 1, the recorded audio is the mono data that is mixed by dual-channel data. The integration scheme also affects the final recorded audio channel. If you need to record in stereo, contact."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_audiospectrumdata",
    "name": "AudioSpectrumData",
    "description": "The audio spectrum data.",
    "parameters": [
      {
        "audioSpectrumData": "The audio spectrum data. Agora divides the audio frequency into 256 frequency domains, and reports the energy value of each frequency domain through this parameter. The value range of each energy type is [-300, 1] and the unit is dBFS."
      },
      {
        "dataLength": "The audio spectrum data length is 256."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_audiotrackconfig",
    "name": "AudioTrackConfig",
    "description": "The configuration of custom audio tracks.",
    "parameters": [
      {
        "enableLocalPlayback": "Whether to enable the local audio-playback device: true : (Default) Enable the local audio-playback device. false : Do not enable the local audio-playback device."
      },
      {
        "enableAudioProcessing": "Whether to enable audio processing module: true Enable the audio processing module to apply the Automatic Echo Cancellation (AEC), Automatic Noise Suppression (ANS), and Automatic Gain Control (AGC) effects. false : (Default) Do not enable the audio processing module. This parameter only takes effect on AUDIO_TRACK_DIRECT in custom audio capturing."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_audiovolumeinfo",
    "name": "AudioVolumeInfo",
    "description": "The volume information of users.",
    "parameters": [
      {
        "uid": "The user ID.\n In the local user's callback, uid is 0.\n In the remote users' callback, uid is the user ID of a remote user whose instantaneous volume is the highest."
      },
      {
        "volume": "The volume of the user. The value ranges between 0 (the lowest volume) and 255 (the highest volume). If the local user enables audio capturing and calls MuteLocalAudioStream and set it as true to mute, the value of volume indicates the volume of locally captured audio signal."
      },
      {
        "vad": "Voice activity status of the local user.\n 0: The local user is not speaking.\n 1: The local user is speaking.\n The vad parameter does not report the voice activity status of remote users. In a remote user's callback, the value of vad is always 1.\n To use this parameter, you must set reportVad to true when calling EnableAudioVolumeIndication."
      },
      {
        "voicePitch": "The voice pitch of the local user. The value ranges between 0.0 and 4000.0. The voicePitch parameter does not report the voice pitch of remote users. In the remote users' callback, the value of voicePitch is always 0.0."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_beautyoptions",
    "name": "BeautyOptions",
    "description": "Image enhancement options.",
    "parameters": [
      {
        "lighteningContrastLevel": "The contrast level, used with the lighteningLevel parameter. The larger the value, the greater the contrast between light and dark. See LIGHTENING_CONTRAST_LEVEL."
      },
      {
        "lighteningLevel": "The brightening level, in the range [0.0,1.0], where 0.0 means the original brightening. The default value is 0.0. The higher the value, the greater the degree of brightening."
      },
      {
        "smoothnessLevel": "The smoothness level, in the range [0.0,1.0], where 0.0 means the original smoothness. The default value is 0.0. The greater the value, the greater the smoothness level."
      },
      {
        "rednessLevel": "The redness level, in the range [0.0,1.0], where 0.0 means the original redness. The default value is 0.0. The larger the value, the greater the redness level."
      },
      {
        "sharpnessLevel": "The sharpness level, in the range [0.0,1.0], where 0.0 means the original sharpness. The default value is 0.0. The larger the value, the greater the sharpness level."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_cachestatistics",
    "name": "CacheStatistics",
    "description": "Statistics about the media files being cached.",
    "parameters": [
      {
        "fileSize": "The size (bytes) of the media file being played."
      },
      {
        "cacheSize": "The size (bytes) of the media file that you want to cache."
      },
      {
        "downloadSize": "The size (bytes) of the media file that has been downloaded."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_cameracapturerconfiguration",
    "name": "CameraCapturerConfiguration",
    "description": "The camera capturer preference.",
    "parameters": [
      {
        "cameraDirection": "(Optional) The camera direction. See CAMERA_DIRECTION. This parameter is for Android and iOS only."
      },
      {
        "cameraId": "(Optional) The camera ID. The default value is the camera ID of the front camera. You can get the camera ID through the Android native system API, see and for details.\n This parameter is for Android only.\n This parameter and cameraDirection are mutually exclusive in specifying the camera; you can choose one based on your needs. The differences are as follows:\n Specifying the camera via cameraDirection is more straightforward. You only need to indicate the camera direction (front or rear), without specifying a specific camera ID; the SDK will retrieve and confirm the actual camera ID through Android native system APIs.\n Specifying via cameraId allows for more precise identification of a particular camera. For devices with multiple cameras, where cameraDirection cannot recognize or access all available cameras, it is recommended to use cameraId to specify the desired camera ID directly."
      },
      {
        "cameraFocalLengthType": "(Optional) The camera focal length type. See CAMERA_FOCAL_LENGTH_TYPE.\n This parameter is for Android and iOS only.\n To set the focal length type of the camera, it is only supported to specify the camera through cameraDirection, and not supported to specify it through cameraId.\n For iOS devices equipped with multi-lens rear cameras, such as those featuring dual-camera (wide-angle and ultra-wide-angle) or triple-camera (wide-angle, ultra-wide-angle, and telephoto), you can use one of the following methods to capture video with an ultra-wide-angle perspective:\n Method one: Set this parameter to CAMERA_FOCAL_LENGTH_ULTRA_WIDE (2) (ultra-wide lens).\n Method two: Set this parameter to CAMERA_FOCAL_LENGTH_DEFAULT (0) (standard lens), then call SetCameraZoomFactor to set the camera's zoom factor to a value less than 1.0, with the minimum setting being 0.5. The difference is that the size of the ultra-wide angle in method one is not adjustable, whereas method two supports adjusting the camera's zoom factor freely."
      },
      {
        "format": "(Optional) The format of the video frame. See VideoFormat."
      },
      {
        "deviceId": "The camera ID. This parameter is for Windows and macOS only."
      },
      {
        "followEncodeDimensionRatio": "(Optional) Whether to follow the video aspect ratio set in SetVideoEncoderConfiguration : true : (Default) Follow the set video aspect ratio. The SDK crops the captured video according to the set video aspect ratio and synchronously changes the local preview screen and the video frame in OnCaptureVideoFrame and OnPreEncodeVideoFrame. false : Do not follow the system default audio playback device. The SDK does not change the aspect ratio of the captured video frame."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_channelmediainfo",
    "name": "ChannelMediaInfo",
    "description": "Channel media information.",
    "parameters": [
      {
        "channelName": "The channel name."
      },
      {
        "token": "The token that enables the user to join the channel."
      },
      {
        "uid": "The user ID."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_channelmediaoptions",
    "name": "ChannelMediaOptions",
    "description": "The channel media options.\n\nAgora supports publishing multiple audio streams and one video stream at the same time and in the same RtcConnection. For example, publishMicrophoneTrack, publishCustomAudioTrack, and publishMediaPlayerAudioTrack can be set as true at the same time, but only one of publishCameraTrack, publishScreenCaptureVideo, publishScreenTrack, publishCustomVideoTrack, or publishEncodedVideoTrack can be set as true. Agora recommends that you set member parameter values yourself according to your business scenario, otherwise the SDK will automatically assign values to member parameters.",
    "parameters": [
      {
        "audienceLatencyLevel": "The latency level of an audience member in interactive live streaming. See AUDIENCE_LATENCY_LEVEL_TYPE."
      },
      {
        "publishCameraTrack": "Whether to publish the video captured by the camera: true : Publish the video captured by the camera. false : Do not publish the video captured by the camera."
      },
      {
        "publishSecondaryCameraTrack": "Whether to publish the video captured by the second camera: true : Publish the video captured by the second camera. false : Do not publish the video captured by the second camera."
      },
      {
        "publishMicrophoneTrack": "Whether to publish the audio captured by the microphone: true : Publish the audio captured by the microphone. false : Do not publish the audio captured by the microphone."
      },
      {
        "publishThirdCameraTrack": "Whether to publish the video captured by the third camera: true : Publish the video captured by the third camera. false : Do not publish the video captured by the third camera. This parameter is for Android, Windows and macOS only."
      },
      {
        "publishFourthCameraTrack": "Whether to publish the video captured by the fourth camera: true : Publish the video captured by the fourth camera. false : Do not publish the video captured by the fourth camera. This parameter is for Android, Windows and macOS only."
      },
      {
        "publishScreenTrack": "Whether to publish the video captured from the screen: true : Publish the video captured from the screen. false : Do not publish the video captured from the screen. This is for Windows and macOS only."
      },
      {
        "publishScreenCaptureVideo": "Whether to publish the video captured from the screen: true : Publish the video captured from the screen. false : Do not publish the video captured from the screen. This parameter is for Android and iOS only."
      },
      {
        "publishScreenCaptureAudio": "Whether to publish the audio captured from the screen: true : Publish the audio captured from the screen. false : Publish the audio captured from the screen. This parameter is for Android and iOS only."
      },
      {
        "publishSecondaryScreenTrack": "Whether to publish the video captured from the second screen: true : Publish the video captured from the second screen. false : Do not publish the video captured from the second screen."
      },
      {
        "publishThirdScreenTrack": "Whether to publish the video captured from the third screen: true : Publish the captured video from the third screen. false : Do not publish the video captured from the third screen. This is for Windows and macOS only."
      },
      {
        "publishFourthScreenTrack": "Whether to publish the video captured from the fourth screen: true : Publish the captured video from the fourth screen. false : Do not publish the video captured from the fourth screen. This is for Windows and macOS only."
      },
      {
        "publishTranscodedVideoTrack": "Whether to publish the local transcoded video: true : Publish the local transcoded video. false : Do not publish the local transcoded video."
      },
      {
        "publishMixedAudioTrack": "Whether to publish the mixed audio track: true : Publish the mixed audio track. false : Do not publish the mixed audio track."
      },
      {
        "publishCustomAudioTrack": "Whether to publish the audio captured from a custom source: true : Publish the audio captured from the custom source. false : Do not publish the captured audio from a custom source."
      },
      {
        "publishCustomAudioTrackId": "The ID of the custom audio track to be published. The default value is 0. You can obtain the custom audio track ID through the CreateCustomAudioTrack method."
      },
      {
        "publishCustomVideoTrack": "Whether to publish the video captured from a custom source: true : Publish the video captured from the custom source. false : Do not publish the captured video from a custom source."
      },
      {
        "publishEncodedVideoTrack": "Whether to publish the encoded video: true : Publish the encoded video. false : Do not publish the encoded video."
      },
      {
        "publishMediaPlayerAudioTrack": "Whether to publish the audio from the media player: true : Publish the audio from the media player. false : Do not publish the audio from the media player."
      },
      {
        "publishMediaPlayerVideoTrack": "Whether to publish the video from the media player: true : Publish the video from the media player. false : Do not publish the video from the media player."
      },
      {
        "autoSubscribeAudio": "Whether to automatically subscribe to all remote audio streams when the user joins a channel: true : Subscribe to all remote audio streams. false : Do not automatically subscribe to any remote audio streams."
      },
      {
        "autoSubscribeVideo": "Whether to automatically subscribe to all remote video streams when the user joins the channel: true : Subscribe to all remote video streams. false : Do not automatically subscribe to any remote video streams."
      },
      {
        "enableAudioRecordingOrPlayout": "Whether to enable audio capturing or playback: true : Enable audio capturing or playback. false : Do not enable audio capturing or playback. If you need to publish the audio streams captured by your microphone, ensure this parameter is set as true."
      },
      {
        "publishMediaPlayerId": "The ID of the media player to be published. The default value is 0."
      },
      {
        "clientRoleType": "The user role. See CLIENT_ROLE_TYPE. If you set the user role as an audience member, you cannot publish audio and video streams in the channel. If you want to publish media streams in a channel during live streaming, ensure you set the user role as broadcaster."
      },
      {
        "defaultVideoStreamType": "The default video-stream type. See VIDEO_STREAM_TYPE."
      },
      {
        "channelProfile": "The channel profile. See CHANNEL_PROFILE_TYPE."
      },
      {
        "audioDelayMs": "Delay (in milliseconds) for sending audio frames. You can use this parameter to set the delay of the audio frames that need to be sent, to ensure audio and video synchronization. To switch off the delay, set the value to 0."
      },
      {
        "token": "(Optional) The token generated on your server for authentication.\n This parameter takes effect only when calling UpdateChannelMediaOptions or UpdateChannelMediaOptionsEx.\n Ensure that the App ID, channel name, and user name used for creating the token are the same as those used by the Initialize method for initializing the RTC engine, and those used by the JoinChannel [2/2] and JoinChannelEx methods for joining the channel."
      },
      {
        "publishRhythmPlayerTrack": "Whether to publish the sound of a metronome to remote users: true : Publish processed audio frames. Both the local user and remote users can hear the metronome. false : Do not publish the sound of the metronome. Only the local user can hear the metronome."
      },
      {
        "isInteractiveAudience": "Whether to enable interactive mode: true : Enable interactive mode. Once this mode is enabled and the user role is set as audience, the user can receive remote video streams with low latency. false :Do not enable interactive mode. If this mode is disabled, the user receives the remote video streams in default settings.\n This parameter only applies to co-streaming scenarios. The cohosts need to call the JoinChannelEx method to join the other host's channel as an audience member, and set isInteractiveAudience to true.\n This parameter takes effect only when the user role is CLIENT_ROLE_AUDIENCE."
      },
      {
        "customVideoTrackId": "The video track ID returned by calling the CreateCustomVideoTrack method. The default value is 0."
      },
      {
        "isAudioFilterable": "Whether the audio stream being published is filtered according to the volume algorithm: true : The audio stream is filtered. If the audio stream filter is not enabled, this setting does not takes effect. false : The audio stream is not filtered. If you need to enable this function, contact."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_channelmediarelayconfiguration",
    "name": "ChannelMediaRelayConfiguration",
    "description": "Configuration of cross channel media relay.",
    "parameters": [
      {
        "srcInfo": "The information of the source channel. See ChannelMediaInfo. It contains the following members: channelName : The name of the source channel. The default value is NULL, which means the SDK applies the name of the current channel. token : The token for joining the source channel. This token is generated with the channelName and uid you set in srcInfo.\n If you have not enabled the App Certificate, set this parameter as the default value NULL, which means the SDK applies the App ID.\n If you have enabled the App Certificate, you must use the token generated with the channelName and uid, and the uid must be set as 0. uid : The unique user ID to identify the relay stream in the source channel. Agora recommends leaving the default value of 0 unchanged."
      },
      {
        "destInfos": "The information of the target channel ChannelMediaInfo. It contains the following members: channelName : The name of the target channel. token : The token for joining the target channel. It is generated with the channelName and uid you set in destInfos.\n If you have not enabled the App Certificate, set this parameter as the default value NULL, which means the SDK applies the App ID.\n If you have enabled the App Certificate, you must use the token generated with the channelName and uid. If the token of any target channel expires, the whole media relay stops; hence Agora recommends that you specify the same expiration time for the tokens of all the target channels. uid : The unique user ID to identify the relay stream in the target channel. The value ranges from 0 to (2 32 -1). To avoid user ID conflicts, this user ID must be different from any other user ID in the target channel. The default value is 0, which means the SDK generates a random UID."
      },
      {
        "destCount": "The number of target channels. The default value is 0, and the value range is from 0 to 6. Ensure that the value of this parameter corresponds to the number of ChannelMediaInfo structs you define in destInfo."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_clientroleoptions",
    "name": "ClientRoleOptions",
    "description": "Setting of user role properties.",
    "parameters": [
      {
        "audienceLatencyLevel": "The latency level of an audience member in interactive live streaming. See AUDIENCE_LATENCY_LEVEL_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_climaxsegment",
    "name": "ClimaxSegment",
    "description": "The climax parts of the music.",
    "parameters": [
      {
        "startTimeMs": "The time (ms) when the climax part begins."
      },
      {
        "endTimeMs": "The time (ms) when the climax part ends."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_cloudspatialaudioconfig",
    "name": "CloudSpatialAudioConfig",
    "description": "The configuration of ICloudSpatialAudioEngine.",
    "parameters": [
      {
        "rtcEngine": "IRtcEngine."
      },
      {
        "eventHandler": "The event handler for the ICloudSpatialAudioEngine object. See ICloudSpatialAudioEventHandler."
      },
      {
        "appId": "The App ID issued by Agora for your project. This parameter needs to be the same as the App ID set during the initialization of IRtcEngine."
      },
      {
        "deployRegion": "The region in which the Agora Spatial Audio Server to be used is located. The following regions are supported: SAE_DEPLOY_REGION_CN : (Default) Mainland China. SAE_DEPLOY_REGION_NA : North America. SAE_DEPLOY_REGION_EU : Europe. SAE_DEPLOY_REGION_AS Asia, excluding Mainland China. After specifying the region, apps using the spatial audio effect connect to the Agora Spatial Audio Server within that region."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_codeccapinfo",
    "name": "CodecCapInfo",
    "description": "The codec capability of the SDK.",
    "parameters": [
      {
        "codecType": "The video codec types. See VIDEO_CODEC_TYPE."
      },
      {
        "codecCapMask": "Bit mask of the codec types in SDK. See CODEC_CAP_MASK."
      },
      {
        "codecLevels": "Codec capability of the SDK. See CodecCapLevels."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_codeccaplevels",
    "name": "CodecCapLevels",
    "description": "The level of the codec capability.",
    "parameters": [
      {
        "hwDecodingLevel": "Hardware decoding capability level, which represents the device's ability to perform hardware decoding on videos of different quality. See VIDEO_CODEC_CAPABILITY_LEVEL."
      },
      {
        "swDecodingLevel": "Software decoding capability level, which represents the device's ability to perform software decoding on videos of different quality. See VIDEO_CODEC_CAPABILITY_LEVEL."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_colorenhanceoptions",
    "name": "ColorEnhanceOptions",
    "description": "The color enhancement options.",
    "parameters": [
      {
        "strengthLevel": "The level of color enhancement. The value range is [0.0, 1.0]. 0.0 is the default value, which means no color enhancement is applied to the video. The higher the value, the higher the level of color enhancement. The default value is 0.5."
      },
      {
        "skinProtectLevel": "The level of skin tone protection. The value range is [0.0, 1.0]. 0.0 means no skin tone protection. The higher the value, the higher the level of skin tone protection. The default value is 1.0.\n When the level of color enhancement is higher, the portrait skin tone can be significantly distorted, so you need to set the level of skin tone protection.\n When the level of skin tone protection is higher, the color enhancement effect can be slightly reduced. Therefore, to get the best color enhancement effect, Agora recommends that you adjust strengthLevel and skinProtectLevel to get the most appropriate values."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_contentinspectconfig",
    "name": "ContentInspectConfig",
    "description": "Screenshot and upload configuration.",
    "parameters": [
      {
        "extraInfo": "Additional information on the video content (maximum length: 1024 Bytes). The SDK sends the screenshots and additional information on the video content to the Agora server. Once the video screenshot and upload process is completed, the Agora server sends the additional information and the callback notification to your server."
      },
      {
        "serverConfig": "(Optional) Server configuration related to uploading video screenshots via extensions from Agora Extensions Marketplace. This parameter only takes effect when type in ContentInspectModule is set to CONTENT_INSPECT_IMAGE_MODERATION. If you want to use it, contact."
      },
      {
        "modules": "Functional module. See ContentInspectModule. A maximum of 32 ContentInspectModule instances can be configured, and the value range of MAX_CONTENT_INSPECT_MODULE_COUNT is an integer in [1,32]. A function module can only be configured with one instance at most. Currently only the video screenshot and upload function is supported."
      },
      {
        "moduleCount": "The number of functional modules, that is,the number of configured ContentInspectModule instances, must be the same as the number of instances configured in modules. The maximum number is 32."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_contentinspectmodule",
    "name": "ContentInspectModule",
    "description": "ContentInspectModule A structure used to configure the frequency of video screenshot and upload.",
    "parameters": [
      {
        "type": "Types of functional module. See CONTENT_INSPECT_TYPE."
      },
      {
        "interval": "The frequency (s) of video screenshot and upload. The value should be set as larger than 0. The default value is 0, the SDK does not take screenshots. Agora recommends that you set the value as 10; you can also adjust it according to your business needs."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_datastreamconfig",
    "name": "DataStreamConfig",
    "description": "The configurations for the data stream.\n\nThe following table shows the SDK behaviors under different parameter settings:",
    "parameters": [
      {
        "syncWithAudio": "Whether to synchronize the data packet with the published audio packet. true : Synchronize the data packet with the audio packet. This setting is suitable for special scenarios such as lyrics synchronization. false : Do not synchronize the data packet with the audio packet. This setting is suitable for scenarios where data packets need to arrive at the receiving end immediately. When you set the data packet to synchronize with the audio, then if the data packet delay is within the audio delay, the SDK triggers the OnStreamMessage callback when the synchronized audio packet is played out."
      },
      {
        "ordered": "Whether the SDK guarantees that the receiver receives the data in the sent order. true : Guarantee that the receiver receives the data in the sent order. false : Do not guarantee that the receiver receives the data in the sent order. Do not set this parameter as true if you need the receiver to receive the data packet immediately."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_deviceinfo",
    "name": "DeviceInfoMobile",
    "description": "The audio device information.\n\nThis class is for Android only.",
    "parameters": [
      {
        "isLowLatencyAudioSupported": "Whether the audio device supports ultra-low-latency capture and playback: true : The device supports ultra-low-latency capture and playback. false : The device does not support ultra-low-latency capture and playback."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_directcdnstreamingmediaoptions",
    "name": "DirectCdnStreamingMediaOptions",
    "description": "The media setting options for the host.",
    "parameters": [
      {
        "publishCameraTrack": "Sets whether to publish the video captured by the camera: true : Publish the video captured by the camera. false : (Default) Do not publish the video captured by the camera."
      },
      {
        "publishMicrophoneTrack": "Sets whether to publish the audio captured by the microphone: true : Publish the audio captured by the microphone. false : (Default) Do not publish the audio captured by the microphone."
      },
      {
        "publishCustomAudioTrack": "Sets whether to publish the captured audio from a custom source: true : Publish the captured audio from a custom source. false : (Default) Do not publish the captured audio from the custom source."
      },
      {
        "publishCustomVideoTrack": "Sets whether to publish the captured video from a custom source: true : Publish the captured video from a custom source. false : (Default) Do not publish the captured video from the custom source."
      },
      {
        "customVideoTrackId": "The video track ID returned by calling the CreateCustomVideoTrack method. The default value is 0."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_directcdnstreamingstats",
    "name": "DirectCdnStreamingStats",
    "description": "The statistics of the current CDN streaming.",
    "parameters": [
      {
        "videoWidth": "The width (px) of the video frame."
      },
      {
        "videoHeight": "The height (px) of the video frame."
      },
      {
        "fps": "The frame rate (fps) of the current video frame."
      },
      {
        "videoBitrate": "The bitrate (bps) of the current video frame."
      },
      {
        "audioBitrate": "The bitrate (bps) of the current audio frame."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_echotestconfiguration",
    "name": "EchoTestConfiguration",
    "description": "The configuration of the audio and video call loop test.",
    "parameters": [
      {
        "view": "The view used to render the local user's video. This parameter is only applicable to scenarios testing video devices, that is, when enableVideo is true."
      },
      {
        "enableAudio": "Whether to enable the audio device for the loop test: true : (Default) Enable the audio device. To test the audio device, set this parameter as true. false : Disable the audio device."
      },
      {
        "enableVideo": "Whether to enable the video device for the loop test. Currently, video device loop test is not supported. Please set this parameter to false."
      },
      {
        "token": "The token used to secure the audio and video call loop test. If you do not enable App Certificate in Agora Console, you do not need to pass a value in this parameter; if you have enabled App Certificate in Agora Console, you must pass a token in this parameter; the uid used when you generate the token must be 0xFFFFFFFF, and the channel name used must be the channel name that identifies each audio and video call loop tested. For server-side token generation, see."
      },
      {
        "channelId": "The channel name that identifies each audio and video call loop. To ensure proper loop test functionality, the channel name passed in to identify each loop test cannot be the same when users of the same project (App ID) perform audio and video call loop tests on different devices."
      },
      {
        "intervalInSeconds": "Set the time interval or delay for returning the results of the audio and video loop test. The value range is [2,10], in seconds, with the default value being 2 seconds.\n For audio loop tests, the test results will be returned according to the time interval you set.\n For video loop tests, the video will be displayed in a short time, after which the delay will gradually increase until it reaches the delay you set."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_encodedaudioframeinfo",
    "name": "EncodedAudioFrameInfo",
    "description": "Audio information after encoding.",
    "parameters": [
      {
        "codec": "Audio Codec type: AUDIO_CODEC_TYPE."
      },
      {
        "sampleRateHz": "Audio sample rate (Hz)."
      },
      {
        "samplesPerChannel": "The number of audio samples per channel."
      },
      {
        "numberOfChannels": "The number of audio channels."
      },
      {
        "advancedSettings": "This function is currently not supported."
      },
      {
        "captureTimeMs": "The Unix timestamp (ms) for capturing the external encoded video frames."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_encodedvideoframeinfo",
    "name": "EncodedVideoFrameInfo",
    "description": "Information about externally encoded video frames.",
    "parameters": [
      {
        "codecType": "The codec type of the local video stream. See VIDEO_CODEC_TYPE. The default value is VIDEO_CODEC_H264 (2)."
      },
      {
        "width": "Width (pixel) of the video frame."
      },
      {
        "height": "Height (pixel) of the video frame."
      },
      {
        "framesPerSecond": "The number of video frames per second. When this parameter is not 0, you can use it to calculate the Unix timestamp of externally encoded video frames."
      },
      {
        "frameType": "The video frame type. See VIDEO_FRAME_TYPE."
      },
      {
        "rotation": "The rotation information of the video frame. See VIDEO_ORIENTATION."
      },
      {
        "trackId": "Reserved for future use."
      },
      {
        "captureTimeMs": "The Unix timestamp (ms) for capturing the external encoded video frames."
      },
      {
        "uid": "The user ID to push the externally encoded video frame."
      },
      {
        "streamType": "The type of video streams. See VIDEO_STREAM_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_encryptionconfig",
    "name": "EncryptionConfig",
    "description": "Built-in encryption configurations.",
    "parameters": [
      {
        "encryptionMode": "The built-in encryption mode. See ENCRYPTION_MODE. Agora recommends using AES_128_GCM2 or AES_256_GCM2 encrypted mode. These two modes support the use of salt for higher security."
      },
      {
        "encryptionKey": "Encryption key in string type with unlimited length. Agora recommends using a 32-byte key. If you do not set an encryption key or set it as NULL, you cannot use the built-in encryption, and the SDK returns -2."
      },
      {
        "encryptionKdfSalt": "Salt, 32 bytes in length. Agora recommends that you use OpenSSL to generate salt on the server side. See Media Stream Encryption for details. This parameter takes effect only in AES_128_GCM2 or AES_256_GCM2 encrypted mode. In this case, ensure that this parameter is not 0."
      },
      {
        "datastreamEncryptionEnabled": "Whether to enable data stream encryption: true : Enable data stream encryption. false : (Default) Disable data stream encryption."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_extensioncontext",
    "name": "ExtensionContext",
    "description": "The context information of the extension.",
    "parameters": [
      {
        "isValid": "Whether the uid in ExtensionContext is valid: true : The uid is valid. false : The uid is invalid."
      },
      {
        "uid": "The user ID. 0 represents a local user, while greater than 0 represents a remote user."
      },
      {
        "providerName": "The name of the extension provider."
      },
      {
        "extensionName": "The name of the extension."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_externalvideoframe",
    "name": "ExternalVideoFrame",
    "description": "The external video frame.",
    "parameters": [
      {
        "colorSpace": "By default, the color space properties of video frames will apply the Full Range and BT.709 standard configurations. You can configure the settings according your needs for custom video capturing and rendering."
      },
      {
        "alphaStitchMode": "When the video frame contains alpha channel data, it represents the relative position of alphaBuffer and the video frame. See ALPHA_STITCH_MODE."
      },
      {
        "type": "The video type. See VIDEO_BUFFER_TYPE."
      },
      {
        "format": "The pixel format. See VIDEO_PIXEL_FORMAT."
      },
      {
        "buffer": "Video frame buffer."
      },
      {
        "stride": "Line spacing of the incoming video frame, which must be in pixels instead of bytes. For textures, it is the width of the texture."
      },
      {
        "height": "Height of the incoming video frame."
      },
      {
        "eglContext": "This parameter only applies to video data in Texture format.\n When using the OpenGL interface (javax.microedition.khronos.egl.*) defined by Khronos, set eglContext to this field.\n When using the OpenGL interface (android.opengl.*) defined by Android, set eglContext to this field."
      },
      {
        "eglType": "This parameter only applies to video data in Texture format. Texture ID of the video frame."
      },
      {
        "textureId": "This parameter only applies to video data in Texture format. Incoming 4 × 4 transformational matrix. The typical value is a unit matrix."
      },
      {
        "metadataBuffer": "This parameter only applies to video data in Texture format. The MetaData buffer. The default value is NULL."
      },
      {
        "metadataSize": "This parameter only applies to video data in Texture format. The MetaData size. The default value is 0."
      },
      {
        "d3d11Texture2d": "This parameter only applies to video data in Windows Texture format. It represents a pointer to an object of type ID3D11Texture2D, which is used by a video frame."
      },
      {
        "alphaBuffer": "The alpha channel data output by using portrait segmentation algorithm. This data matches the size of the video frame, with each pixel value ranging from [0,255], where 0 represents the background and 255 represents the foreground (portrait). By setting this parameter, you can render the video background into various effects, such as transparent, solid color, image, video, etc. In custom video rendering scenarios, ensure that both the video frame and alphaBuffer are of the Full Range type; other types may cause abnormal alpha data rendering."
      },
      {
        "fillAlphaBuffer": "This parameter only applies to video data in BGRA or RGBA format. Whether to extract the alpha channel data from the video frame and automatically fill it into alphaBuffer : true ：Extract and fill the alpha channel data. false : (Default) Do not extract and fill the Alpha channel data. For video data in BGRA or RGBA format, you can set the Alpha channel data in either of the following ways:\n Automatically by setting this parameter to true.\n Manually through the alphaBuffer parameter."
      },
      {
        "textureSliceIndex": "This parameter only applies to video data in Windows Texture format. It represents an index of an ID3D11Texture2D texture object used by the video frame in the ID3D11Texture2D array."
      },
      {
        "cropLeft": "Raw data related parameter. The number of pixels trimmed from the left. The default value is 0."
      },
      {
        "cropTop": "Raw data related parameter. The number of pixels trimmed from the top. The default value is 0."
      },
      {
        "cropRight": "Raw data related parameter. The number of pixels trimmed from the right. The default value is 0."
      },
      {
        "cropBottom": "Raw data related parameter. The number of pixels trimmed from the bottom. The default value is 0."
      },
      {
        "rotation": "Raw data related parameter. The clockwise rotation of the video frame. You can set the rotation angle as 0, 90, 180, or 270. The default value is 0."
      },
      {
        "timestamp": "Timestamp (ms) of the incoming video frame. An incorrect timestamp results in frame loss or unsynchronized audio and video."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_filtereffectoptions",
    "name": "FilterEffectOptions",
    "description": "Filter effect options.",
    "parameters": [
      {
        "path": "The absolute path to the local cube map texture file, which can be used to customize the filter effect. The specified .cude file should strictly follow the Cube LUT Format Specification; otherwise, the filter options do not take effect. The following is a sample of the .cude file:\nLUT_3D_SIZE 32\n0.0039215689 0 0.0039215682\n0.0086021447 0.0037950677 0\n...\n0.0728652592 0.0039215689 0\n The identifier LUT_3D_SIZE on the first line of the cube map file represents the size of the three-dimensional lookup table. The LUT size for filter effect can only be set to 32.\n The SDK provides a built-in built_in_whiten_filter.cube file. You can pass the absolute path of this file to get the whitening filter effect."
      },
      {
        "strength": "The intensity of the filter effect, with a range value of [0.0,1.0], in which 0.0 represents no filter effect. The default value is 0.5. The higher the value, the stronger the filter effect."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_focallengthinfo",
    "name": "FocalLengthInfo",
    "description": "Focal length information supported by the camera, including the camera direction and focal length type.\n\nThis enumeration class applies to Android and iOS only.",
    "parameters": [
      {
        "cameraDirection": "The camera direction. See CAMERA_DIRECTION."
      },
      {
        "focalLengthType": "The focal length type. See CAMERA_FOCAL_LENGTH_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_iaudiodevicemanager",
    "name": "IAudioDeviceManager",
    "description": "Audio device management methods.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_iaudioencodedframeobserver",
    "name": "IAudioEncodedFrameObserver",
    "description": "The encoded audio observer.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_iaudioframeobserver",
    "name": "IAudioFrameObserver",
    "description": "The audio frame observer.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_iaudiopcmframesink",
    "name": "IAudioPcmFrameSink",
    "description": "This class is used to get raw PCM audio.\n\nYou can inherit this class and implement the OnFrame callback to get raw PCM audio.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_iaudiospectrumobserver",
    "name": "IAudioSpectrumObserver",
    "description": "The audio spectrum observer.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_icloudspatialaudioengine",
    "name": "ICloudSpatialAudioEngine",
    "description": "This class calculates user positions through the Agora Spatial Audio Server to implement the spatial audio effect.\n\nThis class inherits from. Before calling other APIs in this class, you need to call the Initialize method to initialize this class.",
    "parameters": [],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_icloudspatialaudioeventhandler",
    "name": "ICloudSpatialAudioEventHandler",
    "description": "The class that sends event notifications relating to the spatial audio effect.",
    "parameters": [],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_ifaceinfoobserver",
    "name": "IFaceInfoObserver",
    "description": "Facial information observer.\n\nYou can call RegisterFaceInfoObserver to register one IFaceInfoObserver observer.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_ilocalspatialaudioengine",
    "name": "ILocalSpatialAudioEngine",
    "description": "This class calculates user positions through the SDK to implement the spatial audio effect.\n\nBefore calling other APIs in this class, you need to call the Initialize method to initialize this class.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_imagetrackoptions",
    "name": "ImageTrackOptions",
    "description": "Image configurations.",
    "parameters": [
      {
        "imageUrl": "The image URL. Supported formats of images include JPEG, JPG, PNG and GIF. This method supports adding an image from the local absolute or relative file path. On the Android platform, adding images from /assets/ is not supported."
      },
      {
        "fps": "The frame rate of the video streams being published. The value range is [1,30]. The default value is 1."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_imediaplayer",
    "name": "IMediaPlayer",
    "description": "This class provides media player functions and supports multiple instances.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_imediaplayeraudioframeobserver",
    "name": "IMediaPlayerAudioFrameObserver",
    "description": "The audio frame observer for the media player.\n\nYou can call RegisterAudioFrameObserver [2/2] to register or unregister the IMediaPlayerAudioFrameObserver object.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_imediaplayercachemanager",
    "name": "IMediaPlayerCacheManager",
    "description": "This class provides methods to manage cached media files.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_imediaplayercustomdataprovider",
    "name": "IMediaPlayerCustomDataProvider",
    "description": "The callback for custom media resource files.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_imediaplayersourceobserver",
    "name": "IMediaPlayerSourceObserver",
    "description": "Provides callbacks for media players.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_imediarecorder",
    "name": "IMediaRecorder",
    "description": "This class provides APIs for local and remote recording.",
    "parameters": [],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_imediarecorderobserver",
    "name": "IMediaRecorderObserver",
    "description": "Provides callback events for audio and video recording.",
    "parameters": [],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_imetadataobserver",
    "name": "IMetadataObserver",
    "description": "The metadata observer.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_imusiccontentcenter",
    "name": "IMusicContentCenter",
    "description": "IMusicContentCenter 接口类提供音乐内容中心的相关方法。",
    "parameters": [],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_imusiccontentcentereventhandler",
    "name": "IMusicContentCenterEventHandler",
    "description": "IMusicContentCenterEventHandler 接口类，用于 SDK 向客户端发送音乐内容中心事件通知。",
    "parameters": [],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_imusicplayer",
    "name": "IMusicPlayer",
    "description": "继承自 IMediaPlayer 类，提供音乐播放器的相关方法。",
    "parameters": [],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_injectstreamconfig",
    "name": "InjectStreamConfig",
    "description": "Configurations of injecting an external audio or video stream.",
    "parameters": [
      {
        "width": "The width of the external video stream after injecting. The default value is 0, which represents the same width as the original."
      },
      {
        "height": "The height of the external video stream after injecting. The default value is 0, which represents the same height as the original."
      },
      {
        "videoGop": "The GOP (in frames) of injecting the external video stream. The default value is 30 frames."
      },
      {
        "videoFramerate": "The frame rate (fps) of injecting the external video stream. The default rate is 15 fps."
      },
      {
        "videoBitrate": "The bitrate (Kbps) of injecting the external video stream. The default value is 400 Kbps. The bitrate setting is closely linked to the video resolution. If the bitrate you set is beyond a reasonable range, the SDK sets it within a reasonable range."
      },
      {
        "audioSampleRate": "The sampling rate (Hz) of injecting the external audio stream. The default value is 48000 Hz. See AUDIO_SAMPLE_RATE_TYPE. Agora recommends using the default value."
      },
      {
        "audioBitrate": "The bitrate (Kbps) of injecting the external audio stream. The default value is 48 Kbps. Agora recommends using the default value."
      },
      {
        "audioChannels": "The number of channels of the external audio stream after injecting.\n 1: Mono (default)\n 2: Stereo. Agora recommends using the default value."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_irtcengine",
    "name": "IRtcEngine",
    "description": "The basic interface of the Agora SDK that implements the core functions of real-time communication.\n\nIRtcEngine provides the main methods that your app can call. Before calling other APIs, you must call CreateAgoraRtcEngine to create an IRtcEngine object.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_irtcengineeventhandler",
    "name": "IRtcEngineEventHandler",
    "description": "The SDK uses the IRtcEngineEventHandler interface to send event notifications to your app. Your app can get those notifications through methods that inherit this interface.\n\nAll methods in this interface have default (empty) implementation. You can choose to inherit events related to your app scenario.\n In the callbacks, avoid implementing time-consuming tasks or calling APIs that may cause thread blocking (such as sendMessage). Otherwise, the SDK may not work properly.\n The SDK no longer catches exceptions in the code logic that developers implement themselves in IRtcEngineEventHandler class. You need to handle this exception yourself, otherwise the app may crash when the exception occurs.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_irtcengineex",
    "name": "IRtcEngineEx",
    "description": "This interface class contains multi-channel methods.\n\nInherited from IRtcEngine.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_ivideodevicemanager",
    "name": "IVideoDeviceManager",
    "description": "Video device management methods.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_ivideoencodedframeobserver",
    "name": "IVideoEncodedFrameObserver",
    "description": "Receives encoded video images.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_ivideoframeobserver",
    "name": "IVideoFrameObserver",
    "description": "The IVideoFrameObserver class.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_lastmileprobeconfig",
    "name": "LastmileProbeConfig",
    "description": "Configurations of the last-mile network test.",
    "parameters": [
      {
        "probeUplink": "Sets whether to test the uplink network. Some users, for example, the audience members in a LIVE_BROADCASTING channel, do not need such a test. true : Test the uplink network. false : Do not test the uplink network."
      },
      {
        "probeDownlink": "Sets whether to test the downlink network: true : Test the downlink network. false : Do not test the downlink network."
      },
      {
        "expectedUplinkBitrate": "The expected maximum uplink bitrate (bps) of the local user. The value range is [100000, 5000000]. Agora recommends referring to SetVideoEncoderConfiguration to set the value."
      },
      {
        "expectedDownlinkBitrate": "The expected maximum downlink bitrate (bps) of the local user. The value range is [100000,5000000]."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_lastmileprobeonewayresult",
    "name": "LastmileProbeOneWayResult",
    "description": "Results of the uplink or downlink last-mile network test.",
    "parameters": [
      {
        "packetLossRate": "The packet loss rate (%)."
      },
      {
        "jitter": "The network jitter (ms)."
      },
      {
        "availableBandwidth": "The estimated available bandwidth (bps)."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_lastmileproberesult",
    "name": "LastmileProbeResult",
    "description": "Results of the uplink and downlink last-mile network tests.",
    "parameters": [
      {
        "state": "The status of the last-mile network tests. See LASTMILE_PROBE_RESULT_STATE."
      },
      {
        "uplinkReport": "Results of the uplink last-mile network test. See LastmileProbeOneWayResult."
      },
      {
        "downlinkReport": "Results of the downlink last-mile network test. See LastmileProbeOneWayResult."
      },
      {
        "rtt": "The round-trip time (ms)."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_leavechanneloptions",
    "name": "LeaveChannelOptions",
    "description": "The options for leaving a channel.",
    "parameters": [
      {
        "stopAudioMixing": "Whether to stop playing and mixing the music file when a user leaves the channel. true : (Default) Stop playing and mixing the music file. false : Do not stop playing and mixing the music file."
      },
      {
        "stopAllEffect": "Whether to stop playing all audio effects when a user leaves the channel. true : (Default) Stop playing all audio effects. false : Do not stop playing any audio effect."
      },
      {
        "stopMicrophoneRecording": "Whether to stop microphone recording when a user leaves the channel. true : (Default) Stop microphone recording. false : Do not stop microphone recording."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_livestreamadvancedfeature",
    "name": "LiveStreamAdvancedFeature",
    "description": "The configuration for advanced features of the RTMP or RTMPS streaming with transcoding.\n\nIf you want to enable the advanced features of streaming with transcoding, contact.",
    "parameters": [
      {
        "featureName": "The feature names, including LBHQ (high-quality video with a lower bitrate) and VEO (optimized video encoder)."
      },
      {
        "opened": "Whether to enable the advanced features of streaming with transcoding: true : Enable the advanced features. false : (Default) Do not enable the advanced features."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_livetranscoding",
    "name": "LiveTranscoding",
    "description": "Transcoding configurations for Media Push.",
    "parameters": [
      {
        "width": "The width of the video in pixels. The default value is 360.\n When pushing video streams to the CDN, the value range of width is [64,1920]. If the value is less than 64, Agora server automatically adjusts it to 64; if the value is greater than 1920, Agora server automatically adjusts it to 1920.\n When pushing audio streams to the CDN, set width and height as 0."
      },
      {
        "height": "The height of the video in pixels. The default value is 640.\n When pushing video streams to the CDN, the value range of height is [64,1080]. If the value is less than 64, Agora server automatically adjusts it to 64; if the value is greater than 1080, Agora server automatically adjusts it to 1080.\n When pushing audio streams to the CDN, set width and height as 0."
      },
      {
        "videoBitrate": "The encoding bitrate (Kbps) of the video. See BITRATE. This parameter does not need to be set; keeping the default value STANDARD_BITRATE is sufficient. The SDK automatically matches the most suitable bitrate based on the video resolution and frame rate you have set. For the correspondence between video resolution and frame rate, see."
      },
      {
        "videoFrameRate": "Frame rate (fps) of the output video stream set for Media Push. The default value is 15. The value range is (0,30]. The Agora server adjusts any value over 30 to 30."
      },
      {
        "lowLatency": "Deprecated This member is deprecated. Latency mode: true : Low latency with unassured quality. false : (Default) High latency with assured quality."
      },
      {
        "videoGop": "GOP (Group of Pictures) in fps of the video frames for Media Push. The default value is 30."
      },
      {
        "videoCodecProfile": "Video codec profile type for Media Push. Set it as 66, 77, or 100 (default). See VIDEO_CODEC_PROFILE_TYPE for details. If you set this parameter to any other value, Agora adjusts it to the default value."
      },
      {
        "videoCodecType": "Video codec profile types for Media Push. See VIDEO_CODEC_TYPE_FOR_STREAM."
      },
      {
        "transcodingUsers": "Manages the user layout configuration in the Media Push. Agora supports a maximum of 17 transcoding users in a Media Push channel. See TranscodingUser."
      },
      {
        "transcodingExtraInfo": "Reserved property. Extra user-defined information to send SEI for the H.264/H.265 video stream to the CDN live client. Maximum length: 4096 bytes. For more information on SEI, see SEI-related questions."
      },
      {
        "backgroundColor": "The background color in RGB hex value. Value only. Do not include a preceeding #. For example, 0xFFB6C1 (light pink). The default value is 0x000000 (black)."
      },
      {
        "userCount": "The number of users in the Media Push. The value range is [0,17]."
      },
      {
        "metadata": "Deprecated Obsolete and not recommended for use. The metadata sent to the CDN client."
      },
      {
        "watermark": "The watermark on the live video. The image format needs to be PNG. See RtcImage. You can add one watermark, or add multiple watermarks using an array. This parameter is used with watermarkCount."
      },
      {
        "backgroundImage": "The number of background images on the live video. The image format needs to be PNG. See RtcImage. You can add a background image or use an array to add multiple background images. This parameter is used with backgroundImageCount."
      },
      {
        "audioSampleRate": "The audio sampling rate (Hz) of the output media stream. See AUDIO_SAMPLE_RATE_TYPE."
      },
      {
        "audioBitrate": "Bitrate (Kbps) of the audio output stream for Media Push. The default value is 48, and the highest value is 128."
      },
      {
        "audioChannels": "The number of audio channels for Media Push. Agora recommends choosing 1 (mono), or 2 (stereo) audio channels. Special players are required if you choose 3, 4, or 5.\n 1: (Default) Mono\n 2: Stereo.\n 3: Three audio channels.\n 4: Four audio channels.\n 5: Five audio channels."
      },
      {
        "audioCodecProfile": "Audio codec profile type for Media Push. See AUDIO_CODEC_PROFILE_TYPE."
      },
      {
        "watermarkCount": "The number of watermarks on the live video. The total number of watermarks and background images can range from 0 to 10. This parameter is used with watermark."
      },
      {
        "backgroundImageCount": "The number of background images on the live video. The total number of watermarks and background images can range from 0 to 10. This parameter is used with backgroundImage."
      },
      {
        "advancedFeatures": "Advanced features of the Media Push with transcoding. See LiveStreamAdvancedFeature."
      },
      {
        "advancedFeatureCount": "The number of enabled advanced features. The default value is 0."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_localaudiomixerconfiguration",
    "name": "LocalAudioMixerConfiguration",
    "description": "The configurations for mixing the lcoal audio.",
    "parameters": [
      {
        "streamCount": "The number of the audio streams that are mixed locally."
      },
      {
        "audioInputStreams": "The source of the audio streams that are mixed locally. See MixedAudioStream."
      },
      {
        "syncWithLocalMic": "Whether the mxied audio stream uses the timestamp of the audio frames captured by the local microphone. true : (Default) Yes. Set to this value if you want all locally captured audio streams synchronized. false : No. The SDK uses the timestamp of the audio frames at the time when they are mixed."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_localaudiostats",
    "name": "LocalAudioStats",
    "description": "Local audio statistics.",
    "parameters": [
      {
        "numChannels": "The number of audio channels."
      },
      {
        "sentSampleRate": "The sampling rate (Hz) of sending the local user's audio stream."
      },
      {
        "sentBitrate": "The average bitrate (Kbps) of sending the local user's audio stream."
      },
      {
        "txPacketLossRate": "The packet loss rate (%) from the local client to the Agora server before applying the anti-packet loss strategies."
      },
      {
        "internalCodec": "The internal payload codec."
      },
      {
        "audioDeviceDelay": "The audio device module delay (ms) when playing or recording audio."
      },
      {
        "earMonitorDelay": "The ear monitor delay (ms), which is the delay from microphone input to headphone output."
      },
      {
        "aecEstimatedDelay": "Acoustic echo cancellation (AEC) module estimated delay (ms), which is the signal delay between when audio is played locally before being locally captured."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_localspatialaudioconfig",
    "name": "LocalSpatialAudioConfig",
    "description": "The configuration of ILocalSpatialAudioEngine.",
    "parameters": [
      {
        "rtcEngine": "IRtcEngine."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_localtranscoderconfiguration",
    "name": "LocalTranscoderConfiguration",
    "description": "The configuration of the video mixing on the local client.",
    "parameters": [
      {
        "streamCount": "The number of the video streams for the video mixing on the local client."
      },
      {
        "videoInputStreams": "The video streams for local video mixing. See TranscodingVideoStream."
      },
      {
        "videoOutputConfiguration": "The encoding configuration of the mixed video stream after the local video mixing. See VideoEncoderConfiguration."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_localvideostats",
    "name": "LocalVideoStats",
    "description": "The statistics of the local video stream.",
    "parameters": [
      {
        "uid": "The ID of the local user."
      },
      {
        "sentBitrate": "The actual bitrate (Kbps) while sending the local video stream. This value does not include the bitrate for resending the video after packet loss."
      },
      {
        "sentFrameRate": "The actual frame rate (fps) while sending the local video stream. This value does not include the frame rate for resending the video after packet loss."
      },
      {
        "captureFrameRate": "The frame rate (fps) for capturing the local video stream."
      },
      {
        "captureFrameWidth": "The width (px) for capturing the local video stream."
      },
      {
        "captureFrameHeight": "The height (px) for capturing the local video stream."
      },
      {
        "regulatedCaptureFrameRate": "The frame rate (fps) adjusted by the built-in video capture adapter (regulator) of the SDK for capturing the local video stream. The regulator adjusts the frame rate of the video captured by the camera according to the video encoding configuration."
      },
      {
        "regulatedCaptureFrameWidth": "The width (px) adjusted by the built-in video capture adapter (regulator) of the SDK for capturing the local video stream. The regulator adjusts the height and width of the video captured by the camera according to the video encoding configuration."
      },
      {
        "regulatedCaptureFrameHeight": "The height (px) adjusted by the built-in video capture adapter (regulator) of the SDK for capturing the local video stream. The regulator adjusts the height and width of the video captured by the camera according to the video encoding configuration."
      },
      {
        "encoderOutputFrameRate": "The output frame rate (fps) of the local video encoder."
      },
      {
        "rendererOutputFrameRate": "The output frame rate (fps) of the local video renderer."
      },
      {
        "targetBitrate": "The target bitrate (Kbps) of the current encoder. This is an estimate made by the SDK based on the current network conditions."
      },
      {
        "targetFrameRate": "The target frame rate (fps) of the current encoder."
      },
      {
        "qualityAdaptIndication": "The quality adaptation of the local video stream in the reported interval (based on the target frame rate and target bitrate). See QUALITY_ADAPT_INDICATION."
      },
      {
        "encodedBitrate": "The bitrate (Kbps) while encoding the local video stream. This value does not include the bitrate for resending the video after packet loss."
      },
      {
        "encodedFrameHeight": "The height of the encoded video (px)."
      },
      {
        "encodedFrameWidth": "The width of the encoded video (px)."
      },
      {
        "encodedFrameCount": "The number of the sent video frames, represented by an aggregate value."
      },
      {
        "codecType": "The codec type of the local video. See VIDEO_CODEC_TYPE."
      },
      {
        "txPacketLossRate": "The video packet loss rate (%) from the local client to the Agora server before applying the anti-packet loss strategies."
      },
      {
        "captureBrightnessLevel": "The brightness level of the video image captured by the local camera. See CAPTURE_BRIGHTNESS_LEVEL_TYPE."
      },
      {
        "hwEncoderAccelerating": "The local video encoding acceleration type.\n 0: Software encoding is applied without acceleration.\n 1: Hardware encoding is applied for acceleration."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_logconfig",
    "name": "LogConfig",
    "description": "Configuration of Agora SDK log files.",
    "parameters": [
      {
        "filePath": "The complete path of the log files. Agora recommends using the default log directory. If you need to modify the default directory, ensure that the directory you specify exists and is writable. The default log directory is:\n Android: /storage/emulated/0/Android/data/<packagename>/files/agorasdk.log.\n iOS: App Sandbox/Library/caches/agorasdk.log.\n macOS:\n If Sandbox is enabled: App Sandbox/Library/Logs/agorasdk.log. For example, /Users/<username>/Library/Containers/<AppBundleIdentifier>/Data/Library/Logs/agorasdk.log.\n If Sandbox is disabled: ~/Library/Logs/agorasdk.log\n Windows: C:\\Users\\<user_name>\\AppData\\Local\\Agora\\<process_name>\\agorasdk.log."
      },
      {
        "fileSizeInKB": "The size (KB) of an agorasdk.log file. The value range is [128,20480]. The default value is 2,048 KB. If you set fileSizeInKByte smaller than 128 KB, the SDK automatically adjusts it to 128 KB; if you set fileSizeInKByte greater than 20,480 KB, the SDK automatically adjusts it to 20,480 KB."
      },
      {
        "level": "The output level of the SDK log file. See LOG_LEVEL. For example, if you set the log level to WARN, the SDK outputs the logs within levels FATAL, ERROR, and WARN."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_loguploadserverinfo",
    "name": "LogUploadServerInfo",
    "description": "Configuration information of the log server.",
    "parameters": [
      {
        "serverDomain": "The domain name of the log server."
      },
      {
        "serverPath": "The log storage path on the server."
      },
      {
        "serverPort": "The port of the log server."
      },
      {
        "serverHttps": "Whether the log server uses the HTTPS protocol: true : Use HTTPS protocol. false : Do not use HTTPS protocol. Uses the HTTP protocol."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_lowlightenhanceoptions",
    "name": "LowlightEnhanceOptions",
    "description": "The low-light enhancement options.",
    "parameters": [
      {
        "level": "The low-light enhancement level. See LOW_LIGHT_ENHANCE_LEVEL."
      },
      {
        "mode": "The low-light enhancement mode. See LOW_LIGHT_ENHANCE_MODE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_mediarecorderconfiguration",
    "name": "MediaRecorderConfiguration",
    "description": "The recording configuration.",
    "parameters": [
      {
        "storagePath": "The absolute path (including the filename extensions) of the recording file. For example:\n Windows: C:\\Users\\<user_name>\\AppData\\Local\\Agora\\<process_name>\\example.mp4\n iOS: /App Sandbox/Library/Caches/example.mp4\n macOS: ～/Library/Logs/example.mp4\n Android: /storage/emulated/0/Android/data/<package name>/files/agorasdk.mp4 Ensure that the directory for the log files exists and is writable."
      },
      {
        "containerFormat": "The format of the recording file. See MediaRecorderContainerFormat."
      },
      {
        "streamType": "The recording content. See MediaRecorderStreamType."
      },
      {
        "maxDurationMs": "The maximum recording duration, in milliseconds. The default value is 120000."
      },
      {
        "recorderInfoUpdateInterval": "The interval (ms) of updating the recording information. The value range is [1000,10000]. Based on the value you set in this parameter, the SDK triggers the OnRecorderInfoUpdated callback to report the updated recording information."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_mediasource",
    "name": "MediaSource",
    "description": "Information related to the media file to be played and the playback scenario configurations.",
    "parameters": [
      {
        "url": "The URL of the media file to be played. If you open a common media resource, pass in the value to url. If you open a custom media resource, pass in the value to provider. Agora recommends that you do not pass in values to both parameters in one call; otherwise, this call may fail."
      },
      {
        "uri": "The URI (Uniform Resource Identifier) of the media file."
      },
      {
        "startPos": "The starting position (ms) for playback. The default value is 0."
      },
      {
        "autoPlay": "Whether to enable autoplay once the media file is opened: true : (Default) Yes. false : No. If autoplay is disabled, you need to call the Play method to play a media file after it is opened."
      },
      {
        "enableCache": "Whether to cache the media file when it is being played: true :Enables caching. false : (Default) Disables caching.\n Agora only supports caching on-demand audio and video streams that are not transmitted in HLS protocol.\n If you need to enable caching, pass in a value to uri; otherwise, caching is based on the url of the media file.\n If you enable this function, the Media Player caches part of the media file being played on your local device, and you can play the cached media file without internet connection. The statistics about the media file being cached are updated every second after the media file is played. See CacheStatistics."
      },
      {
        "enableMultiAudioTrack": "Whether to allow the selection of different audio tracks when playing this media file: true : Allow to select different audio tracks. false : (Default) Do not allow to select different audio tracks. If you need to set different audio tracks for local playback and publishing to the channel, you need to set this parameter to true, and then call the SelectMultiAudioTrack method to select the audio track."
      },
      {
        "isAgoraSource": "Whether the media resource to be opened is a live stream or on-demand video distributed through Media Broadcast service: true : The media resource to be played is a live or on-demand video distributed through Media Broadcast service. false : (Default) The media resource is not a live stream or on-demand video distributed through Media Broadcast service. If you need to open a live stream or on-demand video distributed through Broadcast Streaming service, pass in the URL of the media resource to url, and set isAgoraSource as true; otherwise, you don't need to set the isAgoraSource parameter."
      },
      {
        "isLiveSource": "Whether the media resource to be opened is a live stream: true : The media resource is a live stream. false : (Default) The media resource is not a live stream. If the media resource you want to open is a live stream, Agora recommends that you set this parameter as true so that the live stream can be loaded more quickly. If the media resource you open is not a live stream, but you set isLiveSource as true, the media resource is not to be loaded more quickly."
      },
      {
        "provider": "The callback for custom media resource files. See IMediaPlayerCustomDataProvider. If you open a custom media resource, pass in the value to provider. If you open a common media resource, pass in the value to url. Agora recommends that you do not pass in values to both url and provider in one call; otherwise, this call may fail."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_metadata",
    "name": "Metadata",
    "description": "Media metadata.",
    "parameters": [
      {
        "channelId": "The channel name."
      },
      {
        "uid": "The user ID.\n For the recipient: The ID of the remote user who sent the Metadata.\n For the sender: Ignore it."
      },
      {
        "size": "The buffer size of the sent or received Metadata."
      },
      {
        "buffer": "The buffer address of the received Metadata."
      },
      {
        "timeStampMs": "The timestamp (ms) of when the Metadata is sent."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_mixedaudiostream",
    "name": "MixedAudioStream",
    "description": "The source of the audio streams that are mixed locally.",
    "parameters": [
      {
        "sourceType": "The type of the audio source. See AUDIO_SOURCE_TYPE."
      },
      {
        "remoteUserUid": "The user ID of the remote user. Set this parameter if the source type of the locally mixed audio steams is AUDIO_SOURCE_REMOTE_USER."
      },
      {
        "channelId": "The channel name. This parameter signifies the channel in which users engage in real-time audio and video interaction. Under the premise of the same App ID, users who fill in the same channel ID enter the same channel for audio and video interaction. The string length must be less than 64 bytes. Supported characters (89 characters in total):\n All lowercase English letters: a to z.\n All uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n \"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"<\", \"=\", \".\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\" Set this parameter if the source type of the locally mixed audio streams is AUDIO_SOURCE_REMOTE_CHANNEL or AUDIO_SOURCE_REMOTE_USER."
      },
      {
        "trackId": "The audio track ID. Set this parameter to the custom audio track ID returned in CreateCustomAudioTrack. Set this parameter if the source type of the locally mixed audio steams is AUDIO_SOURCE_CUSTOM."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_music",
    "name": "Music",
    "description": "The details of musci assets in the collection.",
    "parameters": [
      {
        "songCode": "The code of the music, which is an unique identifier of the music."
      },
      {
        "name": "音乐资源名称。"
      },
      {
        "singer": "歌手名。"
      },
      {
        "poster": "音乐资源海报的下载地址。"
      },
      {
        "releaseTime": "音乐资源发布的时间。"
      },
      {
        "type": "音乐资源类型：\n 1：左声道伴奏，右声道原唱的单音轨音源。\n 2：只有伴唱的单音轨音源。\n 3：只有原唱的单音轨音源。\n 4：多音轨的纯音频音源。"
      },
      {
        "pitchType": "歌曲是否支持演唱评分功能：\n 1：歌曲支持演唱评分功能。\n 2：歌曲不支持演唱评分功能。"
      },
      {
        "durationS": "音乐资源总时长 （秒）。"
      },
      {
        "lyricList": "支持的歌词类型：\n 0: xml.\n 1: lrc."
      },
      {
        "climaxSegmentList": "音乐高潮片段列表，详见 ClimaxSegment 。"
      },
      {
        "lyricCount": "歌曲的歌词数量。"
      },
      {
        "climaxSegmentCount": "高潮片段的数量。"
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_musiccacheinfo",
    "name": "MusicCacheInfo",
    "description": "缓存的音乐资源的相关信息。",
    "parameters": [
      {
        "songCode": "The code of the music, which is an unique identifier of the music."
      },
      {
        "status": "音乐资源的缓存状态，详见 MUSIC_CACHE_STATUS_TYPE 。"
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_musiccollection",
    "name": "MusicCollection",
    "description": "The details of musci assets in the collection.",
    "parameters": [
      {
        "count": "此次请求的音乐资源列表中的音乐数量。"
      },
      {
        "total": "The total number of music assets in the collection."
      },
      {
        "page": "当前页面编号，默认从 1 开始。"
      },
      {
        "pageSize": "当前音乐资源列表的页面总数量，最大值为 50。"
      },
      {
        "musicList": "The details of music assets on the current page. See Music."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_musiccontentcenterconfiguration",
    "name": "MusicContentCenterConfiguration",
    "description": "The configurations for music content center.",
    "parameters": [
      {
        "appId": "The App ID of the project that has enabled music content center."
      },
      {
        "token": "The token used for authentication when using music content center. When your token is about to expire, you can call RenewToken to pass in a new token."
      },
      {
        "mccUid": "The ID of the user who uses music content center. This ID can be the same as the uid used when joining the RTC channel, but cannot be 0."
      },
      {
        "maxCacheSize": "可缓存的音乐资源数量，最多不能超过 50。"
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_mvproperty",
    "name": "MvProperty",
    "description": "MV 的属性。",
    "parameters": [
      {
        "bandwidth": "MV 的带宽，单位为 Kbps。"
      },
      {
        "resolution": "MV 的分辨率。"
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_playerplaybackstats",
    "name": "PlayerPlaybackStats",
    "description": "The information of the media file being played.",
    "parameters": [
      {
        "videoFps": "The frame rate (fps) of the video."
      },
      {
        "videoBitrateInKbps": "The bitrate (kbps) of the video."
      },
      {
        "audioBitrateInKbps": "The bitrate (kbps) of the audio."
      },
      {
        "totalBitrateInKbps": "The total bitrate (kbps) of the media stream."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_playerstreaminfo",
    "name": "PlayerStreamInfo",
    "description": "The detailed information of the media stream.",
    "parameters": [
      {
        "streamIndex": "The index of the media stream."
      },
      {
        "streamType": "The type of the media stream. See MEDIA_STREAM_TYPE."
      },
      {
        "codecName": "The codec of the media stream."
      },
      {
        "language": "The language of the media stream."
      },
      {
        "videoFrameRate": "This parameter only takes effect for video streams, and indicates the video frame rate (fps)."
      },
      {
        "videoBitrate": "This parameter only takes effect for video streams, and indicates the video bitrate (bps)."
      },
      {
        "videoWidth": "This parameter only takes effect for video streams, and indicates the video width (pixel)."
      },
      {
        "videoHeight": "This parameter only takes effect for video streams, and indicates the video height (pixel)."
      },
      {
        "videoRotation": "This parameter only takes effect for video streams, and indicates the video rotation angle."
      },
      {
        "audioSampleRate": "This parameter only takes effect for audio streams, and indicates the audio sample rate (Hz)."
      },
      {
        "audioChannels": "This parameter only takes effect for audio streams, and indicates the audio channel number."
      },
      {
        "audioBitsPerSample": "This parameter only takes effect for audio streams, and indicates the bit number of each audio sample."
      },
      {
        "duration": "The total duration (ms) of the media stream."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_playerupdatedinfo",
    "name": "PlayerUpdatedInfo",
    "description": "Information related to the media player.",
    "parameters": [
      {
        "deviceId": "The ID of a deivce."
      },
      {
        "videoHeight": "Height (pixel) of the video."
      },
      {
        "videoWidth": "Width (pixel) of the video."
      },
      {
        "audioSampleRate": "Audio sample rate (Hz)."
      },
      {
        "audioChannels": "The number of audio channels."
      },
      {
        "audioBitsPerSample": "The number of bits per audio sample point."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_recorderinfo",
    "name": "RecorderInfo",
    "description": "The information about the file that is recorded.",
    "parameters": [
      {
        "filename": "The absolute path of the recording file."
      },
      {
        "durationMs": "The recording duration (ms)."
      },
      {
        "fileSize": "The size (byte) of the recording file."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "class_recorderstreaminfo",
    "name": "RecorderStreamInfo",
    "description": "The information about the media streams to be recorded.",
    "parameters": [
      {
        "channelId": "The name of the channel in which the media streams publish."
      },
      {
        "uid": "The ID of the user whose media streams you want to record."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_rectangle",
    "name": "Rectangle",
    "description": "The location of the target area relative to the screen or window. If you do not set this parameter, the SDK selects the whole screen or window.",
    "parameters": [
      {
        "x": "The horizontal offset from the top-left corner."
      },
      {
        "y": "The vertical offset from the top-left corner."
      },
      {
        "width": "The width of the target area."
      },
      {
        "height": "The height of the target area."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_remoteaudiostats",
    "name": "RemoteAudioStats",
    "description": "Audio statistics of the remote user.",
    "parameters": [
      {
        "uid": "The user ID of the remote user."
      },
      {
        "quality": "The quality of the audio stream sent by the user. See QUALITY_TYPE."
      },
      {
        "networkTransportDelay": "The network delay (ms) from the sender to the receiver."
      },
      {
        "jitterBufferDelay": "The network delay (ms) from the audio receiver to the jitter buffer. When the receiving end is an audience member and audienceLatencyLevel of ClientRoleOptions is 1, this parameter does not take effect."
      },
      {
        "audioLossRate": "The frame loss rate (%) of the remote audio stream in the reported interval."
      },
      {
        "numChannels": "The number of audio channels."
      },
      {
        "receivedSampleRate": "The sampling rate of the received audio stream in the reported interval."
      },
      {
        "receivedBitrate": "The average bitrate (Kbps) of the received audio stream in the reported interval."
      },
      {
        "totalFrozenTime": "The total freeze time (ms) of the remote audio stream after the remote user joins the channel. In a session, audio freeze occurs when the audio frame loss rate reaches 4%."
      },
      {
        "frozenRate": "The total audio freeze time as a percentage (%) of the total time when the audio is available. The audio is considered available when the remote user neither stops sending the audio stream nor disables the audio module after joining the channel."
      },
      {
        "totalActiveTime": "The total active time (ms) between the start of the audio call and the callback of the remote user. The active time refers to the total duration of the remote user without the mute state."
      },
      {
        "publishDuration": "The total duration (ms) of the remote audio stream."
      },
      {
        "qoeQuality": "The Quality of Experience (QoE) of the local user when receiving a remote audio stream. See EXPERIENCE_QUALITY_TYPE."
      },
      {
        "qualityChangedReason": "Reasons why the QoE of the local user when receiving a remote audio stream is poor. See EXPERIENCE_POOR_REASON."
      },
      {
        "mosValue": "The quality of the remote audio stream in the reported interval. The quality is determined by the Agora real-time audio MOS (Mean Opinion Score) measurement method. The return value range is [0, 500]. Dividing the return value by 100 gets the MOS score, which ranges from 0 to 5. The higher the score, the better the audio quality. The subjective perception of audio quality corresponding to the Agora real-time audio MOS scores is as follows: MOS score Perception of audio quality Greater than 4 Excellent. The audio sounds clear and smooth. From 3.5 to 4 Good. The audio has some perceptible impairment but still sounds clear. From 3 to 3.5 Fair. The audio freezes occasionally and requires attentive listening. From 2.5 to 3 Poor. The audio sounds choppy and requires considerable effort to understand. From 2 to 2.5 Bad. The audio has occasional noise. Consecutive audio dropouts occur, resulting in some information loss. The users can communicate only with difficulty. Less than 2 Very bad. The audio has persistent noise. Consecutive audio dropouts are frequent, resulting in severe information loss. Communication is nearly impossible."
      },
      {
        "e2eDelay": "End-to-end audio delay (in milliseconds), which refers to the time from when the audio is captured by the remote user to when it is played by the local user."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_remotevideostats",
    "name": "RemoteVideoStats",
    "description": "Statistics of the remote video stream.",
    "parameters": [
      {
        "uid": "The user ID of the remote user sending the video stream."
      },
      {
        "delay": "Deprecated: In scenarios where audio and video are synchronized, you can get the video delay data from networkTransportDelay and jitterBufferDelay in RemoteAudioStats. The video delay (ms)."
      },
      {
        "e2eDelay": "End-to-end video latency (ms). That is, the time elapsed from the video capturing on the remote user's end to the receiving and rendering of the video on the local user's end."
      },
      {
        "width": "The width (pixels) of the video."
      },
      {
        "height": "The height (pixels) of the video."
      },
      {
        "receivedBitrate": "The bitrate (Kbps) of the remote video received since the last count."
      },
      {
        "decoderOutputFrameRate": "The frame rate (fps) of decoding the remote video."
      },
      {
        "rendererOutputFrameRate": "The frame rate (fps) of rendering the remote video."
      },
      {
        "frameLossRate": "The packet loss rate (%) of the remote video."
      },
      {
        "packetLossRate": "The packet loss rate (%) of the remote video after using the anti-packet-loss technology."
      },
      {
        "rxStreamType": "The type of the video stream. See VIDEO_STREAM_TYPE."
      },
      {
        "totalFrozenTime": "The total freeze time (ms) of the remote video stream after the remote user joins the channel. In a video session where the frame rate is set to no less than 5 fps, video freeze occurs when the time interval between two adjacent renderable video frames is more than 500 ms."
      },
      {
        "frozenRate": "The total video freeze time as a percentage (%) of the total time the video is available. The video is considered available as long as that the remote user neither stops sending the video stream nor disables the video module after joining the channel."
      },
      {
        "totalActiveTime": "The total active time (ms) of the video. As long as the remote user or host neither stops sending the video stream nor disables the video module after joining the channel, the video is available."
      },
      {
        "publishDuration": "The total duration (ms) of the remote video stream."
      },
      {
        "avSyncTimeMs": "The amount of time (ms) that the audio is ahead of the video. If this value is negative, the audio is lagging behind the video."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_remotevoicepositioninfo",
    "name": "RemoteVoicePositionInfo",
    "description": "The spatial position of the remote user or the media player.",
    "parameters": [
      {
        "position": "The coordinates in the world coordinate system. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn. And the front, right, and top coordinates correspond to the positive directions of Unity's Vector3 axes (z, x, and y, respectively)."
      },
      {
        "forward": "The unit vector of the x axis in the coordinate system. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn. And the front, right, and top coordinates correspond to the positive directions of Unity's Vector3 axes (z, x, and y, respectively)."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_rtcconnection",
    "name": "RtcConnection",
    "description": "Contains connection information.",
    "parameters": [
      {
        "channelId": "The channel name."
      },
      {
        "localUid": "The ID of the local user."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_rtcengineconfig",
    "name": "RtcEngineContext",
    "description": "Configurations for the RtcEngineContext instance.",
    "parameters": [
      {
        "eventHandler": "The event handler for IRtcEngine. See IRtcEngineEventHandler."
      },
      {
        "appId": "The App ID issued by Agora for your project. Only users in apps with the same App ID can join the same channel and communicate with each other. An App ID can only be used to create one IRtcEngine instance. To change your App ID, call Dispose to destroy the current IRtcEngine instance, and then create a new one."
      },
      {
        "context": "For Windows, it is the window handle of the app. Once set, this parameter enables you to connect or disconnect the video devices while they are powered.\n For Android, it is the context of Android Activity."
      },
      {
        "channelProfile": "The channel profile. See CHANNEL_PROFILE_TYPE."
      },
      {
        "audioScenario": "The audio scenarios. Under different audio scenarios, the device uses different volume types. See AUDIO_SCENARIO_TYPE."
      },
      {
        "areaCode": "The region for connection. This is an advanced feature and applies to scenarios that have regional restrictions. For details on supported regions, see AREA_CODE. The area codes support bitwise operation."
      },
      {
        "logConfig": "The SDK log files are: agorasdk.log, agorasdk.1.log, agorasdk.2.log, agorasdk.3.log, and agorasdk.4.log.\n The API call log files are: agoraapi.log, agoraapi.1.log, agoraapi.2.log, agoraapi.3.log, and agoraapi.4.log.\n The default size of each SDK log file and API log file is 2,048 KB. These log files are encoded in UTF-8.\n The SDK writes the latest logs in agorasdk.log or agoraapi.log.\n When agorasdk.log is full, the SDK processes the log files in the following order:\n Delete the agorasdk.4.log file (if any).\n Rename agorasdk.3.log to agorasdk.4.log.\n Rename agorasdk.2.log to agorasdk.3.log.\n Rename agorasdk.1.log to agorasdk.2.log.\n Create a new agorasdk.log file.\n The overwrite rules for the agoraapi.log file are the same as for agorasdk.log. Sets the log file size. See LogConfig. By default, the SDK generates five SDK log files and five API call log files with the following rules:"
      },
      {
        "domainLimit": "Whether to enable domain name restriction: true : Enables the domain name restriction. This value is suitable for scenarios where IoT devices use IoT cards for network access. The SDK will only connect to servers in the domain name or IP whitelist that has been reported to the operator. false : (Default) Disables the domain name restriction. This value is suitable for most common scenarios."
      },
      {
        "autoRegisterAgoraExtensions": "Whether to automatically register the Agora extensions when initializing IRtcEngine : true : (Default) Automatically register the Agora extensions when initializing IRtcEngine. false : Do not register the Agora extensions when initializing IRtcEngine. You need to call EnableExtension to register the Agora extensions."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_rtcimage",
    "name": "RtcImage",
    "description": "Image properties.\n\nThis class sets the properties of the watermark and background images in the live video.",
    "parameters": [
      {
        "url": "The HTTP/HTTPS URL address of the image in the live video. The maximum length of this parameter is 1024 bytes."
      },
      {
        "x": "The x-coordinate (px) of the image on the video frame (taking the upper left corner of the video frame as the origin)."
      },
      {
        "y": "The y-coordinate (px) of the image on the video frame (taking the upper left corner of the video frame as the origin)."
      },
      {
        "width": "The width (px) of the image on the video frame."
      },
      {
        "height": "The height (px) of the image on the video frame."
      },
      {
        "zOrder": "The layer index of the watermark or background image. When you use the watermark array to add a watermark or multiple watermarks, you must pass a value to zOrder in the range [1,255]; otherwise, the SDK reports an error. In other cases, zOrder can optionally be passed in the range [0,255], with 0 being the default value. 0 means the bottom layer and 255 means the top layer."
      },
      {
        "alpha": "The transparency of the watermark or background image. The range of the value is [0.0,1.0]:\n 0.0: Completely transparent.\n 1.0: (Default) Opaque."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_rtcstats",
    "name": "RtcStats",
    "description": "Statistics of a call session.",
    "parameters": [
      {
        "duration": "Call duration of the local user in seconds, represented by an aggregate value."
      },
      {
        "txBytes": "The number of bytes sent."
      },
      {
        "rxBytes": "The number of bytes received."
      },
      {
        "txAudioBytes": "The total number of audio bytes sent, represented by an aggregate value."
      },
      {
        "txVideoBytes": "The total number of video bytes sent, represented by an aggregate value."
      },
      {
        "rxAudioBytes": "The total number of audio bytes received, represented by an aggregate value."
      },
      {
        "rxVideoBytes": "The total number of video bytes received, represented by an aggregate value."
      },
      {
        "txKBitRate": "The actual bitrate (Kbps) while sending the local video stream."
      },
      {
        "rxKBitRate": "The receiving bitrate (Kbps)."
      },
      {
        "rxAudioKBitRate": "The bitrate (Kbps) of receiving the audio."
      },
      {
        "txAudioKBitRate": "The bitrate (Kbps) of sending the audio packet."
      },
      {
        "rxVideoKBitRate": "The bitrate (Kbps) of receiving the video."
      },
      {
        "txVideoKBitRate": "The bitrate (Kbps) of sending the video."
      },
      {
        "lastmileDelay": "The client-to-server delay (milliseconds)."
      },
      {
        "txPacketLossRate": "The packet loss rate (%) from the client to the Agora server before applying the anti-packet-loss algorithm."
      },
      {
        "rxPacketLossRate": "The packet loss rate (%) from the Agora server to the client before using the anti-packet-loss method."
      },
      {
        "userCount": "The number of users in the channel."
      },
      {
        "cpuAppUsage": "Application CPU usage (%).\n The value of cpuAppUsage is always reported as 0 in the OnLeaveChannel callback.\n As of Android 8.1, you cannot get the CPU usage from this attribute due to system limitations."
      },
      {
        "cpuTotalUsage": "The system CPU usage (%). For Windows, in the multi-kernel environment, this member represents the average CPU usage. The value = (100 - System Idle Progress in Task Manager)/100.\n The value of cpuTotalUsage is always reported as 0 in the OnLeaveChannel callback.\n As of Android 8.1, you cannot get the CPU usage from this attribute due to system limitations."
      },
      {
        "connectTimeMs": "The duration (ms) between the SDK starts connecting and the connection is established. If the value reported is 0, it means invalid."
      },
      {
        "gatewayRtt": "The round-trip time delay (ms) from the client to the local router. This property is disabled on devices running iOS 14 or later, and enabled on devices running versions earlier than iOS 14 by default. To enable this property on devices running iOS 14 or later,. On Android, to get gatewayRtt, ensure that you add the android.permission.ACCESS_WIFI_STATE permission after </application> in the AndroidManifest.xml file in your project."
      },
      {
        "memoryAppUsageRatio": "The memory ratio occupied by the app (%). This value is for reference only. Due to system limitations, you may not get this value."
      },
      {
        "memoryTotalUsageRatio": "The memory occupied by the system (%). This value is for reference only. Due to system limitations, you may not get this value."
      },
      {
        "memoryAppUsageInKbytes": "The memory size occupied by the app (KB). This value is for reference only. Due to system limitations, you may not get this value."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_screenaudioparameters",
    "name": "ScreenAudioParameters",
    "description": "The audio configuration for the shared screen stream.\n\nOnly available where captureAudio is true.",
    "parameters": [
      {
        "sampleRate": "Audio sample rate (Hz). The default value is 16000."
      },
      {
        "channels": "The number of audio channels. The default value is 2, which means stereo."
      },
      {
        "captureSignalVolume": "The volume of the captured system audio. The value range is [0, 100]. The default value is 100."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_screencaptureconfiguration",
    "name": "ScreenCaptureConfiguration",
    "description": "The configuration of the captured screen.",
    "parameters": [
      {
        "isCaptureWindow": "Whether to capture the window on the screen: true : Capture the window. false : (Default) Capture the screen, not the window."
      },
      {
        "displayId": "(macOS only) The display ID of the screen. This parameter takes effect only when you want to capture the screen on macOS."
      },
      {
        "screenRect": "(Windows only) The relative position of the shared screen to the virtual screen. This parameter takes effect only when you want to capture the screen on Windows."
      },
      {
        "windowId": "(For Windows and macOS only) Window ID. This parameter takes effect only when you want to capture the window."
      },
      {
        "parameters": "(For Windows and macOS only) The screen capture configuration. See ScreenCaptureParameters."
      },
      {
        "regionRect": "(For Windows and macOS only) The relative position of the shared region to the whole screen. See Rectangle. If you do not set this parameter, the SDK shares the whole screen. If the region you set exceeds the boundary of the screen, only the region within in the screen is shared. If you set width or height in Rectangle as 0, the whole screen is shared."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_screencaptureparameters",
    "name": "ScreenCaptureParameters",
    "description": "Screen sharing configurations.",
    "parameters": [
      {
        "dimensions": "The video encoding resolution of the screen sharing stream. See VideoDimensions. The default value is 1920 × 1080, that is, 2,073,600 pixels. Agora uses the value of this parameter to calculate the charges. If the screen dimensions are different from the value of this parameter, Agora applies the following strategies for encoding. Suppose dimensions is set to 1920 × 1080:\n If the value of the screen dimensions is lower than that of dimensions, for example, 1000 × 1000 pixels, the SDK uses the screen dimensions, that is, 1000 × 1000 pixels, for encoding.\n If the value of the screen dimensions is higher than that of dimensions, for example, 2000 × 1500, the SDK uses the maximum value under dimensions with the aspect ratio of the screen dimension (4:3) for encoding, that is, 1440 × 1080. When setting the encoding resolution in the scenario of sharing documents (SCREEN_SCENARIO_DOCUMENT), choose one of the following two methods:\n If you require the best image quality, it is recommended to set the encoding resolution to be the same as the capture resolution.\n If you wish to achieve a relative balance between image quality, bandwidth, and system performance, then:\n When the capture resolution is greater than 1920 × 1080, it is recommended that the encoding resolution is not less than 1920 × 1080.\n When the capture resolution is less than 1920 × 1080, it is recommended that the encoding resolution is not less than 1280 × 720."
      },
      {
        "frameRate": "On Windows and macOS, this represents the video encoding frame rate (fps) of the screen sharing stream. The frame rate (fps) of the shared region. The default value is 5. Agora does not recommend setting this to a value greater than 15."
      },
      {
        "bitrate": "The bitrate of the shared region. On Windows and macOS, this represents the video encoding bitrate of the screen sharing stream. The bitrate (Kbps) of the shared region. The default value is 0 (the SDK works out a bitrate according to the dimensions of the current screen)."
      },
      {
        "captureMouseCursor": "Whether to capture the mouse in screen sharing: true : (Default) Capture the mouse. false : Do not capture the mouse. Due to macOS system restrictions, setting this parameter to false is ineffective during screen sharing (it has no impact when sharing a window)."
      },
      {
        "windowFocus": "Whether to bring the window to the front when calling the StartScreenCaptureByWindowId method to share it: true : Bring the window to the front. false : (Default) Do not bring the window to the front. Due to macOS system limitations, when setting this member to bring the window to the front, if the current app has multiple windows, only the main window will be brought to the front."
      },
      {
        "excludeWindowList": "The ID list of the windows to be blocked. When calling StartScreenCaptureByDisplayId to start screen sharing, you can use this parameter to block a specified window. When calling UpdateScreenCaptureParameters to update screen sharing configurations, you can use this parameter to dynamically block a specified window."
      },
      {
        "enableHighLight": "(For macOS and Windows only) Whether to place a border around the shared window or screen: true : Place a border. false : (Default) Do not place a border. When you share a part of a window or screen, the SDK places a border around the entire window or screen if you set this parameter to true."
      },
      {
        "highLightColor": "(For macOS and Windows only)\n On Windows platforms, the color of the border in ARGB format. The default value is 0xFF8CBF26.\n On macOS, COLOR_CLASS refers to NSColor."
      },
      {
        "highLightWidth": "(For macOS and Windows only) The width (px) of the border. The default value is 5, and the value range is (0, 50]. This parameter only takes effect when highLighted is set to true."
      },
      {
        "excludeWindowCount": "The number of windows to be excluded. On the Windows platform, the maximum value of this parameter is 24; if this value is exceeded, excluding the window fails."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_screencaptureparameters2",
    "name": "ScreenCaptureParameters2",
    "description": "Screen sharing configurations.",
    "parameters": [
      {
        "captureAudio": "Determines whether to capture system audio during screen sharing: true : Capture system audio. false : (Default) Do not capture system audio.\n Due to system limitations, capturing system audio is only applicable to Android API level 29 and later (that is, Android 10 and later).\n To improve the success rate of capturing system audio during screen sharing, ensure that you have called the SetAudioScenario method and set the audio scenario to AUDIO_SCENARIO_GAME_STREAMING."
      },
      {
        "audioParams": "The audio configuration for the shared screen stream. See ScreenAudioParameters. This parameter only takes effect when captureAudio is true."
      },
      {
        "captureVideo": "Whether to capture the screen when screen sharing: true : (Default) Capture the screen. false : Do not capture the screen. Due to system limitations, the capture screen is only applicable to Android API level 21 and above, that is, Android 5 and above."
      },
      {
        "videoParams": "The video configuration for the shared screen stream. See ScreenVideoParameters. This parameter only takes effect when captureVideo is true."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_screencapturesourceinfo",
    "name": "ScreenCaptureSourceInfo",
    "description": "The information about the specified shareable window or screen.",
    "parameters": [
      {
        "type": "The type of the shared target. See ScreenCaptureSourceType."
      },
      {
        "sourceId": "The window ID for a window or the display ID for a screen."
      },
      {
        "sourceName": "The name of the window or screen. UTF-8 encoding."
      },
      {
        "thumbImage": "The image content of the thumbnail. See ThumbImageBuffer."
      },
      {
        "iconImage": "The image content of the icon. See ThumbImageBuffer."
      },
      {
        "processPath": "The process to which the window belongs. UTF-8 encoding."
      },
      {
        "sourceTitle": "The title of the window. UTF-8 encoding."
      },
      {
        "primaryMonitor": "Determines whether the screen is the primary display: true : The screen is the primary display. false : The screen is not the primary display."
      },
      {
        "position": "The position of a window relative to the entire screen space (including all shareable screens). See Rectangle."
      },
      {
        "sourceDisplayId": "(For Windows only) Screen ID where the window is located. If the window is displayed across multiple screens, this parameter indicates the ID of the screen with which the window has the largest intersection area. If the window is located outside of the visible screens, the value of this member is -2."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_screenvideoparameters",
    "name": "ScreenVideoParameters",
    "description": "The video configuration for the shared screen stream.",
    "parameters": [
      {
        "dimensions": "The video encoding dimension. The default value is 1280 × 720."
      },
      {
        "frameRate": "The video encoding frame rate (fps). The default value is 15."
      },
      {
        "bitrate": "The video encoding bitrate (Kbps)."
      },
      {
        "contentHint": "The content hint for screen sharing."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_segmentationproperty",
    "name": "SegmentationProperty",
    "description": "Processing properties for background images.",
    "parameters": [
      {
        "modelType": "The type of algorithms to user for background processing. See SEG_MODEL_TYPE."
      },
      {
        "greenCapacity": "The accuracy range for recognizing background colors in the image. The value range is [0,1], and the default value is 0.5. The larger the value, the wider the range of identifiable shades of pure color. When the value of this parameter is too large, the edge of the portrait and the pure color in the portrait range are also detected. Agora recommends that you dynamically adjust the value of this parameter according to the actual effect. This parameter only takes effect when modelType is set to SEG_MODEL_GREEN."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_simulcaststreamconfig",
    "name": "SimulcastStreamConfig",
    "description": "The configuration of the low-quality video stream.",
    "parameters": [
      {
        "dimensions": "The video dimension. See VideoDimensions. The default value is 50% of the high-quality video stream."
      },
      {
        "kBitrate": "Video receive bitrate (Kbps), represented by an instantaneous value. This parameter does not need to be set. The SDK automatically matches the most suitable bitrate based on the video resolution and frame rate you set."
      },
      {
        "frameRate": "The frame rate (fps) of the local video. The default value is 5."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_snapshotconfig",
    "name": "SnapshotConfig",
    "description": "The snapshot configuration.",
    "parameters": [
      {
        "filePath": "The local path (including filename extensions) of the snapshot. For example:\n Windows: C:\\Users\\<user_name>\\AppData\\Local\\Agora\\<process_name>\\example.jpg\n iOS: /App Sandbox/Library/Caches/example.jpg\n macOS: ～/Library/Logs/example.jpg\n Android: /storage/emulated/0/Android/data/<package name>/files/example.jpg Ensure that the path you specify exists and is writable."
      },
      {
        "position": "The position of the snapshot video frame in the video pipeline. See VIDEO_MODULE_POSITION."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_spatialaudioparams",
    "name": "SpatialAudioParams",
    "description": "The spatial audio parameters.",
    "parameters": [
      {
        "speaker_azimuth": "The azimuth angle of the remote user or media player relative to the local user. The value range is [0,360], and the unit is degrees, The values are as follows:\n 0: (Default) 0 degrees, which means directly in front on the horizontal plane.\n 90: 90 degrees, which means directly to the left on the horizontal plane.\n 180: 180 degrees, which means directly behind on the horizontal plane.\n 270: 270 degrees, which means directly to the right on the horizontal plane.\n 360: 360 degrees, which means directly in front on the horizontal plane."
      },
      {
        "speaker_elevation": "The elevation angle of the remote user or media player relative to the local user. The value range is [-90,90], and the unit is degrees, The values are as follows:\n 0: (Default) 0 degrees, which means that the horizontal plane is not rotated.\n -90: -90 degrees, which means that the horizontal plane is rotated 90 degrees downwards.\n 90: 90 degrees, which means that the horizontal plane is rotated 90 degrees upwards."
      },
      {
        "speaker_distance": "The distance of the remote user or media player relative to the local user. The value range is [1,50], and the unit is meters. The default value is 1 meter."
      },
      {
        "speaker_orientation": "The orientation of the remote user or media player relative to the local user. The value range is [0,180], and the unit is degrees, The values are as follows:\n 0: (Default) 0 degrees, which means that the sound source and listener face the same direction.\n 180: 180 degrees, which means that the sound source and listener face each other."
      },
      {
        "enable_blur": "Whether to enable audio blurring: true : Enable audio blurring. false : (Default) Disable audio blurring."
      },
      {
        "enable_air_absorb": "Whether to enable air absorption, that is, to simulate the sound attenuation effect of sound transmitting in the air; under a certain transmission distance, the attenuation speed of high-frequency sound is fast, and the attenuation speed of low-frequency sound is slow. true : (Default) Enable air absorption. Make sure that the value of speaker_attenuation is not 0; otherwise, this setting does not take effect. false : Disable air absorption."
      },
      {
        "speaker_attenuation": "The sound attenuation coefficient of the remote user or media player. The value range is [0,1]. The values are as follows:\n 0: Broadcast mode, where the volume and timbre are not attenuated with distance, and the volume and timbre heard by local users do not change regardless of distance.\n (0,0.5): Weak attenuation mode, where the volume and timbre only have a weak attenuation during the propagation, and the sound can travel farther than that in a real environment. enable_air_absorb needs to be enabled at the same time.\n 0.5: (Default) Simulates the attenuation of the volume in the real environment; the effect is equivalent to not setting the speaker_attenuation parameter.\n (0.5,1]: Strong attenuation mode, where volume and timbre attenuate rapidly during the propagation. enable_air_absorb needs to be enabled at the same time."
      },
      {
        "enable_doppler": "Whether to enable the Doppler effect: When there is a relative displacement between the sound source and the receiver of the sound source, the tone heard by the receiver changes. true : Enable the Doppler effect. false : (Default) Disable the Doppler effect.\n This parameter is suitable for scenarios where the sound source is moving at high speed (for example, racing games). It is not recommended for common audio and video interactive scenarios (for example, voice chat, co-streaming, or online KTV).\n When this parameter is enabled, Agora recommends that you set a regular period (such as 30 ms), and then call the UpdatePlayerPositionInfo, UpdateSelfPosition, and UpdateRemotePosition methods to continuously update the relative distance between the sound source and the receiver. The following factors can cause the Doppler effect to be unpredictable or the sound to be jittery: the period of updating the distance is too long, the updating period is irregular, or the distance information is lost due to network packet loss or delay."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_spatialaudiozone",
    "name": "SpatialAudioZone",
    "description": "Sound insulation area settings.",
    "parameters": [
      {
        "zoneSetId": "The ID of the sound insulation area."
      },
      {
        "position": "The spatial center point of the sound insulation area. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn."
      },
      {
        "forward": "Starting at position, the forward unit vector. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn."
      },
      {
        "right": "Starting at position, the right unit vector. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn."
      },
      {
        "up": "Starting at position, the up unit vector. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn."
      },
      {
        "forwardLength": "The entire sound insulation area is regarded as a cube; this represents the length of the forward side in the unit length of the game engine."
      },
      {
        "rightLength": "The entire sound insulation area is regarded as a cube; this represents the length of the right side in the unit length of the game engine."
      },
      {
        "upLength": "The entire sound insulation area is regarded as a cube; this represents the length of the up side in the unit length of the game engine."
      },
      {
        "audioAttenuation": "The sound attenuation coefficient when users within the sound insulation area communicate with external users. The value range is [0,1]. The values are as follows:\n 0: Broadcast mode, where the volume and timbre are not attenuated with distance, and the volume and timbre heard by local users do not change regardless of distance.\n (0,0.5): Weak attenuation mode, that is, the volume and timbre are only weakly attenuated during the propagation process, and the sound can travel farther than the real environment.\n 0.5: (Default) simulates the attenuation of the volume in the real environment; the effect is equivalent to not setting the audioAttenuation parameter.\n (0.5,1]: Strong attenuation mode (default value is 1), that is, the volume and timbre attenuate rapidly during propagation."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_srcinfo",
    "name": "SrcInfo",
    "description": "Information about the video bitrate of the media resource being played.",
    "parameters": [
      {
        "bitrateInKbps": "The video bitrate (Kbps) of the media resource being played."
      },
      {
        "name": "The name of the media resource."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_thumbimagebuffer",
    "name": "ThumbImageBuffer",
    "description": "The image content of the thumbnail or icon. Set in ScreenCaptureSourceInfo.\n\nThe default image is in the ARGB format. If you need to use another format, you need to convert the image on your own.",
    "parameters": [
      {
        "buffer": "The buffer of the thumbnail or icon."
      },
      {
        "length": "The buffer length of the thumbnail or icon, in bytes."
      },
      {
        "width": "The actual width (px) of the thumbnail or icon."
      },
      {
        "height": "The actual height (px) of the thumbnail or icon."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_transcodinguser",
    "name": "TranscodingUser",
    "description": "Transcoding configurations of each host.",
    "parameters": [
      {
        "uid": "The user ID of the host."
      },
      {
        "x": "The x coordinate (pixel) of the host's video on the output video frame (taking the upper left corner of the video frame as the origin). The value range is [0, width], where width is the width set in LiveTranscoding."
      },
      {
        "y": "The y coordinate (pixel) of the host's video on the output video frame (taking the upper left corner of the video frame as the origin). The value range is [0, height], where height is the height set in LiveTranscoding."
      },
      {
        "width": "The width (pixel) of the host's video."
      },
      {
        "height": "The height (pixel) of the host's video."
      },
      {
        "zOrder": "The layer index number of the host's video. The value range is [0, 100].\n 0: (Default) The host's video is the bottom layer.\n 100: The host's video is the top layer.\n If the value is less than 0 or greater than 100, ERR_INVALID_ARGUMENT error is returned.\n Setting zOrder to 0 is supported."
      },
      {
        "alpha": "The transparency of the host's video. The value range is [0.0,1.0].\n 0.0: Completely transparent.\n 1.0: (Default) Opaque."
      },
      {
        "audioChannel": "The audio channel used by the host's audio in the output audio. The default value is 0, and the value range is [0, 5]. 0 : (Recommended) The defaut setting, which supports dual channels at most and depends on the upstream of the host. 1 : The host's audio uses the FL audio channel. If the host's upstream uses multiple audio channels, the Agora server mixes them into mono first. 2 : The host's audio uses the FC audio channel. If the host's upstream uses multiple audio channels, the Agora server mixes them into mono first. 3 : The host's audio uses the FR audio channel. If the host's upstream uses multiple audio channels, the Agora server mixes them into mono first. 4 : The host's audio uses the BL audio channel. If the host's upstream uses multiple audio channels, the Agora server mixes them into mono first. 5 : The host's audio uses the BR audio channel. If the host's upstream uses multiple audio channels, the Agora server mixes them into mono first. 0xFF or a value greater than 5 : The host's audio is muted, and the Agora server removes the host's audio. If the value is not 0, a special player is required."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_transcodingvideostream",
    "name": "TranscodingVideoStream",
    "description": "The video streams for local video mixing.",
    "parameters": [
      {
        "sourceType": "The video source type for local video mixing. See VIDEO_SOURCE_TYPE."
      },
      {
        "remoteUserUid": "The user ID of the remote user. Use this parameter only when the source type is VIDEO_SOURCE_REMOTE for local video mixing."
      },
      {
        "imageUrl": "The URL of the image. Use this parameter only when the source type is the image for local video mixing."
      },
      {
        "mediaPlayerId": "(Optional) Media player ID. Use the parameter only when you set sourceType to VIDEO_SOURCE_MEDIA_PLAYER."
      },
      {
        "x": "The relative lateral displacement of the top left corner of the video for local video mixing to the origin (the top left corner of the canvas)."
      },
      {
        "y": "The relative longitudinal displacement of the top left corner of the captured video to the origin (the top left corner of the canvas)."
      },
      {
        "width": "The width (px) of the video for local video mixing on the canvas."
      },
      {
        "height": "The height (px) of the video for local video mixing on the canvas."
      },
      {
        "zOrder": "The number of the layer to which the video for the local video mixing belongs. The value range is [0, 100].\n 0: (Default) The layer is at the bottom.\n 100: The layer is at the top."
      },
      {
        "alpha": "The transparency of the video for local video mixing. The value range is [0.0, 1.0]. 0.0 indicates that the video is completely transparent, and 1.0 indicates that it is opaque."
      },
      {
        "mirror": "Whether to mirror the video for the local video mixing. true : Mirror the video for the local video mixing. false : (Default) Do not mirror the video for the local video mixing. This parameter only takes effect on video source types that are cameras."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_uplinknetworkinfo",
    "name": "UplinkNetworkInfo",
    "description": "The uplink network information.",
    "parameters": [
      {
        "video_encoder_target_bitrate_bps": "The target video encoder bitrate (bps)."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_useraudiospectruminfo",
    "name": "UserAudioSpectrumInfo",
    "description": "Audio spectrum information of the remote user.",
    "parameters": [
      {
        "uid": "The user ID of the remote user."
      },
      {
        "spectrumData": "Audio spectrum information of the remote user. See AudioSpectrumData."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_userinfo",
    "name": "UserInfo",
    "description": "The information of the user.",
    "parameters": [
      {
        "uid": "The user ID."
      },
      {
        "userAccount": "User account. The maximum data length is MAX_USER_ACCOUNT_LENGTH_TYPE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_videocanvas",
    "name": "VideoCanvas",
    "description": "Attributes of the video canvas object.",
    "parameters": [
      {
        "uid": "User ID that publishes the video source."
      },
      {
        "subviewUid": "The ID of the user who publishes a specific sub-video stream within the mixed video stream."
      },
      {
        "view": "The video display window. In one VideoCanvas, you can only choose to set either view or surfaceTexture. If both are set, only the settings in view take effect."
      },
      {
        "renderMode": "The rendering mode of the video. See RENDER_MODE_TYPE."
      },
      {
        "mirrorMode": "The mirror mode of the view. See VIDEO_MIRROR_MODE_TYPE.\n For the mirror mode of the local video view: If you use a front camera, the SDK enables the mirror mode by default; if you use a rear camera, the SDK disables the mirror mode by default.\n For the remote user: The mirror mode is disabled by default."
      },
      {
        "sourceType": "The type of the video source. See VIDEO_SOURCE_TYPE."
      },
      {
        "setupMode": "Setting mode of the view. See VIDEO_VIEW_SETUP_MODE."
      },
      {
        "mediaPlayerId": "The ID of the media player. You can get the Device ID by calling GetId."
      },
      {
        "cropArea": "(Optional) Display area of the video frame, see Rectangle. width and height represent the video pixel width and height of the area. The default value is null (width or height is 0), which means that the actual resolution of the video frame is displayed."
      },
      {
        "backgroundColor": "The background color of the video canvas in RGBA format. The default value is 0x00000000, which represents completely transparent black."
      },
      {
        "enableAlphaMask": "(Optional) Whether to enable alpha mask rendering: true : Enable alpha mask rendering. false : (Default) Disable alpha mask rendering. Alpha mask rendering can create images with transparent effects and extract portraits from videos. When used in combination with other methods, you can implement effects such as portrait-in-picture and watermarking.\n The receiver can render alpha channel information only when the sender enables alpha transmission.\n To enable alpha transmission,."
      },
      {
        "position": "The observation position of the video frame in the video link. See VIDEO_MODULE_POSITION."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_videodenoiseroptions",
    "name": "VideoDenoiserOptions",
    "description": "Video noise reduction options.",
    "parameters": [
      {
        "level": "Video noise reduction level."
      },
      {
        "mode": "Video noise reduction mode."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_videodeviceinfo",
    "name": "DeviceInfo",
    "description": "The DeviceInfo class that contains the ID and device name of the video devices.",
    "parameters": [
      {
        "deviceId": "The device ID."
      },
      {
        "deviceName": "The device name."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_videodimensions",
    "name": "VideoDimensions",
    "description": "The video dimension.",
    "parameters": [
      {
        "width": "The width (pixels) of the video."
      },
      {
        "height": "The height (pixels) of the video."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_videoencoderconfiguration",
    "name": "VideoEncoderConfiguration",
    "description": "Video encoder configurations.",
    "parameters": [
      {
        "dimensions": "The dimensions of the encoded video (px). See VideoDimensions. This parameter measures the video encoding quality in the format of length × width. The default value is 960 × 540. You can set a custom value."
      },
      {
        "codecType": "The codec type of the local video stream. See VIDEO_CODEC_TYPE."
      },
      {
        "frameRate": "The frame rate (fps) of the encoding video frame. The default value is 15. See FRAME_RATE."
      },
      {
        "bitrate": "The encoding bitrate (Kbps) of the video. See BITRATE. This parameter does not need to be set; keeping the default value STANDARD_BITRATE is sufficient. The SDK automatically matches the most suitable bitrate based on the video resolution and frame rate you have set. For the correspondence between video resolution and frame rate, see."
      },
      {
        "minBitrate": "The minimum encoding bitrate (Kbps) of the video. The SDK automatically adjusts the encoding bitrate to adapt to the network conditions. Using a value greater than the default value forces the video encoder to output high-quality images but may cause more packet loss and sacrifice the smoothness of the video transmission. Unless you have special requirements for image quality, Agora does not recommend changing this value. This parameter only applies to the interactive streaming profile."
      },
      {
        "orientationMode": "The orientation mode of the encoded video. See ORIENTATION_MODE."
      },
      {
        "degradationPreference": "Video degradation preference under limited bandwidth. See DEGRADATION_PREFERENCE. When this parameter is set to MAINTAIN_FRAMERATE (1) or MAINTAIN_BALANCED (2), orientationMode needs to be set to ORIENTATION_MODE_ADAPTIVE (0) at the same time, otherwise the setting will not take effect."
      },
      {
        "mirrorMode": "Sets the mirror mode of the published local video stream. It only affects the video that the remote user sees. See VIDEO_MIRROR_MODE_TYPE. By default, the video is not mirrored."
      },
      {
        "advanceOptions": "Advanced options for video encoding. See AdvanceOptions."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_videoformat",
    "name": "VideoFormat",
    "description": "The format of the video frame.",
    "parameters": [
      {
        "width": "The width (px) of the video frame. The default value is 960."
      },
      {
        "height": "The height (px) of the video frame. The default value is 540."
      },
      {
        "fps": "The video frame rate (fps). The default value is 15."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_videoframe",
    "name": "VideoFrame",
    "description": "Configurations of the video frame.\n\nNote that the buffer provides a pointer to a pointer. This interface cannot modify the pointer of the buffer, but it can modify the content of the buffer.",
    "parameters": [
      {
        "type": "The pixel format. See VIDEO_PIXEL_FORMAT."
      },
      {
        "width": "The width of the video, in pixels."
      },
      {
        "height": "The height of the video, in pixels."
      },
      {
        "yStride": "For YUV data, the line span of the Y buffer; for RGBA data, the total data length. When dealing with video data, it is necessary to process the offset between each line of pixel data based on this parameter, otherwise it may result in image distortion."
      },
      {
        "uStride": "For YUV data, the line span of the U buffer; for RGBA data, the value is 0. When dealing with video data, it is necessary to process the offset between each line of pixel data based on this parameter, otherwise it may result in image distortion."
      },
      {
        "vStride": "For YUV data, the line span of the V buffer; for RGBA data, the value is 0. When dealing with video data, it is necessary to process the offset between each line of pixel data based on this parameter, otherwise it may result in image distortion."
      },
      {
        "yBuffer": "For YUV data, the pointer to the Y buffer; for RGBA data, the data buffer."
      },
      {
        "uBuffer": "For YUV data, the pointer to the U buffer; for RGBA data, the value is 0."
      },
      {
        "vBuffer": "For YUV data, the pointer to the V buffer; for RGBA data, the value is 0."
      },
      {
        "rotation": "The clockwise rotation of the video frame before rendering. Supported values include 0, 90, 180, and 270 degrees."
      },
      {
        "renderTimeMs": "The Unix timestamp (ms) when the video frame is rendered. This timestamp can be used to guide the rendering of the video frame. This parameter is required."
      },
      {
        "avsync_type": "Reserved for future use."
      },
      {
        "metadata_buffer": "This parameter only applies to video data in Texture format. The MetaData buffer. The default value is NULL."
      },
      {
        "metadata_size": "This parameter only applies to video data in Texture format. The MetaData size. The default value is 0."
      },
      {
        "sharedContext": "This parameter only applies to video data in Texture format. EGL Context."
      },
      {
        "textureId": "This parameter only applies to video data in Texture format. Texture ID."
      },
      {
        "d3d11Texture2d": "This parameter only applies to video data in Windows Texture format. It represents a pointer to an object of type ID3D11Texture2D, which is used by a video frame."
      },
      {
        "matrix": "This parameter only applies to video data in Texture format. Incoming 4 × 4 transformational matrix. The typical value is a unit matrix."
      },
      {
        "colorSpace": "By default, the color space properties of video frames will apply the Full Range and BT.709 standard configurations. You can configure the settings according your needs for custom video capturing and rendering."
      },
      {
        "alphaBuffer": "The alpha channel data output by using portrait segmentation algorithm. This data matches the size of the video frame, with each pixel value ranging from [0,255], where 0 represents the background and 255 represents the foreground (portrait). By setting this parameter, you can render the video background into various effects, such as transparent, solid color, image, video, etc.\n In custom video rendering scenarios, ensure that both the video frame and alphaBuffer are of the Full Range type; other types may cause abnormal alpha data rendering.\n Make sure that alphaBuffer is exactly the same size as the video frame (width × height), otherwise it may cause the app to crash."
      },
      {
        "alphaStitchMode": "When the video frame contains alpha channel data, it represents the relative position of alphaBuffer and the video frame. See ALPHA_STITCH_MODE."
      },
      {
        "metaInfo": "The meta information in the video frame. To use this parameter, contact."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_videoframebufferconfig",
    "name": "VideoFrameBufferConfig",
    "description": "Video frame settings.",
    "parameters": [
      {
        "type": "The video source type. See VIDEO_SOURCE_TYPE."
      },
      {
        "id": "The user ID."
      },
      {
        "key": "The channel name."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_videolayout",
    "name": "VideoLayout",
    "description": "Layout information of a specific sub-video stream within the mixed stream.",
    "parameters": [
      {
        "channelId": "The channel name to which the sub-video stream belongs."
      },
      {
        "uid": "User ID who published this sub-video stream."
      },
      {
        "strUid": "Reserved for future use."
      },
      {
        "x": "X-coordinate (px) of the sub-video stream on the mixing canvas. The relative lateral displacement of the top left corner of the video for video mixing to the origin (the top left corner of the canvas)."
      },
      {
        "y": "Y-coordinate (px) of the sub-video stream on the mixing canvas. The relative longitudinal displacement of the top left corner of the captured video to the origin (the top left corner of the canvas)."
      },
      {
        "width": "Width (px) of the sub-video stream."
      },
      {
        "height": "Heitht (px) of the sub-video stream."
      },
      {
        "videoState": "Status of the sub-video stream on the video mixing canvas.\n 0: Normal. The sub-video stream has been rendered onto the mixing canvas.\n 1: Placeholder image. The sub-video stream has no video frames and is displayed as a placeholder on the mixing canvas.\n 2: Black image. The sub-video stream is replaced by a black image."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_videorenderingtracinginfo",
    "name": "VideoRenderingTracingInfo",
    "description": "Indicators during video frame rendering progress.",
    "parameters": [
      {
        "elapsedTime": "The time interval (ms) from StartMediaRenderingTracing to SDK triggering the OnVideoRenderingTracingResult callback. Agora recommends you call StartMediaRenderingTracing before joining a channel."
      },
      {
        "start2JoinChannel": "The time interval (ms) from StartMediaRenderingTracing to JoinChannel [1/2] or JoinChannel [2/2]. A negative number indicates that StartMediaRenderingTracing is called after calling JoinChannel [2/2]."
      },
      {
        "join2JoinSuccess": "The time interval (ms) from JoinChannel [1/2] or JoinChannel [2/2] to successfully joining the channel."
      },
      {
        "joinSuccess2RemoteJoined": "If the local user calls StartMediaRenderingTracing before successfully joining the channel, this value is the time interval (ms) from the local user successfully joining the channel to the remote user joining the channel.\n If the local user calls StartMediaRenderingTracing after successfully joining the channel, the value is the time interval (ms) from StartMediaRenderingTracing to when the remote user joins the channel.\n If the local user calls StartMediaRenderingTracing after the remote user joins the channel, the value is 0 and meaningless.\n In order to reduce the time of rendering the first frame for remote users, Agora recommends that the local user joins the channel when the remote user is in the channel to reduce this value."
      },
      {
        "remoteJoined2SetView": "If the local user calls StartMediaRenderingTracing before the remote user joins the channel, this value is the time interval (ms) from when the remote user joins the channel to when the local user sets the remote view.\n If the local user calls StartMediaRenderingTracing after the remote user joins the channel, this value is the time interval (ms) from calling StartMediaRenderingTracing to setting the remote view.\n If the local user calls StartMediaRenderingTracing after setting the remote view, the value is 0 and has no effect.\n In order to reduce the time of rendering the first frame for remote users, Agora recommends that the local user sets the remote view before the remote user joins the channel, or sets the remote view immediately after the remote user joins the channel to reduce this value."
      },
      {
        "remoteJoined2UnmuteVideo": "If the local user calls StartMediaRenderingTracing before the remote user joins the channel, this value is the time interval (ms) from the remote user joining the channel to subscribing to the remote video stream.\n If the local user calls StartMediaRenderingTracing after the remote user joins the channel, this value is the time interval (ms) from StartMediaRenderingTracing to subscribing to the remote video stream.\n If the local user calls StartMediaRenderingTracing after subscribing to the remote video stream, the value is 0 and has no effect.\n In order to reduce the time of rendering the first frame for remote users, Agora recommends that after the remote user joins the channel, the local user immediately subscribes to the remote video stream to reduce this value."
      },
      {
        "remoteJoined2PacketReceived": "If the local user calls StartMediaRenderingTracing before the remote user joins the channel, this value is the time interval (ms) from when the remote user joins the channel to when the local user receives the remote video stream.\n If the local user calls StartMediaRenderingTracing after the remote user joins the channel, this value is the time interval (ms) from StartMediaRenderingTracing to receiving the remote video stream.\n If the local user calls StartMediaRenderingTracing after receiving the remote video stream, the value is 0 and has no effect.\n In order to reduce the time of rendering the first frame for remote users, Agora recommends that the remote user publishes video streams immediately after joining the channel, and the local user immediately subscribes to remote video streams to reduce this value."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_videosubscriptionoptions",
    "name": "VideoSubscriptionOptions",
    "description": "Video subscription options.",
    "parameters": [
      {
        "type": "The video stream type that you want to subscribe to. The default value is VIDEO_STREAM_HIGH, indicating that the high-quality video streams are subscribed. See VIDEO_STREAM_TYPE."
      },
      {
        "encodedFrameOnly": "Whether to subscribe to encoded video frames only: true : Subscribe to the encoded video data (structured data) only; the SDK does not decode or render raw video data. false : (Default) Subscribe to both raw video data and encoded video data."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_videosurface",
    "name": "VideoSurface",
    "description": "This class contains Unity native methods related to video rendering.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_videosurfaceyuv",
    "name": "VideoSurfaceYUV",
    "description": "Porivdes APIs for rendering videos. This class inherits all APIs from the VideoSurface class, but enables you to render video images with high resolutions (such as 4K) faster and at higher frame rates. As of v4.2.0, Agora Unity SDK does not support rendering different video sources with both VideoSurface and VideoSurfaceYUV at the same time. Specifically, after successfully creating IRtcEngine, if the first view is rendered with VideoSurfaceYUV, then only VideoSurfaceYUV can be used for rendering throughout the entire lifecycle of IRtcEngine.",
    "parameters": [],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_virtualbackgroundsource",
    "name": "VirtualBackgroundSource",
    "description": "The custom background.",
    "parameters": [
      {
        "background_source_type": "The custom background. See BACKGROUND_SOURCE_TYPE."
      },
      {
        "color": "The type of the custom background image. The color of the custom background image. The format is a hexadecimal integer defined by RGB, without the # sign, such as 0xFFB6C1 for light pink. The default value is 0xFFFFFF, which signifies white. The value range is [0x000000, 0xffffff]. If the value is invalid, the SDK replaces the original background image with a white background image. This parameter is only applicable to custom backgrounds of the following types: BACKGROUND_COLOR : The background image is a solid-colored image of the color passed in by the parameter. BACKGROUND_IMG : If the image in source has a transparent background, the transparent background will be filled with the color passed in by the parameter."
      },
      {
        "source": "The local absolute path of the custom background image. Supports PNG, JPG, MP4, AVI, MKV, and FLV formats. If the path is invalid, the SDK will use either the original background image or the solid color image specified by color. This parameter takes effect only when the type of the custom background image is BACKGROUND_IMG or BACKGROUND_VIDEO."
      },
      {
        "blur_degree": "The degree of blurring applied to the custom background image. See BACKGROUND_BLUR_DEGREE. This parameter takes effect only when the type of the custom background image is BACKGROUND_BLUR."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_watermarkoptions",
    "name": "WatermarkOptions",
    "description": "Configurations of the watermark image.",
    "parameters": [
      {
        "visibleInPreview": "Whether the watermark is visible in the local preview view: true : (Default) The watermark is visible in the local preview view. false : The watermark is not visible in the local preview view."
      },
      {
        "positionInLandscapeMode": "When the adaptation mode of the watermark is FIT_MODE_COVER_POSITION, it is used to set the area of the watermark image in landscape mode. See Rectangle."
      },
      {
        "positionInPortraitMode": "When the adaptation mode of the watermark is FIT_MODE_COVER_POSITION, it is used to set the area of the watermark image in portrait mode. See Rectangle."
      },
      {
        "watermarkRatio": "When the watermark adaptation mode is FIT_MODE_USE_IMAGE_RATIO, this parameter is used to set the watermark coordinates. See WatermarkRatio."
      },
      {
        "mode": "The adaptation mode of the watermark. See WATERMARK_FIT_MODE."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "class_watermarkratio",
    "name": "WatermarkRatio",
    "description": "The position and size of the watermark on the screen.\n\nThe position and size of the watermark on the screen are determined by xRatio, yRatio, and widthRatio :\n (xRatio, yRatio) refers to the coordinates of the upper left corner of the watermark, which determines the distance from the upper left corner of the watermark to the upper left corner of the screen.\n The widthRatio determines the width of the watermark.",
    "parameters": [
      {
        "xRatio": "The x-coordinate of the upper left corner of the watermark. The horizontal position relative to the origin, where the upper left corner of the screen is the origin, and the x-coordinate is the upper left corner of the watermark. The value range is [0.0,1.0], and the default value is 0."
      },
      {
        "yRatio": "The y-coordinate of the upper left corner of the watermark. The vertical position relative to the origin, where the upper left corner of the screen is the origin, and the y-coordinate is the upper left corner of the screen. The value range is [0.0,1.0], and the default value is 0."
      },
      {
        "widthRatio": "The width of the watermark. The SDK calculates the height of the watermark proportionally according to this parameter value to ensure that the enlarged or reduced watermark image is not distorted. The value range is [0,1], and the default value is 0, which means no watermark is displayed."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_alphastitchmode",
    "name": "ALPHA_STITCH_MODE",
    "description": "The relative position of alphaBuffer and video frames.",
    "parameters": [
      {
        "NO_ALPHA_STITCH": "0: (Default) Only video frame, that is, alphaBuffer is not stitched with the video frame."
      },
      {
        "ALPHA_STITCH_UP": "1: alphaBuffer is above the video frame."
      },
      {
        "ALPHA_STITCH_BELOW": "2: alphaBuffer is below the video frame."
      },
      {
        "ALPHA_STITCH_LEFT": "3: alphaBuffer is to the left of the video frame."
      },
      {
        "ALPHA_STITCH_RIGHT": "4: alphaBuffer is to the right of the video frame."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_areacode",
    "name": "AREA_CODE",
    "description": "The region for connection, which is the region where the server the SDK connects to is located.",
    "parameters": [
      {
        "AREA_CODE_CN": "Mainland China."
      },
      {
        "AREA_CODE_NA": "North America."
      },
      {
        "AREA_CODE_EU": "Europe."
      },
      {
        "AREA_CODE_AS": "Asia, excluding Mainland China."
      },
      {
        "AREA_CODE_JP": "Japan."
      },
      {
        "AREA_CODE_IN": "India."
      },
      {
        "AREA_CODE_GLOB": "Global."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audiencelatencyleveltype",
    "name": "AUDIENCE_LATENCY_LEVEL_TYPE",
    "description": "The latency level of an audience member in interactive live streaming. This enum takes effect only when the user role is set to CLIENT_ROLE_AUDIENCE .",
    "parameters": [
      {
        "AUDIENCE_LATENCY_LEVEL_LOW_LATENCY": "1: Low latency."
      },
      {
        "AUDIENCE_LATENCY_LEVEL_ULTRA_LOW_LATENCY": "2: (Default) Ultra low latency."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audioainsmode",
    "name": "AUDIO_AINS_MODE",
    "description": "AI noise suppression modes.",
    "parameters": [
      {
        "AINS_MODE_BALANCED": "0: (Default) Balance mode. This mode allows for a balanced performance on noice suppression and time delay."
      },
      {
        "AINS_MODE_AGGRESSIVE": "1: Aggressive mode. In scenarios where high performance on noise suppression is required, such as live streaming outdoor events, this mode reduces nosie more dramatically, but may sometimes affect the original character of the audio."
      },
      {
        "AINS_MODE_ULTRALOWLATENCY": "2: Aggressive mode with low latency. The noise suppression delay of this mode is about only half of that of the balance and aggressive modes. It is suitable for scenarios that have high requirements on noise suppression with low latency, such as sing together online in real time."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audiocodecprofiletype",
    "name": "AUDIO_CODEC_PROFILE_TYPE",
    "description": "Self-defined audio codec profile.",
    "parameters": [
      {
        "AUDIO_CODEC_PROFILE_LC_AAC": "0: (Default) LC-AAC."
      },
      {
        "AUDIO_CODEC_PROFILE_HE_AAC": "1: HE-AAC."
      },
      {
        "AUDIO_CODEC_PROFILE_HE_AAC_V2": "2: HE-AAC v2."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audiocodectype",
    "name": "AUDIO_CODEC_TYPE",
    "description": "The codec type of audio.",
    "parameters": [
      {
        "AUDIO_CODEC_OPUS": "1: OPUS."
      },
      {
        "AUDIO_CODEC_PCMA": "3: PCMA."
      },
      {
        "AUDIO_CODEC_PCMU": "4: PCMU."
      },
      {
        "AUDIO_CODEC_G722": "5: G722."
      },
      {
        "AUDIO_CODEC_AACLC": "8: LC-AAC."
      },
      {
        "AUDIO_CODEC_HEAAC": "9: HE-AAC."
      },
      {
        "AUDIO_CODEC_JC1": "10: JC1."
      },
      {
        "AUDIO_CODEC_HEAAC2": "11: HE-AAC v2."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audiodualmonomode",
    "name": "AUDIO_DUAL_MONO_MODE",
    "description": "The channel mode.",
    "parameters": [
      {
        "AUDIO_DUAL_MONO_STEREO": "0: Original mode."
      },
      {
        "AUDIO_DUAL_MONO_L": "1: Left channel mode. This mode replaces the audio of the right channel with the audio of the left channel, which means the user can only hear the audio of the left channel."
      },
      {
        "AUDIO_DUAL_MONO_R": "2: Right channel mode. This mode replaces the audio of the left channel with the audio of the right channel, which means the user can only hear the audio of the right channel."
      },
      {
        "AUDIO_DUAL_MONO_MIX": "3: Mixed channel mode. This mode mixes the audio of the left channel and the right channel, which means the user can hear the audio of the left channel and the right channel at the same time."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audioeffectpreset",
    "name": "AUDIO_EFFECT_PRESET",
    "description": "Preset audio effects.\n\nTo get better audio effects, Agora recommends calling SetAudioProfile [1/2] and setting the profile parameter as recommended below before using the preset audio effects.",
    "parameters": [
      {
        "AUDIO_EFFECT_OFF": "Turn off voice effects, that is, use the original voice."
      },
      {
        "ROOM_ACOUSTICS_KTV": "The voice effect typical of a KTV venue."
      },
      {
        "ROOM_ACOUSTICS_VOCAL_CONCERT": "The voice effect typical of a concert hall."
      },
      {
        "ROOM_ACOUSTICS_STUDIO": "The voice effect typical of a recording studio."
      },
      {
        "ROOM_ACOUSTICS_PHONOGRAPH": "The voice effect typical of a vintage phonograph."
      },
      {
        "ROOM_ACOUSTICS_VIRTUAL_STEREO": "The virtual stereo effect, which renders monophonic audio as stereo audio."
      },
      {
        "ROOM_ACOUSTICS_SPACIAL": "A more spatial voice effect."
      },
      {
        "ROOM_ACOUSTICS_ETHEREAL": "A more ethereal voice effect."
      },
      {
        "ROOM_ACOUSTICS_VIRTUAL_SURROUND_SOUND": "Virtual surround sound, that is, the SDK generates a simulated surround sound field on the basis of stereo channels, thereby creating a surround sound effect. If the virtual surround sound is enabled, users need to use stereo audio playback devices to hear the anticipated audio effect."
      },
      {
        "ROOM_ACOUSTICS_CHORUS": "The audio effect of chorus. Agora recommends using this effect in chorus scenarios to enhance the sense of depth and dimension in the vocals."
      },
      {
        "ROOM_ACOUSTICS_3D_VOICE": "A 3D voice effect that makes the voice appear to be moving around the user. The default cycle period is 10 seconds. After setting this effect, you can call SetAudioEffectParameters to modify the movement period. If the 3D voice effect is enabled, users need to use stereo audio playback devices to hear the anticipated voice effect."
      },
      {
        "VOICE_CHANGER_EFFECT_UNCLE": "A middle-aged man's voice. Agora recommends using this preset to process a male-sounding voice; otherwise, you may not hear the anticipated voice effect."
      },
      {
        "VOICE_CHANGER_EFFECT_OLDMAN": "An older man's voice. Agora recommends using this preset to process a male-sounding voice; otherwise, you may not hear the anticipated voice effect."
      },
      {
        "VOICE_CHANGER_EFFECT_BOY": "A boy's voice. Agora recommends using this preset to process a male-sounding voice; otherwise, you may not hear the anticipated voice effect."
      },
      {
        "VOICE_CHANGER_EFFECT_SISTER": "A young woman's voice. Agora recommends using this preset to process a female-sounding voice; otherwise, you may not hear the anticipated voice effect."
      },
      {
        "VOICE_CHANGER_EFFECT_GIRL": "A girl's voice. Agora recommends using this preset to process a female-sounding voice; otherwise, you may not hear the anticipated voice effect."
      },
      {
        "VOICE_CHANGER_EFFECT_PIGKING": "The voice of Pig King, a character in Journey to the West who has a voice like a growling bear."
      },
      {
        "VOICE_CHANGER_EFFECT_HULK": "The Hulk's voice."
      },
      {
        "STYLE_TRANSFORMATION_RNB": "The voice effect typical of R&B music."
      },
      {
        "STYLE_TRANSFORMATION_POPULAR": "The voice effect typical of popular music."
      },
      {
        "PITCH_CORRECTION": "A pitch correction effect that corrects the user's pitch based on the pitch of the natural C major scale. After setting this voice effect, you can call SetAudioEffectParameters to adjust the basic mode of tuning and the pitch of the main tone."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audioencodedframeobserverposition",
    "name": "AUDIO_ENCODED_FRAME_OBSERVER_POSITION",
    "description": "Audio profile.",
    "parameters": [
      {
        "AUDIO_ENCODED_FRAME_OBSERVER_POSITION_RECORD": "1: Only records the audio of the local user."
      },
      {
        "AUDIO_ENCODED_FRAME_OBSERVER_POSITION_PLAYBACK": "2: Only records the audio of all remote users."
      },
      {
        "AUDIO_ENCODED_FRAME_OBSERVER_POSITION_MIXED": "3: Records the mixed audio of the local and all remote users."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audioencodingtype",
    "name": "AUDIO_ENCODING_TYPE",
    "description": "Audio encoding type.",
    "parameters": [
      {
        "AUDIO_ENCODING_TYPE_AAC_16000_LOW": "AAC encoding format, 16000 Hz sampling rate, bass quality. A file with an audio duration of 10 minutes is approximately 1.2 MB after encoding."
      },
      {
        "AUDIO_ENCODING_TYPE_AAC_16000_MEDIUM": "AAC encoding format, 16000 Hz sampling rate, medium sound quality. A file with an audio duration of 10 minutes is approximately 2 MB after encoding."
      },
      {
        "AUDIO_ENCODING_TYPE_AAC_32000_LOW": "AAC encoding format, 32000 Hz sampling rate, bass quality. A file with an audio duration of 10 minutes is approximately 1.2 MB after encoding."
      },
      {
        "AUDIO_ENCODING_TYPE_AAC_32000_MEDIUM": "AAC encoding format, 32000 Hz sampling rate, medium sound quality. A file with an audio duration of 10 minutes is approximately 2 MB after encoding."
      },
      {
        "AUDIO_ENCODING_TYPE_AAC_32000_HIGH": "AAC encoding format, 32000 Hz sampling rate, high sound quality. A file with an audio duration of 10 minutes is approximately 3.5 MB after encoding."
      },
      {
        "AUDIO_ENCODING_TYPE_AAC_48000_MEDIUM": "AAC encoding format, 48000 Hz sampling rate, medium sound quality. A file with an audio duration of 10 minutes is approximately 2 MB after encoding."
      },
      {
        "AUDIO_ENCODING_TYPE_AAC_48000_HIGH": "AAC encoding format, 48000 Hz sampling rate, high sound quality. A file with an audio duration of 10 minutes is approximately 3.5 MB after encoding."
      },
      {
        "AUDIO_ENCODING_TYPE_OPUS_16000_LOW": "OPUS encoding format, 16000 Hz sampling rate, bass quality. A file with an audio duration of 10 minutes is approximately 2 MB after encoding."
      },
      {
        "AUDIO_ENCODING_TYPE_OPUS_16000_MEDIUM": "OPUS encoding format, 16000 Hz sampling rate, medium sound quality. A file with an audio duration of 10 minutes is approximately 2 MB after encoding."
      },
      {
        "AUDIO_ENCODING_TYPE_OPUS_48000_MEDIUM": "OPUS encoding format, 48000 Hz sampling rate, medium sound quality. A file with an audio duration of 10 minutes is approximately 2 MB after encoding."
      },
      {
        "AUDIO_ENCODING_TYPE_OPUS_48000_HIGH": "OPUS encoding format, 48000 Hz sampling rate, high sound quality. A file with an audio duration of 10 minutes is approximately 3.5 MB after encoding."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audioequalizationbandfrequency",
    "name": "AUDIO_EQUALIZATION_BAND_FREQUENCY",
    "description": "The midrange frequency for audio equalization.",
    "parameters": [
      {
        "AUDIO_EQUALIZATION_BAND_31": "0: 31 Hz"
      },
      {
        "AUDIO_EQUALIZATION_BAND_62": "1: 62 Hz"
      },
      {
        "AUDIO_EQUALIZATION_BAND_125": "2: 125 Hz"
      },
      {
        "AUDIO_EQUALIZATION_BAND_250": "3: 250 Hz"
      },
      {
        "AUDIO_EQUALIZATION_BAND_500": "4: 500 Hz"
      },
      {
        "AUDIO_EQUALIZATION_BAND_1K": "5: 1 kHz"
      },
      {
        "AUDIO_EQUALIZATION_BAND_2K": "6: 2 kHz"
      },
      {
        "AUDIO_EQUALIZATION_BAND_4K": "7: 4 kHz"
      },
      {
        "AUDIO_EQUALIZATION_BAND_8K": "8: 8 kHz"
      },
      {
        "AUDIO_EQUALIZATION_BAND_16K": "9: 16 kHz"
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audiofilerecordingtype",
    "name": "AUDIO_FILE_RECORDING_TYPE",
    "description": "Recording content. Set in StartAudioRecording [3/3].",
    "parameters": [
      {
        "AUDIO_FILE_RECORDING_MIC": "1: Only records the audio of the local user."
      },
      {
        "AUDIO_FILE_RECORDING_PLAYBACK": "2: Only records the audio of all remote users."
      },
      {
        "AUDIO_FILE_RECORDING_MIXED": "3: Records the mixed audio of the local and all remote users."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audioframetype",
    "name": "AUDIO_FRAME_TYPE",
    "description": "Audio frame type.",
    "parameters": [
      {
        "FRAME_TYPE_PCM16": "0: PCM 16"
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audiomixingdualmonomode",
    "name": "AUDIO_MIXING_DUAL_MONO_MODE",
    "description": "The channel mode.",
    "parameters": [
      {
        "AUDIO_MIXING_DUAL_MONO_AUTO": "0: Original mode."
      },
      {
        "AUDIO_MIXING_DUAL_MONO_L": "1: Left channel mode. This mode replaces the audio of the right channel with the audio of the left channel, which means the user can only hear the audio of the left channel."
      },
      {
        "AUDIO_MIXING_DUAL_MONO_R": "2: Right channel mode. This mode replaces the audio of the left channel with the audio of the right channel, which means the user can only hear the audio of the right channel."
      },
      {
        "AUDIO_MIXING_DUAL_MONO_MIX": "3: Mixed channel mode. This mode mixes the audio of the left channel and the right channel, which means the user can hear the audio of the left channel and the right channel at the same time."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audiomixingreasontype",
    "name": "AUDIO_MIXING_REASON_TYPE",
    "description": "The reason why the playback state of the music file changes. Reported in the OnAudioMixingStateChanged callback.",
    "parameters": [
      {
        "AUDIO_MIXING_REASON_OK": "0: The SDK opens music file successfully."
      },
      {
        "AUDIO_MIXING_REASON_CAN_NOT_OPEN": "701: The SDK cannot open the music file. For example, the local music file does not exist, the SDK does not support the file format, or the the SDK cannot access the music file URL."
      },
      {
        "AUDIO_MIXING_REASON_TOO_FREQUENT_CALL": "702: The SDK opens the music file too frequently. If you need to call startAudioMixing multiple times, ensure that the call interval is more than 500 ms."
      },
      {
        "AUDIO_MIXING_REASON_INTERRUPTED_EOF": "703: The music file playback is interrupted."
      },
      {
        "AUDIO_MIXING_REASON_ONE_LOOP_COMPLETED": "721: The music file completes a loop playback."
      },
      {
        "AUDIO_MIXING_REASON_ALL_LOOPS_COMPLETED": "723: The music file completes all loop playback."
      },
      {
        "AUDIO_MIXING_REASON_STOPPED_BY_USER": "724: Successfully call StopAudioMixing to stop playing the music file."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audiomixingstatetype",
    "name": "AUDIO_MIXING_STATE_TYPE",
    "description": "The playback state of the music file.",
    "parameters": [
      {
        "AUDIO_MIXING_STATE_PLAYING": "710: The music file is playing."
      },
      {
        "AUDIO_MIXING_STATE_PAUSED": "711: The music file pauses playing."
      },
      {
        "AUDIO_MIXING_STATE_STOPPED": "713: The music file stops playing. The possible reasons include: AUDIO_MIXING_REASON_ALL_LOOPS_COMPLETED (723) AUDIO_MIXING_REASON_STOPPED_BY_USER (724)"
      },
      {
        "AUDIO_MIXING_STATE_FAILED": "714: An error occurs during the playback of the audio mixing file. The possible reasons include: AUDIO_MIXING_REASON_CAN_NOT_OPEN (701) AUDIO_MIXING_REASON_TOO_FREQUENT_CALL (702) AUDIO_MIXING_REASON_INTERRUPTED_EOF (703)"
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audioprocessingchannels",
    "name": "AUDIO_PROCESSING_CHANNELS",
    "description": "The number of channels for audio preprocessing.\n\nIn scenarios that require enhanced realism, such as concerts, local users might need to capture stereo audio and send stereo signals to remote users. For example, the singer, guitarist, and drummer are standing in different positions on the stage. The audio capture device captures their stereo audio and sends stereo signals to remote users. Remote users can hear the song, guitar, and drum from different directions as if they were at the auditorium. You can set the dual-channel processing to implement stereo audio in this class. Agora recommends the following settings:\n Preprocessing: call SetAdvancedAudioOptions and set audioProcessingChannels to AUDIO_PROCESSING_STEREO (2) in AdvancedAudioOptions.\n Post-processing: call SetAudioProfile [2/2] and set profile to AUDIO_PROFILE_MUSIC_STANDARD_STEREO (3) or AUDIO_PROFILE_MUSIC_HIGH_QUALITY_STEREO (5). The stereo setting only takes effect when the SDK uses the media volume.\n On iOS, the minimum version of system for stereo sound is 14.0. The minimum device requirements are as follows:\n iPhone XS\n iPhone XS Max\n iPhone XR\n iPhone 11\n iPhone 11 Pro\n iPhone 11 Pro Max\n iPhone SE (2020)\n iPad Pro 11\" and 12.9\" (3rd generation)\n iPad Pro 11\" and 12.9\" (4th generation)",
    "parameters": [
      {
        "AUDIO_PROCESSING_MONO": "1: (Default) Mono."
      },
      {
        "AUDIO_PROCESSING_STEREO": "2: Stereo."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audioprofiletype",
    "name": "AUDIO_PROFILE_TYPE",
    "description": "The audio profile.",
    "parameters": [
      {
        "AUDIO_PROFILE_DEFAULT": "0: The default audio profile.\n For the interactive streaming profile: A sample rate of 48 kHz, music encoding, mono, and a bitrate of up to 64 Kbps.\n For the communication profile:\n Windows: A sample rate of 16 kHz, audio encoding, mono, and a bitrate of up to 16 Kbps. Android/macOS/iOS: A sample rate of 32 kHz, audio encoding, mono, and a bitrate of up to 18 Kbps."
      },
      {
        "AUDIO_PROFILE_SPEECH_STANDARD": "1: A sample rate of 32 kHz, audio encoding, mono, and a bitrate of up to 18 Kbps."
      },
      {
        "AUDIO_PROFILE_MUSIC_STANDARD": "2: A sample rate of 48 kHz, music encoding, mono, and a bitrate of up to 64 Kbps."
      },
      {
        "AUDIO_PROFILE_MUSIC_STANDARD_STEREO": "3: A sample rate of 48 kHz, music encoding, stereo, and a bitrate of up to 80 Kbps. To implement stereo audio, you also need to call SetAdvancedAudioOptions and set audioProcessingChannels to AUDIO_PROCESSING_STEREO in AdvancedAudioOptions."
      },
      {
        "AUDIO_PROFILE_MUSIC_HIGH_QUALITY": "4: A sample rate of 48 kHz, music encoding, mono, and a bitrate of up to 96 Kbps."
      },
      {
        "AUDIO_PROFILE_MUSIC_HIGH_QUALITY_STEREO": "5: A sample rate of 48 kHz, music encoding, stereo, and a bitrate of up to 128 Kbps. To implement stereo audio, you also need to call SetAdvancedAudioOptions and set audioProcessingChannels to AUDIO_PROCESSING_STEREO in AdvancedAudioOptions."
      },
      {
        "AUDIO_PROFILE_IOT": "6: A sample rate of 16 kHz, audio encoding, mono, and Acoustic Echo Cancellation (AES) enabled."
      },
      {
        "AUDIO_PROFILE_NUM": "Enumerator boundary."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audiorangemodetype",
    "name": "RANGE_AUDIO_MODE_TYPE",
    "description": "The audio range mode.",
    "parameters": [
      {
        "RANGE_AUDIO_MODE_WORLD": "0: Everyone mode. In this mode, whether a user can hear other users in the room depends on their settings for audio reception range, audio range mode, and team ID.\n If both users A and B set the RANGE_AUDIO_MODE_WORLD mode, users A and B can hear each other when they are in the audio reception range of each other or belong to the same team.\n If users A and B set the RANGE_AUDIO_MODE_WORLD and RANGE_AUDIO_MODE_TEAM mode respectively, they can only hear each other when they belong to the same team."
      },
      {
        "RANGE_AUDIO_MODE_TEAM": "1: Team mode. In this mode, the user can only hear other users of the same team in the room."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_audiorecordingqualitytype",
    "name": "AUDIO_RECORDING_QUALITY_TYPE",
    "description": "Recording quality.",
    "parameters": [
      {
        "AUDIO_RECORDING_QUALITY_LOW": "0: Low quality. The sample rate is 32 kHz, and the file size is around 1.2 MB after 10 minutes of recording."
      },
      {
        "AUDIO_RECORDING_QUALITY_MEDIUM": "1: Medium quality. The sample rate is 32 kHz, and the file size is around 2 MB after 10 minutes of recording."
      },
      {
        "AUDIO_RECORDING_QUALITY_HIGH": "2: High quality. The sample rate is 32 kHz, and the file size is around 3.75 MB after 10 minutes of recording."
      },
      {
        "AUDIO_RECORDING_QUALITY_ULTRA_HIGH": "3: Ultra high quality. The sample rate is 32 kHz, and the file size is around 7.5 MB after 10 minutes of recording."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audioreverbpreset",
    "name": "AUDIO_REVERB_PRESET",
    "description": "Voice reverb presets.\n\nDeprecated: Deprecated as of v3.2.0.",
    "parameters": [
      {
        "AUDIO_REVERB_OFF": "Turn off voice reverb, that is, to use the original voice."
      },
      {
        "AUDIO_REVERB_FX_KTV": "The reverb style typical of a KTV venue (enhanced)."
      },
      {
        "AUDIO_REVERB_FX_VOCAL_CONCERT": "The reverb style typical of a concert hall (enhanced)."
      },
      {
        "AUDIO_REVERB_FX_UNCLE": "A middle-aged man's voice."
      },
      {
        "AUDIO_REVERB_FX_SISTER": "The reverb style typical of a young woman's voice."
      },
      {
        "AUDIO_REVERB_FX_STUDIO": "The reverb style typical of a recording studio (enhanced)."
      },
      {
        "AUDIO_REVERB_FX_POPULAR": "The reverb style typical of popular music (enhanced)."
      },
      {
        "AUDIO_REVERB_FX_RNB": "The reverb style typical of R&B music (enhanced)."
      },
      {
        "AUDIO_REVERB_FX_PHONOGRAPH": "The voice effect typical of a vintage phonograph."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audioreverbtype",
    "name": "AUDIO_REVERB_TYPE",
    "description": "Audio reverberation types.",
    "parameters": [
      {
        "AUDIO_REVERB_DRY_LEVEL": "0: The level of the dry signal (dB). The value is between -20 and 10."
      },
      {
        "AUDIO_REVERB_WET_LEVEL": "1: The level of the early reflection signal (wet signal) (dB). The value is between -20 and 10."
      },
      {
        "AUDIO_REVERB_ROOM_SIZE": "2: The room size of the reflection. The value is between 0 and 100."
      },
      {
        "AUDIO_REVERB_WET_DELAY": "3: The length of the initial delay of the wet signal (ms). The value is between 0 and 200."
      },
      {
        "AUDIO_REVERB_STRENGTH": "4: The reverberation strength. The value is between 0 and 100."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audioroute",
    "name": "AudioRoute",
    "description": "The type of the audio route.",
    "parameters": [
      {
        "ROUTE_DEFAULT": "-1: The default audio route."
      },
      {
        "ROUTE_HEADSET": "0: Audio output routing is a headset with microphone."
      },
      {
        "ROUTE_EARPIECE": "1: The audio route is an earpiece."
      },
      {
        "ROUTE_HEADSETNOMIC": "2: The audio route is a headset without a microphone."
      },
      {
        "ROUTE_SPEAKERPHONE": "3: The audio route is the speaker that comes with the device."
      },
      {
        "ROUTE_LOUDSPEAKER": "4: The audio route is an external speaker. (iOS and macOS only)"
      },
      {
        "ROUTE_BLUETOOTH_DEVICE_HFP": "5: The audio route is a Bluetooth device using the HFP protocol."
      },
      {
        "ROUTE_USB": "6: The audio route is a USB peripheral device. (For macOS only)"
      },
      {
        "ROUTE_HDMI": "7: The audio route is an HDMI peripheral device. (For macOS only)"
      },
      {
        "ROUTE_DISPLAYPORT": "8: The audio route is a DisplayPort peripheral device. (For macOS only)"
      },
      {
        "ROUTE_AIRPLAY": "9: The audio route is Apple AirPlay. (For macOS only)"
      },
      {
        "ROUTE_BLUETOOTH_DEVICE_A2DP": "10: The audio route is a Bluetooth device using the A2DP protocol."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audiosampleratetype",
    "name": "AUDIO_SAMPLE_RATE_TYPE",
    "description": "The audio sampling rate of the stream to be pushed to the CDN.",
    "parameters": [
      {
        "AUDIO_SAMPLE_RATE_32000": "32000: 32 kHz"
      },
      {
        "AUDIO_SAMPLE_RATE_44100": "44100: 44.1 kHz"
      },
      {
        "AUDIO_SAMPLE_RATE_48000": "48000: (Default) 48 kHz"
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audioscenariotype",
    "name": "AUDIO_SCENARIO_TYPE",
    "description": "The audio scenarios.",
    "parameters": [
      {
        "AUDIO_SCENARIO_DEFAULT": "0: (Default) Automatic scenario match, where the SDK chooses the appropriate audio quality according to the user role and audio route."
      },
      {
        "AUDIO_SCENARIO_GAME_STREAMING": "3: High-quality audio scenario, where users mainly play music. For example, instrument tutoring."
      },
      {
        "AUDIO_SCENARIO_CHATROOM": "5: Chatroom scenario, where users need to frequently switch the user role or mute and unmute the microphone. For example, education scenarios."
      },
      {
        "AUDIO_SCENARIO_CHORUS": "7: Real-time chorus scenario, where users have good network conditions and require ultra-low latency."
      },
      {
        "AUDIO_SCENARIO_MEETING": "8: Meeting scenario that mainly contains the human voice."
      },
      {
        "AUDIO_SCENARIO_NUM": "The number of enumerations."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audiosessionoperationrestriction",
    "name": "AUDIO_SESSION_OPERATION_RESTRICTION",
    "description": "The operation permissions of the SDK on the audio session.",
    "parameters": [
      {
        "AUDIO_SESSION_OPERATION_RESTRICTION_NONE": "No restriction, the SDK can change the audio session."
      },
      {
        "AUDIO_SESSION_OPERATION_RESTRICTION_SET_CATEGORY": "The SDK cannot change the audio session category."
      },
      {
        "AUDIO_SESSION_OPERATION_RESTRICTION_CONFIGURE_SESSION": "The SDK cannot change the audio session category, mode, or categoryOptions."
      },
      {
        "AUDIO_SESSION_OPERATION_RESTRICTION_DEACTIVATE_SESSION": "The SDK keeps the audio session active when the user leaves the channel, for example, to play an audio file in the background."
      },
      {
        "AUDIO_SESSION_OPERATION_RESTRICTION_ALL": "Completely restricts the operation permissions of the SDK on the audio session; the SDK cannot change the audio session."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audiosourcetype",
    "name": "AUDIO_SOURCE_TYPE",
    "description": "The audio source type.",
    "parameters": [
      {
        "AUDIO_SOURCE_MICROPHONE": "0: (Default) Microphone."
      },
      {
        "AUDIO_SOURCE_CUSTOM": "1: Custom audio stream."
      },
      {
        "AUDIO_SOURCE_MEDIA_PLAYER": "2: Media player."
      },
      {
        "AUDIO_SOURCE_LOOPBACK_RECORDING": "3: System audio stream captured during screen sharing."
      },
      {
        "AUDIO_SOURCE_REMOTE_USER": "5: Audio stream from a specified remote user."
      },
      {
        "AUDIO_SOURCE_REMOTE_CHANNEL": "6: Mixed audio streams from all users in the current channel."
      },
      {
        "AUDIO_SOURCE_UNKNOWN": "100: An unknown audio source."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_audiotracktype",
    "name": "AUDIO_TRACK_TYPE",
    "description": "The type of the audio track.",
    "parameters": [
      {
        "AUDIO_TRACK_MIXABLE": "0: Mixable audio tracks. This type of audio track supports mixing with other audio streams (such as audio streams captured by microphone) and playing locally or publishing to channels after mixing. The latency of mixable audio tracks is higher than that of direct audio tracks."
      },
      {
        "AUDIO_TRACK_DIRECT": "1: Direct audio tracks. This type of audio track will replace the audio streams captured by the microphone and does not support mixing with other audio streams. The latency of direct audio tracks is lower than that of mixable audio tracks. If AUDIO_TRACK_DIRECT is specified for this parameter, you must set publishMicrophoneTrack to false in ChannelMediaOptions when calling JoinChannel [2/2] to join the channel; otherwise, joining the channel fails and returns the error code -2."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_backgroundblurdegree",
    "name": "BACKGROUND_BLUR_DEGREE",
    "description": "The degree of blurring applied to the custom background image.",
    "parameters": [
      {
        "BLUR_DEGREE_LOW": "1: The degree of blurring applied to the custom background image is low. The user can almost see the background clearly."
      },
      {
        "BLUR_DEGREE_MEDIUM": "2: The degree of blurring applied to the custom background image is medium. It is difficult for the user to recognize details in the background."
      },
      {
        "BLUR_DEGREE_HIGH": "3: (Default) The degree of blurring applied to the custom background image is high. The user can barely see any distinguishing features in the background."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_backgroundsourcetype",
    "name": "BACKGROUND_SOURCE_TYPE",
    "description": "The custom background.",
    "parameters": [
      {
        "BACKGROUND_NONE": "0: Process the background as alpha data without replacement, only separating the portrait and the background. After setting this value, you can call StartLocalVideoTranscoder to implement the picture-in-picture effect."
      },
      {
        "BACKGROUND_COLOR": "1: (Default) The background image is a solid color."
      },
      {
        "BACKGROUND_IMG": "2: The background is an image in PNG or JPG format."
      },
      {
        "BACKGROUND_BLUR": "3: The background is a blurred version of the original background."
      },
      {
        "BACKGROUND_VIDEO": "4: The background is a local video in MP4, AVI, MKV, FLV, or other supported formats."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_bitrate",
    "name": "BITRATE",
    "description": "The encoding bitrate of the video.",
    "parameters": [
      {
        "STANDARD_BITRATE": "0: (Recommended) Standard bitrate mode."
      },
      {
        "COMPATIBLE_BITRATE": "-1: Adaptive bitrate mode."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_cameradirection",
    "name": "CAMERA_DIRECTION",
    "description": "The camera direction.",
    "parameters": [
      {
        "CAMERA_REAR": "0: The rear camera."
      },
      {
        "CAMERA_FRONT": "1: (Default) The front camera."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_camerafocallengthtype",
    "name": "CAMERA_FOCAL_LENGTH_TYPE",
    "description": "The camera focal length types.\n\nThis enumeration class applies to Android and iOS only.",
    "parameters": [
      {
        "CAMERA_FOCAL_LENGTH_DEFAULT": "0: (Default) Standard lens."
      },
      {
        "CAMERA_FOCAL_LENGTH_WIDE_ANGLE": "1: Wide-angle lens."
      },
      {
        "CAMERA_FOCAL_LENGTH_ULTRA_WIDE": "2: Ultra-wide-angle lens."
      },
      {
        "CAMERA_FOCAL_LENGTH_TELEPHOTO": "3: (For iOS only) Telephoto lens."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_camerastabilizationmode",
    "name": "CAMERA_STABILIZATION_MODE",
    "description": "Camera stabilization modes.\n\nThe camera stabilization effect increases in the order of 1 < 2 < 3, and the latency will also increase accordingly.",
    "parameters": [
      {
        "CAMERA_STABILIZATION_MODE_OFF": "-1: (Default) Camera stabilization mode off."
      },
      {
        "CAMERA_STABILIZATION_MODE_AUTO": "0: Automatic camera stabilization. The system automatically selects a stabilization mode based on the status of the camera. However, the latency is relatively high in this mode, so it is recommended not to use this enumeration."
      },
      {
        "CAMERA_STABILIZATION_MODE_LEVEL_1": "1: (Recommended) Level 1 camera stabilization."
      },
      {
        "CAMERA_STABILIZATION_MODE_LEVEL_2": "2: Level 2 camera stabilization."
      },
      {
        "CAMERA_STABILIZATION_MODE_LEVEL_3": "3: Level 3 camera stabilization."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_capturebrightnessleveltype",
    "name": "CAPTURE_BRIGHTNESS_LEVEL_TYPE",
    "description": "The brightness level of the video image captured by the local camera.",
    "parameters": [
      {
        "CAPTURE_BRIGHTNESS_LEVEL_INVALID": "-1: The SDK does not detect the brightness level of the video image. Wait a few seconds to get the brightness level from captureBrightnessLevel in the next callback."
      },
      {
        "CAPTURE_BRIGHTNESS_LEVEL_NORMAL": "0: The brightness level of the video image is normal."
      },
      {
        "CAPTURE_BRIGHTNESS_LEVEL_BRIGHT": "1: The brightness level of the video image is too bright."
      },
      {
        "CAPTURE_BRIGHTNESS_LEVEL_DARK": "2: The brightness level of the video image is too dark."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_channelmediarelayerror",
    "name": "CHANNEL_MEDIA_RELAY_ERROR",
    "description": "The error code of the channel media relay.",
    "parameters": [
      {
        "RELAY_OK": "0: No error."
      },
      {
        "RELAY_ERROR_SERVER_ERROR_RESPONSE": "1: An error occurs in the server response."
      },
      {
        "RELAY_ERROR_SERVER_NO_RESPONSE": "2: No server response. This error may be caused by poor network connections. If this error occurs when initiating a channel media relay, you can try again later; if this error occurs during channel media relay, you can call LeaveChannel [2/2] to leave the channel. This error can also occur if the channel media relay service is not enabled in the project. You can contact to enable the service."
      },
      {
        "RELAY_ERROR_NO_RESOURCE_AVAILABLE": "3: The SDK fails to access the service, probably due to limited resources of the server."
      },
      {
        "RELAY_ERROR_FAILED_JOIN_SRC": "4: Fails to send the relay request."
      },
      {
        "RELAY_ERROR_FAILED_JOIN_DEST": "5: Fails to accept the relay request."
      },
      {
        "RELAY_ERROR_FAILED_PACKET_RECEIVED_FROM_SRC": "6: The server fails to receive the media stream."
      },
      {
        "RELAY_ERROR_FAILED_PACKET_SENT_TO_DEST": "7: The server fails to send the media stream."
      },
      {
        "RELAY_ERROR_SERVER_CONNECTION_LOST": "8: The SDK disconnects from the server due to poor network connections. You can call LeaveChannel [2/2] to leave the channel."
      },
      {
        "RELAY_ERROR_INTERNAL_ERROR": "9: An internal error occurs in the server."
      },
      {
        "RELAY_ERROR_SRC_TOKEN_EXPIRED": "10: The token of the source channel has expired."
      },
      {
        "RELAY_ERROR_DEST_TOKEN_EXPIRED": "11: The token of the destination channel has expired."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_channelmediarelaystate",
    "name": "CHANNEL_MEDIA_RELAY_STATE",
    "description": "The state code of the channel media relay.",
    "parameters": [
      {
        "RELAY_STATE_IDLE": "0: The initial state. After you successfully stop the channel media relay by calling StopChannelMediaRelay, the OnChannelMediaRelayStateChanged callback returns this state."
      },
      {
        "RELAY_STATE_CONNECTING": "1: The SDK tries to relay the media stream to the destination channel."
      },
      {
        "RELAY_STATE_RUNNING": "2: The SDK successfully relays the media stream to the destination channel."
      },
      {
        "RELAY_STATE_FAILURE": "3: An error occurs. See code in OnChannelMediaRelayStateChanged for the error code."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_channelprofiletype",
    "name": "CHANNEL_PROFILE_TYPE",
    "description": "The channel profile.",
    "parameters": [
      {
        "CHANNEL_PROFILE_COMMUNICATION": "0: Communication. Use this profile when there are only two users in the channel."
      },
      {
        "CHANNEL_PROFILE_LIVE_BROADCASTING": "1: Live streaming. Live streaming. Use this profile when there are more than two users in the channel."
      },
      {
        "CHANNEL_PROFILE_GAME": "2: Gaming. This profile is deprecated."
      },
      {
        "CHANNEL_PROFILE_CLOUD_GAMING": "Cloud gaming. The scenario is optimized for latency. Use this profile if the use case requires frequent interactions between users."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_clientrolechangefailedreason",
    "name": "CLIENT_ROLE_CHANGE_FAILED_REASON",
    "description": "The reason for a user role switch failure.",
    "parameters": [
      {
        "CLIENT_ROLE_CHANGE_FAILED_TOO_MANY_BROADCASTERS": "1: The number of hosts in the channel exceeds the limit. This enumerator is reported only when the support for 128 users is enabled. The maximum number of hosts is based on the actual number of hosts configured when you enable the 128-user feature."
      },
      {
        "CLIENT_ROLE_CHANGE_FAILED_NOT_AUTHORIZED": "2: The request is rejected by the Agora server. Agora recommends you prompt the user to try to switch their user role again."
      },
      {
        "CLIENT_ROLE_CHANGE_FAILED_REQUEST_TIME_OUT": "3: The request is timed out. Agora recommends you prompt the user to check the network connection and try to switch their user role again. Deprecated: This enumerator is deprecated since v4.4.0 and is not recommended for use."
      },
      {
        "CLIENT_ROLE_CHANGE_FAILED_CONNECTION_FAILED": "4: The SDK is disconnected from the Agora edge server. You can troubleshoot the failure through the reason reported by OnConnectionStateChanged. Deprecated: This enumerator is deprecated since v4.4.0 and is not recommended for use."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_clientroletype",
    "name": "CLIENT_ROLE_TYPE",
    "description": "The user role in the interactive live streaming.",
    "parameters": [
      {
        "CLIENT_ROLE_BROADCASTER": "1: Host. A host can both send and receive streams."
      },
      {
        "CLIENT_ROLE_AUDIENCE": "2: (Default) Audience. An audience member can only receive streams."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_cloudproxytype",
    "name": "CLOUD_PROXY_TYPE",
    "description": "The cloud proxy type.",
    "parameters": [
      {
        "NONE_PROXY": "0: The automatic mode. The SDK has this mode enabled by default. In this mode, the SDK attempts a direct connection to SD-RTN™ and automatically switches to TCP/TLS 443 if the attempt fails."
      },
      {
        "UDP_PROXY": "1: The cloud proxy for the UDP protocol, that is, the Force UDP cloud proxy mode. In this mode, the SDK always transmits data over UDP."
      },
      {
        "TCP_PROXY": "2: The cloud proxy for the TCP (encryption) protocol, that is, the Force TCP cloud proxy mode. In this mode, the SDK always transmits data over TCP/TLS 443."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_codeccapmask",
    "name": "CODEC_CAP_MASK",
    "description": "The bit mask of the codec type.",
    "parameters": [
      {
        "CODEC_CAP_MASK_NONE": "(0): The device does not support encoding or decoding."
      },
      {
        "CODEC_CAP_MASK_HW_DEC": "(1 << 0): The device supports hardware decoding."
      },
      {
        "CODEC_CAP_MASK_HW_ENC": "(1 << 1): The device supports hardware encoding."
      },
      {
        "CODEC_CAP_MASK_SW_DEC": "(1 << 2): The device supports software decoding."
      },
      {
        "CODEC_CAP_MASK_SW_ENC": "(1 << 3): The device supports software ecoding."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_compressionpreference",
    "name": "COMPRESSION_PREFERENCE",
    "description": "Compression preference for video encoding.",
    "parameters": [
      {
        "PREFER_COMPRESSION_AUTO": "-1: (Default) Automatic mode. The SDK will automatically select PREFER_LOW_LATENCY or PREFER_QUALITY based on the video scenario you set to achieve the best user experience."
      },
      {
        "PREFER_LOW_LATENCY": "0: Low latency preference. The SDK compresses video frames to reduce latency. This preference is suitable for scenarios where smoothness is prioritized and reduced video quality is acceptable."
      },
      {
        "PREFER_QUALITY": "1: High quality preference. The SDK compresses video frames while maintaining video quality. This preference is suitable for scenarios where video quality is prioritized."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_connectionchangedreasontype",
    "name": "CONNECTION_CHANGED_REASON_TYPE",
    "description": "Reasons causing the change of the connection state.",
    "parameters": [
      {
        "CONNECTION_CHANGED_CONNECTING": "0: The SDK is connecting to the Agora edge server."
      },
      {
        "CONNECTION_CHANGED_JOIN_SUCCESS": "1: The SDK has joined the channel successfully."
      },
      {
        "CONNECTION_CHANGED_INTERRUPTED": "2: The connection between the SDK and the Agora edge server is interrupted."
      },
      {
        "CONNECTION_CHANGED_BANNED_BY_SERVER": "3: The connection between the SDK and the Agora edge server is banned by the Agora edge server. For example, when a user is kicked out of the channel, this status will be returned."
      },
      {
        "CONNECTION_CHANGED_JOIN_FAILED": "4: The SDK fails to join the channel. When the SDK fails to join the channel for more than 20 minutes, this code will be returned and the SDK stops reconnecting to the channel. You need to prompt the user to try to switch to another network and rejoin the channel."
      },
      {
        "CONNECTION_CHANGED_LEAVE_CHANNEL": "5: The SDK has left the channel."
      },
      {
        "CONNECTION_CHANGED_INVALID_APP_ID": "6: The App ID is invalid. You need to rejoin the channel with a valid APP ID and make sure the App ID you are using is consistent with the one generated in the Agora Console."
      },
      {
        "CONNECTION_CHANGED_INVALID_CHANNEL_NAME": "7: Invalid channel name. Rejoin the channel with a valid channel name. A valid channel name is a string of up to 64 bytes in length. Supported characters (89 characters in total):\n All lowercase English letters: a to z.\n All uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n \"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"<\", \"=\", \".\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\""
      },
      {
        "CONNECTION_CHANGED_INVALID_TOKEN": "8: Invalid token. Possible reasons are as follows:\n The App Certificate for the project is enabled in Agora Console, but you do not pass in a token when joining a channel.\n The uid specified when calling JoinChannel [2/2] to join the channel is inconsistent with the uid passed in when generating the token.\n The generated token and the token used to join the channel are not consistent. Ensure the following:\n When your project enables App Certificate, you need to pass in a token to join a channel.\n The user ID specified when generating the token is consistent with the user ID used when joining the channel.\n The generated token is the same as the token passed in to join the channel."
      },
      {
        "CONNECTION_CHANGED_TOKEN_EXPIRED": "(9): The token currently being used has expired. You need to generate a new token on your server and rejoin the channel with the new token."
      },
      {
        "CONNECTION_CHANGED_REJECTED_BY_SERVER": "10: The connection is rejected by server. Possible reasons are as follows:\n The user is already in the channel and still calls a method, for example, JoinChannel [2/2], to join the channel. Stop calling this method to clear this error.\n The user tries to join a channel while a test call is in progress. The user needs to join the channel after the call test ends."
      },
      {
        "CONNECTION_CHANGED_SETTING_PROXY_SERVER": "11: The connection state changed to reconnecting because the SDK has set a proxy server."
      },
      {
        "CONNECTION_CHANGED_RENEW_TOKEN": "12: The connection state changed because the token is renewed."
      },
      {
        "CONNECTION_CHANGED_CLIENT_IP_ADDRESS_CHANGED": "(13): Client IP address changed. If you receive this code multiple times, You need to prompt the user to switch networks and try joining the channel again."
      },
      {
        "CONNECTION_CHANGED_KEEP_ALIVE_TIMEOUT": "14: Timeout for the keep-alive of the connection between the SDK and the Agora edge server. The SDK tries to reconnect to the server automatically."
      },
      {
        "CONNECTION_CHANGED_REJOIN_SUCCESS": "15: The user has rejoined the channel successfully."
      },
      {
        "CONNECTION_CHANGED_LOST": "16: The connection between the SDK and the server is lost."
      },
      {
        "CONNECTION_CHANGED_ECHO_TEST": "17: The connection state changes due to the echo test."
      },
      {
        "CONNECTION_CHANGED_CLIENT_IP_ADDRESS_CHANGED_BY_USER": "18: The local IP address was changed by the user."
      },
      {
        "CONNECTION_CHANGED_SAME_UID_LOGIN": "19: The user joined the same channel from different devices with the same UID."
      },
      {
        "CONNECTION_CHANGED_TOO_MANY_BROADCASTERS": "20: The number of hosts in the channel has reached the upper limit."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_connectionstatetype",
    "name": "CONNECTION_STATE_TYPE",
    "description": "Connection states.",
    "parameters": [
      {
        "CONNECTION_STATE_DISCONNECTED": "1: The SDK is disconnected from the Agora edge server. The state indicates the SDK is in one of the following phases:\n Theinitial state before calling the JoinChannel [2/2] method.\n The app calls the LeaveChannel [1/2] method."
      },
      {
        "CONNECTION_STATE_CONNECTING": "2: The SDK is connecting to the Agora edge server. This state indicates that the SDK is establishing a connection with the specified channel after the app calls JoinChannel [2/2].\n If the SDK successfully joins the channel, it triggers the OnConnectionStateChanged callback and the connection state switches to CONNECTION_STATE_CONNECTED.\n After the connection is established, the SDK also initializes the media and triggers OnJoinChannelSuccess when everything is ready."
      },
      {
        "CONNECTION_STATE_CONNECTED": "3: The SDK is connected to the Agora edge server. This state also indicates that the user has joined a channel and can now publish or subscribe to a media stream in the channel. If the connection to the channel is lost because, for example, if the network is down or switched, the SDK automatically tries to reconnect and triggers OnConnectionStateChanged callback, notifying that the current network state becomes CONNECTION_STATE_RECONNECTING."
      },
      {
        "CONNECTION_STATE_RECONNECTING": "4: The SDK keeps reconnecting to the Agora edge server. The SDK keeps rejoining the channel after being disconnected from a joined channel because of network issues.\n If the SDK cannot rejoin the channel within 10 seconds, it triggers OnConnectionLost, stays in the CONNECTION_STATE_RECONNECTING state, and keeps rejoining the channel.\n If the SDK fails to rejoin the channel 20 minutes after being disconnected from the Agora edge server, the SDK triggers the OnConnectionStateChanged callback, switches to the CONNECTION_STATE_FAILED state, and stops rejoining the channel."
      },
      {
        "CONNECTION_STATE_FAILED": "5: The SDK fails to connect to the Agora edge server or join the channel. This state indicates that the SDK stops trying to rejoin the channel. You must call LeaveChannel [1/2] to leave the channel.\n You can call JoinChannel [2/2] to rejoin the channel.\n If the SDK is banned from joining the channel by the Agora edge server through the RESTful API, the SDK triggers the OnConnectionStateChanged callback."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_contentinspectresult",
    "name": "CONTENT_INSPECT_RESULT",
    "description": "Content moderation results.",
    "parameters": [
      {
        "CONTENT_INSPECT_NEUTRAL": "1: The image does not contain inappropriate elements."
      },
      {
        "CONTENT_INSPECT_SEXY": "2: The image is sexually suggestive."
      },
      {
        "CONTENT_INSPECT_PORN": "3: The image is pornographic."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_contentinspecttype",
    "name": "CONTENT_INSPECT_TYPE",
    "description": "The type of video content moderation module.",
    "parameters": [
      {
        "CONTENT_INSPECT_INVALID": "0: (Default) This module has no actual function. Do not set type to this value."
      },
      {
        "CONTENT_INSPECT_SUPERVISION": "2: Video screenshot and upload via Agora self-developed extension. SDK takes screenshots of the video stream in the channel and uploads them."
      },
      {
        "CONTENT_INSPECT_IMAGE_MODERATION": "3: Video screenshot and upload via extensions from Agora Extensions Marketplace. SDK uses video moderation extensions from Agora Extensions Marketplace to take screenshots of the video stream in the channel and uploads them."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_degradationpreference",
    "name": "DEGRADATION_PREFERENCE",
    "description": "Video degradation preferences when the bandwidth is a constraint.",
    "parameters": [
      {
        "MAINTAIN_AUTO": "0: (Default) Automatic mode. The SDK will automatically select MAINTAIN_FRAMERATE, MAINTAIN_BALANCED or MAINTAIN_RESOLUTION based on the video scenario you set, in order to achieve the best overall quality of experience (QoE)."
      },
      {
        "MAINTAIN_QUALITY": "0: Prefers to reduce the video frame rate while maintaining video resolution during video encoding under limited bandwidth. This degradation preference is suitable for scenarios where video quality is prioritized. Deprecated: This enumerator is deprecated. Use other enumerations instead."
      },
      {
        "MAINTAIN_FRAMERATE": "1: Reduces the video resolution while maintaining the video frame rate during video encoding under limited bandwidth. This degradation preference is suitable for scenarios where smoothness is prioritized and video quality is allowed to be reduced."
      },
      {
        "MAINTAIN_BALANCED": "2: Reduces the video frame rate and video resolution simultaneously during video encoding under limited bandwidth. The MAINTAIN_BALANCED has a lower reduction than MAINTAIN_QUALITY and MAINTAIN_FRAMERATE, and this preference is suitable for scenarios where both smoothness and video quality are a priority. The resolution of the video sent may change, so remote users need to handle this issue. See OnVideoSizeChanged."
      },
      {
        "MAINTAIN_RESOLUTION": "3: Reduces the video frame rate while maintaining the video resolution during video encoding under limited bandwidth. This degradation preference is suitable for scenarios where video quality is prioritized."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_directcdnstreamingreason",
    "name": "DIRECT_CDN_STREAMING_REASON",
    "description": "Reasons for the changes in CDN streaming status.",
    "parameters": [
      {
        "DIRECT_CDN_STREAMING_REASON_OK": "0: No error."
      },
      {
        "DIRECT_CDN_STREAMING_REASON_FAILED": "1: A general error; no specific reason. You can try to push the media stream again."
      },
      {
        "DIRECT_CDN_STREAMING_REASON_AUDIO_PUBLICATION": "2: An error occurs when pushing audio streams. For example, the local audio capture device is not working properly, is occupied by another process, or does not get the permission required."
      },
      {
        "DIRECT_CDN_STREAMING_REASON_VIDEO_PUBLICATION": "3: An error occurs when pushing video streams. For example, the local video capture device is not working properly, is occupied by another process, or does not get the permission required."
      },
      {
        "DIRECT_CDN_STREAMING_REASON_NET_CONNECT": "4: Fails to connect to the CDN."
      },
      {
        "DIRECT_CDN_STREAMING_REASON_BAD_NAME": "5: The URL is already being used. Use a new URL for streaming."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_directcdnstreamingstate",
    "name": "DIRECT_CDN_STREAMING_STATE",
    "description": "The current CDN streaming state.",
    "parameters": [
      {
        "DIRECT_CDN_STREAMING_STATE_IDLE": "0: The initial state before the CDN streaming starts."
      },
      {
        "DIRECT_CDN_STREAMING_STATE_RUNNING": "1: Streams are being pushed to the CDN. The SDK returns this value when you call the StartDirectCdnStreaming method to push streams to the CDN."
      },
      {
        "DIRECT_CDN_STREAMING_STATE_STOPPED": "2: Stops pushing streams to the CDN. The SDK returns this value when you call the StopDirectCdnStreaming method to stop pushing streams to the CDN."
      },
      {
        "DIRECT_CDN_STREAMING_STATE_FAILED": "3: Fails to push streams to the CDN. You can troubleshoot the issue with the information reported by the OnDirectCdnStreamingStateChanged callback, and then push streams to the CDN again."
      },
      {
        "DIRECT_CDN_STREAMING_STATE_RECOVERING": "4: Tries to reconnect the Agora server to the CDN. The SDK attempts to reconnect a maximum of 10 times; if the connection is not restored, the streaming state becomes DIRECT_CDN_STREAMING_STATE_FAILED."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_earmonitoringfiltertype",
    "name": "EAR_MONITORING_FILTER_TYPE",
    "description": "The audio filter types of in-ear monitoring.",
    "parameters": [
      {
        "EAR_MONITORING_FILTER_NONE": "1<<0: No audio filter added to in-ear monitoring."
      },
      {
        "EAR_MONITORING_FILTER_BUILT_IN_AUDIO_FILTERS": "1<<1: Add vocal effects audio filter to in-ear monitoring. If you implement functions such as voice beautifier and audio effect, users can hear the voice after adding these effects."
      },
      {
        "EAR_MONITORING_FILTER_NOISE_SUPPRESSION": "1<<2: Add noise suppression audio filter to in-ear monitoring."
      },
      {
        "EAR_MONITORING_FILTER_REUSE_POST_PROCESSING_FILTER": "1<<15: Reuse the audio filter that has been processed on the sending end for in-ear monitoring. This enumerator reduces CPU usage while increasing in-ear monitoring latency, which is suitable for latency-tolerant scenarios requiring low CPU consumption."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_encodingpreference",
    "name": "ENCODING_PREFERENCE",
    "description": "Video encoder preference.",
    "parameters": [
      {
        "PREFER_AUTO": "-1: Adaptive preference. The SDK automatically selects the optimal encoding type for encoding based on factors such as platform and device type."
      },
      {
        "PREFER_SOFTWARE": "0: Software coding preference. The SDK prefers software encoders for video encoding."
      },
      {
        "PREFER_HARDWARE": "1: Hardware encoding preference. The SDK prefers a hardware encoder for video encoding. When the device does not support hardware encoding, the SDK automatically uses software encoding and reports the currently used video encoder type through hwEncoderAccelerating in the OnLocalVideoStats callback."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_encryptionerrortype",
    "name": "ENCRYPTION_ERROR_TYPE",
    "description": "Encryption error type.",
    "parameters": [
      {
        "ENCRYPTION_ERROR_INTERNAL_FAILURE": "0: Internal reason."
      },
      {
        "ENCRYPTION_ERROR_DECRYPTION_FAILURE": "1: Media stream decryption error. Ensure that the receiver and the sender use the same encryption mode and key."
      },
      {
        "ENCRYPTION_ERROR_ENCRYPTION_FAILURE": "2: Media stream encryption error."
      },
      {
        "ENCRYPTION_ERROR_DATASTREAM_DECRYPTION_FAILURE": "3: Data stream decryption error. Ensure that the receiver and the sender use the same encryption mode and key."
      },
      {
        "ENCRYPTION_ERROR_DATASTREAM_ENCRYPTION_FAILURE": "4: Data stream encryption error."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_encryptionmode",
    "name": "ENCRYPTION_MODE",
    "description": "The built-in encryption mode.\n\nAgora recommends using AES_128_GCM2 or AES_256_GCM2 encrypted mode. These two modes support the use of salt for higher security.",
    "parameters": [
      {
        "AES_128_XTS": "1: 128-bit AES encryption, XTS mode."
      },
      {
        "AES_128_ECB": "2: 128-bit AES encryption, ECB mode."
      },
      {
        "AES_256_XTS": "3: 256-bit AES encryption, XTS mode."
      },
      {
        "SM4_128_ECB": "4: 128-bit SM4 encryption, ECB mode."
      },
      {
        "AES_128_GCM": "5: 128-bit AES encryption, GCM mode."
      },
      {
        "AES_256_GCM": "6: 256-bit AES encryption, GCM mode."
      },
      {
        "AES_128_GCM2": "7: (Default) 128-bit AES encryption, GCM mode. This encryption mode requires the setting of salt (encryptionKdfSalt)."
      },
      {
        "AES_256_GCM2": "8: 256-bit AES encryption, GCM mode. This encryption mode requires the setting of salt (encryptionKdfSalt)."
      },
      {
        "MODE_END": "Enumerator boundary."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_errorcode",
    "name": "ErrorCode",
    "description": "Error codes. See https://docs.agora.io/en/Interactive%20Broadcast/error_rtc.",
    "parameters": [],
    "returns": ""
  },
  {
    "id": "enum_errorcodetype",
    "name": "ERROR_CODE_TYPE",
    "description": "Error codes.\n\nAn error code indicates that the SDK encountered an unrecoverable error that requires application intervention. For example, an error is returned when the camera fails to open, and the app needs to inform the user that the camera cannot be used.",
    "parameters": [
      {
        "ERR_OK": "0: No error."
      },
      {
        "ERR_FAILED": "1: General error with no classified reason. Try calling the method again."
      },
      {
        "ERR_INVALID_ARGUMENT": "2: An invalid parameter is used. For example, the specified channel name includes illegal characters. Reset the parameter."
      },
      {
        "ERR_NOT_READY": "3: The SDK is not ready. Possible reasons include the following:\n The initialization of IRtcEngine fails. Reinitialize the IRtcEngine.\n No user has joined the channel when the method is called. Check the code logic.\n The user has not left the channel when the Rate or Complain method is called. Check the code logic.\n The audio module is disabled.\n The program is not complete."
      },
      {
        "ERR_NOT_SUPPORTED": "4: The IRtcEngine does not support the request. Possible reasons include the following:\n The built-in encryption mode is incorrect, or the SDK fails to load the external encryption library. Check the encryption mode setting, or reload the external encryption library."
      },
      {
        "ERR_REFUSED": "5: The request is rejected. Possible reasons include the following:\n The IRtcEngine initialization fails. Reinitialize the IRtcEngine.\n The channel name is set as the empty string \"\" when joining the channel. Reset the channel name.\n When the JoinChannelEx method is called to join multiple channels, the specified channel name is already in use. Reset the channel name."
      },
      {
        "ERR_BUFFER_TOO_SMALL": "6: The buffer size is insufficient to store the returned data."
      },
      {
        "ERR_NOT_INITIALIZED": "7: A method is called before the initialization of IRtcEngine. Ensure that the IRtcEngine object is initialized before using this method."
      },
      {
        "ERR_INVALID_STATE": "8: Invalid state."
      },
      {
        "ERR_NO_PERMISSION": "9: Permission to access is not granted. Check whether your app has access to the audio and video device."
      },
      {
        "ERR_TIMEDOUT": "10: A timeout occurs. Some API calls require the SDK to return the execution result. This error occurs if the SDK takes too long (more than 10 seconds) to return the result."
      },
      {
        "ERR_JOIN_CHANNEL_REJECTED": "17: The request to join the channel is rejected. Possible reasons include the following:\n The user is already in the channel. Agora recommends that you use the OnConnectionStateChanged callback to see whether the user is in the channel. Do not call this method to join the channel unless you receive the CONNECTION_STATE_DISCONNECTED (1) state.\n After calling StartEchoTest for the call test, the user tries to join the channel without calling StopEchoTest to end the current test. To join a channel, the call test must be ended by calling StopEchoTest."
      },
      {
        "ERR_LEAVE_CHANNEL_REJECTED": "18: Fails to leave the channel. Possible reasons include the following:\n The user has left the channel before calling the LeaveChannel [2/2] method. Stop calling this method to clear this error.\n The user calls the LeaveChannel [2/2] method to leave the channel before joining the channel. In this case, no extra operation is needed."
      },
      {
        "ERR_ALREADY_IN_USE": "19: Resources are already in use."
      },
      {
        "ERR_ABORTED": "20: The request is abandoned by the SDK, possibly because the request has been sent too frequently."
      },
      {
        "ERR_INIT_NET_ENGINE": "21: The IRtcEngine fails to initialize and has crashed because of specific Windows firewall settings."
      },
      {
        "ERR_RESOURCE_LIMITED": "22: The SDK fails to allocate resources because your app uses too many system resources or system resources are insufficient."
      },
      {
        "ERR_INVALID_APP_ID": "101: The specified App ID is invalid. Rejoin the channel with a valid App ID."
      },
      {
        "ERR_INVALID_CHANNEL_NAME": "102: The specified channel name is invalid. A possible reason is that the parameter's data type is incorrect. Rejoin the channel with a valid channel name."
      },
      {
        "ERR_NO_SERVER_RESOURCES": "103: Fails to get server resources in the specified region. Try another region when initializing IRtcEngine."
      },
      {
        "ERR_TOKEN_EXPIRED": "109: The current token has expired. Apply for a new token on the server and call RenewToken. Deprecated: This enumerator is deprecated. Use CONNECTION_CHANGED_TOKEN_EXPIRED (9) in the OnConnectionStateChanged callback instead."
      },
      {
        "ERR_INVALID_TOKEN": "110: Invalid token. Typical reasons include the following:\n App Certificate is enabled in Agora Console, but the code still uses App ID for authentication. Once App Certificate is enabled for a project, you must use token-based authentication.\n The uid used to generate the token is not the same as the uid used to join the channel. Deprecated: This enumerator is deprecated. Use CONNECTION_CHANGED_INVALID_TOKEN (8) in the OnConnectionStateChanged callback instead."
      },
      {
        "ERR_CONNECTION_INTERRUPTED": "111: The network connection is interrupted. The SDK triggers this callback when it loses connection with the server for more than four seconds after the connection is established."
      },
      {
        "ERR_CONNECTION_LOST": "112: The network connection is lost. Occurs when the SDK cannot reconnect to Agora's edge server 10 seconds after its connection to the server is interrupted."
      },
      {
        "ERR_NOT_IN_CHANNEL": "113: The user is not in the channel when calling the SendStreamMessage method."
      },
      {
        "ERR_SIZE_TOO_LARGE": "114: The data size exceeds 1 KB when calling the SendStreamMessage method."
      },
      {
        "ERR_BITRATE_LIMIT": "115: The data bitrate exceeds 6 KB/s when calling the SendStreamMessage method."
      },
      {
        "ERR_TOO_MANY_DATA_STREAMS": "116: More than five data streams are created when calling the CreateDataStream [2/2] method."
      },
      {
        "ERR_STREAM_MESSAGE_TIMEOUT": "117: The data stream transmission times out."
      },
      {
        "ERR_SET_CLIENT_ROLE_NOT_AUTHORIZED": "119: Switching roles fails, try rejoining the channel."
      },
      {
        "ERR_DECRYPTION_FAILED": "120: Media streams decryption fails. The user might use an incorrect password to join the channel. Check the entered password, or tell the user to try rejoining the channel."
      },
      {
        "ERR_INVALID_USER_ID": "121: The user ID is invalid."
      },
      {
        "ERR_DATASTREAM_DECRYPTION_FAILED": "122: Data streams decryption fails. The user might use an incorrect password to join the channel. Check the entered password, or tell the user to try rejoining the channel."
      },
      {
        "ERR_CLIENT_IS_BANNED_BY_SERVER": "123: The user is banned from the server."
      },
      {
        "ERR_ENCRYPTED_STREAM_NOT_ALLOWED_PUBLISH": "130: The SDK does not support pushing encrypted streams to CDN."
      },
      {
        "ERR_INVALID_USER_ACCOUNT": "134: The user account is invalid, possibly because it contains invalid parameters."
      },
      {
        "ERR_LOAD_MEDIA_ENGINE": "1001: The SDK fails to load the media engine."
      },
      {
        "ERR_ADM_GENERAL_ERROR": "1005: A general error occurs (no specified reason). Check whether the audio device is already in use by another app, or try rejoining the channel."
      },
      {
        "ERR_ADM_INIT_PLAYOUT": "1008: An error occurs when initializing the playback device. Check whether the playback device is already in use by another app, or try rejoining the channel."
      },
      {
        "ERR_ADM_START_PLAYOUT": "1009: An error occurs when starting the playback device. Check the playback device."
      },
      {
        "ERR_ADM_STOP_PLAYOUT": "1010: An error occurs when stopping the playback device."
      },
      {
        "ERR_ADM_INIT_RECORDING": "1011: An error occurs when initializing the recording device. Check the recording device, or try rejoining the channel."
      },
      {
        "ERR_ADM_START_RECORDING": "1012: An error occurs when starting the recording device. Check the recording device."
      },
      {
        "ERR_ADM_STOP_RECORDING": "1013: An error occurs when stopping the recording device."
      },
      {
        "ERR_VDM_CAMERA_NOT_AUTHORIZED": "1501: Permission to access the camera is not granted. Check whether permission to access the camera permission is granted."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_experiencepoorreason",
    "name": "EXPERIENCE_POOR_REASON",
    "description": "Reasons why the QoE of the local user when receiving a remote audio stream is poor.",
    "parameters": [
      {
        "EXPERIENCE_REASON_NONE": "0: No reason, indicating a good QoE of the local user."
      },
      {
        "REMOTE_NETWORK_QUALITY_POOR": "1: The remote user's network quality is poor."
      },
      {
        "LOCAL_NETWORK_QUALITY_POOR": "2: The local user's network quality is poor."
      },
      {
        "WIRELESS_SIGNAL_POOR": "4: The local user's Wi-Fi or mobile network signal is weak."
      },
      {
        "WIFI_BLUETOOTH_COEXIST": "8: The local user enables both Wi-Fi and bluetooth, and their signals interfere with each other. As a result, audio transmission quality is undermined."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_experiencequalitytype",
    "name": "EXPERIENCE_QUALITY_TYPE",
    "description": "The Quality of Experience (QoE) of the local user when receiving a remote audio stream.",
    "parameters": [
      {
        "EXPERIENCE_QUALITY_GOOD": "0: The QoE of the local user is good."
      },
      {
        "EXPERIENCE_QUALITY_BAD": "1: The QoE of the local user is poor."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_externalvideosourcetype",
    "name": "EXTERNAL_VIDEO_SOURCE_TYPE",
    "description": "The external video frame encoding type.",
    "parameters": [
      {
        "VIDEO_FRAME": "0: The video frame is not encoded."
      },
      {
        "ENCODED_VIDEO_FRAME": "1: The video frame is encoded."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_featuretype",
    "name": "FeatureType",
    "description": "The type of the advanced feature.",
    "parameters": [
      {
        "VIDEO_VIRTUAL_BACKGROUND": "1: Virtual background."
      },
      {
        "VIDEO_BEAUTY_EFFECT": "2: Image enhancement."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_framerate",
    "name": "FRAME_RATE",
    "description": "The video frame rate.",
    "parameters": [
      {
        "FRAME_RATE_FPS_1": "1: 1 fps"
      },
      {
        "FRAME_RATE_FPS_7": "7: 7 fps"
      },
      {
        "FRAME_RATE_FPS_10": "10: 10 fps"
      },
      {
        "FRAME_RATE_FPS_15": "15: 15 fps"
      },
      {
        "FRAME_RATE_FPS_24": "24: 24 fps"
      },
      {
        "FRAME_RATE_FPS_30": "30: 30 fps"
      },
      {
        "FRAME_RATE_FPS_60": "60: 60 fps For Windows and macOS only."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_headphoneequalizerpreset",
    "name": "HEADPHONE_EQUALIZER_PRESET",
    "description": "Preset headphone equalizer types.",
    "parameters": [
      {
        "HEADPHONE_EQUALIZER_OFF": "The headphone equalizer is disabled, and the original audio is heard."
      },
      {
        "HEADPHONE_EQUALIZER_OVEREAR": "An equalizer is used for headphones."
      },
      {
        "HEADPHONE_EQUALIZER_INEAR": "An equalizer is used for in-ear headphones."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_interfaceidtype",
    "name": "INTERFACE_ID_TYPE",
    "description": "The interface class.",
    "parameters": [
      {
        "AGORA_IID_AUDIO_DEVICE_MANAGER": "1: The IAudioDeviceManager interface class."
      },
      {
        "AGORA_IID_VIDEO_DEVICE_MANAGER": "2: The IVideoDeviceManager interface class."
      },
      {
        "AGORA_IID_PARAMETER_ENGINE": "This interface class is deprecated."
      },
      {
        "AGORA_IID_SIGNALING_ENGINE": ""
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_lastmileproberesultstate",
    "name": "LASTMILE_PROBE_RESULT_STATE",
    "description": "The status of the last-mile probe test.",
    "parameters": [
      {
        "LASTMILE_PROBE_RESULT_COMPLETE": "1: The last-mile network probe test is complete."
      },
      {
        "LASTMILE_PROBE_RESULT_INCOMPLETE_NO_BWE": "2: The last-mile network probe test is incomplete because the bandwidth estimation is not available due to limited test resources. One possible reason is that testing resources are temporarily limited."
      },
      {
        "LASTMILE_PROBE_RESULT_UNAVAILABLE": "3: The last-mile network probe test is not carried out. Probably due to poor network conditions."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_lighteningcontrastlevel",
    "name": "LIGHTENING_CONTRAST_LEVEL",
    "description": "The contrast level.",
    "parameters": [
      {
        "LIGHTENING_CONTRAST_LOW": "0: Low contrast level."
      },
      {
        "LIGHTENING_CONTRAST_NORMAL": "1: (Default) Normal contrast level."
      },
      {
        "LIGHTENING_CONTRAST_HIGH": "2: High contrast level."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_localaudiostreamreason",
    "name": "LOCAL_AUDIO_STREAM_REASON",
    "description": "Reasons for local audio state changes.",
    "parameters": [
      {
        "LOCAL_AUDIO_STREAM_REASON_OK": "0: The local audio is normal."
      },
      {
        "LOCAL_AUDIO_STREAM_REASON_FAILURE": "1: No specified reason for the local audio failure. Remind your users to try to rejoin the channel."
      },
      {
        "LOCAL_AUDIO_STREAM_REASON_DEVICE_NO_PERMISSION": "2: No permission to use the local audio capturing device. Remind your users to grant permission. Deprecated: This enumerator is deprecated. Please use RECORD_AUDIO in the OnPermissionError callback instead."
      },
      {
        "LOCAL_AUDIO_STREAM_REASON_DEVICE_BUSY": "3: (Android and iOS only) The local audio capture device is already in use. Remind your users to check whether another application occupies the microphone. Local audio capture automatically resumes after the microphone is idle for about five seconds. You can also try to rejoin the channel after the microphone is idle."
      },
      {
        "LOCAL_AUDIO_STREAM_REASON_RECORD_FAILURE": "4: The local audio capture fails."
      },
      {
        "LOCAL_AUDIO_STREAM_REASON_ENCODE_FAILURE": "5: The local audio encoding fails."
      },
      {
        "LOCAL_AUDIO_STREAM_REASON_NO_RECORDING_DEVICE": "6: (Windows and macOS only) No local audio capture device. Remind your users to check whether the microphone is connected to the device properly in the control panel of the device or if the microphone is working properly."
      },
      {
        "LOCAL_AUDIO_STREAM_REASON_NO_PLAYOUT_DEVICE": "7: (Windows and macOS only) No local audio capture device. Remind your users to check whether the speaker is connected to the device properly in the control panel of the device or if the speaker is working properly."
      },
      {
        "LOCAL_AUDIO_STREAM_REASON_INTERRUPTED": "8: (Android and iOS only) The local audio capture is interrupted by a system call, smart assistants, or alarm clock. Prompt your users to end the phone call, smart assistants, or alarm clock if the local audio capture is required."
      },
      {
        "LOCAL_AUDIO_STREAM_REASON_RECORD_INVALID_ID": "9: (Windows only) The ID of the local audio-capture device is invalid. Prompt the user to check the audio capture device ID."
      },
      {
        "LOCAL_AUDIO_STREAM_REASON_PLAYOUT_INVALID_ID": "10: (Windows only) The ID of the local audio-playback device is invalid. Prompt the user to check the audio playback device ID."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_localaudiostreamstate",
    "name": "LOCAL_AUDIO_STREAM_STATE",
    "description": "The state of the local audio.",
    "parameters": [
      {
        "LOCAL_AUDIO_STREAM_STATE_STOPPED": "0: The local audio is in the initial state."
      },
      {
        "LOCAL_AUDIO_STREAM_STATE_RECORDING": "1: The local audio capturing device starts successfully."
      },
      {
        "LOCAL_AUDIO_STREAM_STATE_ENCODING": "2: The first audio frame encodes successfully."
      },
      {
        "LOCAL_AUDIO_STREAM_STATE_FAILED": "3: The local audio fails to start."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_localproxymode",
    "name": "LOCAL_PROXY_MODE",
    "description": "The connection mode with the Agora private media server.",
    "parameters": [
      {
        "kConnectivityFirst": "0: The SDK will first try to connect to the specified Agora private media server; if it cannot connect to the specified Agora private media server, it will connect to the Agora SD-RTN™."
      },
      {
        "kLocalOnly": "1: The SDK only tries to connect to the specified Agora private media server."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_localvideostreamreason",
    "name": "LOCAL_VIDEO_STREAM_REASON",
    "description": "Reasons for local video state changes.",
    "parameters": [
      {
        "LOCAL_VIDEO_STREAM_REASON_OK": "0: The local video is normal."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_FAILURE": "1: No specified reason for the local video failure."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_DEVICE_NO_PERMISSION": "2: No permission to use the local video capturing device. Prompt the user to grant permissions and rejoin the channel. Deprecated: This enumerator is deprecated. Please use CAMERA in the OnPermissionError callback instead."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_DEVICE_BUSY": "3: The local video capturing device is in use. Prompt the user to check if the camera is being used by another app, or try to rejoin the channel."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_CAPTURE_FAILURE": "4: The local video capture fails. Prompt the user to check whether the video capture device is working properly, whether the camera is used by another app, or try to rejoin the channel."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_CODEC_NOT_SUPPORT": "5: The local video encoding fails."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_CAPTURE_INBACKGROUND": "6: (iOS only) The app is in the background. Prompt the user that video capture cannot be performed normally when the app is in the background."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_CAPTURE_MULTIPLE_FOREGROUND_APPS": "7: (iOS only) The current app window is running in Slide Over, Split View, or Picture in Picture mode, and another app is occupying the camera. Prompt the user that the app cannot capture video properly when it is running in Slide Over, Split View, or Picture in Picture mode and another app is occupying the camera."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_DEVICE_NOT_FOUND": "8: Fails to find a local video capture device. Remind the user to check whether the camera is connected to the device properly or the camera is working properly, and then to rejoin the channel."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_DEVICE_DISCONNECTED": "9: (macOS and Windows only) The video capture device currently in use is disconnected (such as being unplugged)."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_DEVICE_INVALID_ID": "10: (macOS and Windows only) The SDK cannot find the video device in the video device list. Check whether the ID of the video device is valid."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_WINDOW_MINIMIZED": "11: (macOS and Windows only) The shared window is minimized when you call the StartScreenCaptureByWindowId method to share a window. The SDK cannot share a minimized window. Please prompt the user to unminimize the shared window."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_WINDOW_CLOSED": "12: (macOS and Windows only) The error code indicates that a window shared by the window ID has been closed or a full-screen window shared by the window ID has exited full-screen mode. After exiting full-screen mode, remote users cannot see the shared window. To prevent remote users from seeing a black screen, Agora recommends that you immediately stop screen sharing. Common scenarios reporting this error code:\n The local user closes the shared window.\n The local user shows some slides in full-screen mode first, and then shares the windows of the slides. After the user exits full-screen mode, the SDK reports this error code.\n The local user watches a web video or reads a web document in full-screen mode first, and then shares the window of the web video or document. After the user exits full-screen mode, the SDK reports this error code."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_WINDOW_OCCLUDED": "13: (Windows only) The window being shared is overlapped by another window, so the overlapped area is blacked out by the SDK during window sharing."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_DEVICE_INTERRUPT": "14: (Android only) Video capture is interrupted. Possible reasons include the following:\n The camera is being used by another app. Prompt the user to check if the camera is being used by another app.\n The current app has been switched to the background. You can use foreground services to notify the operating system and ensure that the app can still collect video when it switches to the background."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_DEVICE_FATAL_ERROR": "15: (Android only) The video capture device encounters an error. Prompt the user to close and restart the camera to restore functionality. If this operation does not solve the problem, check if the camera has a hardware failure."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_FAILURE": "21: (Windows and Android only) The currently captured window has no data."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_NO_PERMISSION": "22: (Windows and macOS only) No permission for screen capture."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_AUTO_FALLBACK": "24: (Windows only) An unexpected error occurred during screen sharing (possibly due to window blocking failure), resulting in decreased performance, but the screen sharing process itself was not affected."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_WINDOW_HIDDEN": "25: (Windows only) The window for the current screen capture is hidden and not visible on the current screen."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_WINDOW_RECOVER_FROM_HIDDEN": "26: (Windows only) The window for screen capture has been restored from hidden state."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_WINDOW_RECOVER_FROM_MINIMIZED": "27: (macOS and Windows only) The window for screen capture has been restored from the minimized state."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_PAUSED": "28: (Windows only) Screen capture has been paused. Common scenarios reporting this error code: The current screen may have been switched to a secure desktop, such as a UAC dialog box or Winlogon desktop."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_RESUMED": "29: (Windows only) Screen capture has resumed from paused state."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_SCREEN_CAPTURE_DISPLAY_DISCONNECTED": "30: (Windows and macOS only) The displayer used for screen capture is disconnected."
      },
      {
        "LOCAL_VIDEO_STREAM_REASON_DEVICE_SYSTEM_PRESSURE": "101: The current video capture device is unavailable due to excessive system pressure."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_localvideostreamstate",
    "name": "LOCAL_VIDEO_STREAM_STATE",
    "description": "Local video state types.",
    "parameters": [
      {
        "LOCAL_VIDEO_STREAM_STATE_STOPPED": "0: The local video is in the initial state."
      },
      {
        "LOCAL_VIDEO_STREAM_STATE_CAPTURING": "1: The local video capturing device starts successfully. The SDK also reports this state when you call StartScreenCaptureByWindowId to share a maximized window."
      },
      {
        "LOCAL_VIDEO_STREAM_STATE_ENCODING": "2: The first video frame is successfully encoded."
      },
      {
        "LOCAL_VIDEO_STREAM_STATE_FAILED": "3: Fails to start the local video."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_loglevel",
    "name": "LOG_LEVEL",
    "description": "The output log level of the SDK.",
    "parameters": [
      {
        "LOG_LEVEL_NONE": "0: Do not output any log information."
      },
      {
        "LOG_LEVEL_INFO": "0x0001: (Default) Output FATAL, ERROR, WARN, and INFO level log information. We recommend setting your log filter to this level."
      },
      {
        "LOG_LEVEL_WARN": "0x0002: Output FATAL, ERROR, and WARN level log information."
      },
      {
        "LOG_LEVEL_ERROR": "0x0004: Output FATAL and ERROR level log information."
      },
      {
        "LOG_LEVEL_FATAL": "0x0008: Output FATAL level log information."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_lowlightenhancelevel",
    "name": "LOW_LIGHT_ENHANCE_LEVEL",
    "description": "The low-light enhancement level.",
    "parameters": [
      {
        "LOW_LIGHT_ENHANCE_LEVEL_HIGH_QUALITY": "0: (Default) Promotes video quality during low-light enhancement. It processes the brightness, details, and noise of the video image. The performance consumption is moderate, the processing speed is moderate, and the overall video quality is optimal."
      },
      {
        "LOW_LIGHT_ENHANCE_LEVEL_FAST": "1: Promotes performance during low-light enhancement. It processes the brightness and details of the video image. The processing speed is faster."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_lowlightenhancemode",
    "name": "LOW_LIGHT_ENHANCE_MODE",
    "description": "The low-light enhancement mode.",
    "parameters": [
      {
        "LOW_LIGHT_ENHANCE_AUTO": "0: (Default) Automatic mode. The SDK automatically enables or disables the low-light enhancement feature according to the ambient light to compensate for the lighting level or prevent overexposure, as necessary."
      },
      {
        "LOW_LIGHT_ENHANCE_MANUAL": "1: Manual mode. Users need to enable or disable the low-light enhancement feature manually."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_maxuseraccountlengthtype",
    "name": "MAX_USER_ACCOUNT_LENGTH_TYPE",
    "description": "The maximum length of the user account.",
    "parameters": [
      {
        "MAX_USER_ACCOUNT_LENGTH": "The maximum length of the user account is 256 bytes."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_mediadevicestatetype",
    "name": "MEDIA_DEVICE_STATE_TYPE",
    "description": "Media device states.",
    "parameters": [
      {
        "MEDIA_DEVICE_STATE_IDLE": "0: The device is ready for use."
      },
      {
        "MEDIA_DEVICE_STATE_ACTIVE": "1: The device is in use."
      },
      {
        "MEDIA_DEVICE_STATE_DISABLED": "2: The device is disabled."
      },
      {
        "MEDIA_DEVICE_STATE_PLUGGED_IN": "3: The device is plugged in."
      },
      {
        "MEDIA_DEVICE_STATE_NOT_PRESENT": "4: The device is not found."
      },
      {
        "MEDIA_DEVICE_STATE_UNPLUGGED": "8: The device is unplugged."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_mediadevicetype",
    "name": "MEDIA_DEVICE_TYPE",
    "description": "Media device types.",
    "parameters": [
      {
        "UNKNOWN_AUDIO_DEVICE": "-1: Unknown device type."
      },
      {
        "AUDIO_PLAYOUT_DEVICE": "0: Audio playback device."
      },
      {
        "AUDIO_RECORDING_DEVICE": "1: Audio capturing device."
      },
      {
        "VIDEO_RENDER_DEVICE": "2: Video rendering device (graphics card)."
      },
      {
        "VIDEO_CAPTURE_DEVICE": "3: Video capturing device."
      },
      {
        "AUDIO_APPLICATION_PLAYOUT_DEVICE": "4: Audio playback device for an app."
      },
      {
        "AUDIO_VIRTUAL_PLAYOUT_DEVICE": "(For macOS only) 5: Virtual audio playback device (virtual sound card)."
      },
      {
        "AUDIO_VIRTUAL_RECORDING_DEVICE": "(For macOS only) 6: Virtual audio capturing device (virtual sound card)."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_mediaplayereason",
    "name": "MEDIA_PLAYER_REASON",
    "description": "Reasons for the changes in the media player status.",
    "parameters": [
      {
        "PLAYER_REASON_NONE": "0: No error."
      },
      {
        "PLAYER_REASON_INVALID_ARGUMENTS": "-1: Invalid arguments."
      },
      {
        "PLAYER_REASON_INTERNAL": "-2: Internal error."
      },
      {
        "PLAYER_REASON_NO_RESOURCE": "-3: No resource."
      },
      {
        "PLAYER_REASON_INVALID_MEDIA_SOURCE": "-4: Invalid media resource."
      },
      {
        "PLAYER_REASON_UNKNOWN_STREAM_TYPE": "-5: The media stream type is unknown."
      },
      {
        "PLAYER_REASON_OBJ_NOT_INITIALIZED": "-6: The object is not initialized."
      },
      {
        "PLAYER_REASON_CODEC_NOT_SUPPORTED": "-7: The codec is not supported."
      },
      {
        "PLAYER_REASON_VIDEO_RENDER_FAILED": "-8: Invalid renderer."
      },
      {
        "PLAYER_REASON_INVALID_STATE": "-9: An error with the internal state of the player occurs."
      },
      {
        "PLAYER_REASON_URL_NOT_FOUND": "-10: The URL of the media resource cannot be found."
      },
      {
        "PLAYER_REASON_INVALID_CONNECTION_STATE": "-11: Invalid connection between the player and the Agora Server."
      },
      {
        "PLAYER_REASON_SRC_BUFFER_UNDERFLOW": "-12: The playback buffer is insufficient."
      },
      {
        "PLAYER_REASON_INTERRUPTED": "-13: The playback is interrupted."
      },
      {
        "PLAYER_REASON_NOT_SUPPORTED": "-14: The SDK does not support the method being called."
      },
      {
        "PLAYER_REASON_TOKEN_EXPIRED": "-15: The authentication information of the media resource is expired."
      },
      {
        "PLAYER_REASON_UNKNOWN": "-17: An unknown error."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_mediaplayerevent",
    "name": "MEDIA_PLAYER_EVENT",
    "description": "Media player events.",
    "parameters": [
      {
        "PLAYER_EVENT_SEEK_BEGIN": "0: The player begins to seek to a new playback position."
      },
      {
        "PLAYER_EVENT_SEEK_COMPLETE": "1: The player finishes seeking to a new playback position."
      },
      {
        "PLAYER_EVENT_SEEK_ERROR": "2: An error occurs when seeking to a new playback position."
      },
      {
        "PLAYER_EVENT_AUDIO_TRACK_CHANGED": "5: The audio track used by the player has been changed."
      },
      {
        "PLAYER_EVENT_BUFFER_LOW": "6: The currently buffered data is not enough to support playback."
      },
      {
        "PLAYER_EVENT_BUFFER_RECOVER": "7: The currently buffered data is just enough to support playback."
      },
      {
        "PLAYER_EVENT_FREEZE_START": "8: The audio or video playback freezes."
      },
      {
        "PLAYER_EVENT_FREEZE_STOP": "9: The audio or video playback resumes without freezing."
      },
      {
        "PLAYER_EVENT_SWITCH_BEGIN": "10: The player starts switching the media resource."
      },
      {
        "PLAYER_EVENT_SWITCH_COMPLETE": "11: Media resource switching is complete."
      },
      {
        "PLAYER_EVENT_SWITCH_ERROR": "12: Media resource switching error."
      },
      {
        "PLAYER_EVENT_FIRST_DISPLAYED": "13: The first video frame is rendered."
      },
      {
        "PLAYER_EVENT_REACH_CACHE_FILE_MAX_COUNT": "14: The cached media files reach the limit in number."
      },
      {
        "PLAYER_EVENT_REACH_CACHE_FILE_MAX_SIZE": "15: The cached media files reach the limit in aggregate storage space."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_mediaplayermetadatatype",
    "name": "MEDIA_PLAYER_METADATA_TYPE",
    "description": "The type of media metadata.",
    "parameters": [
      {
        "PLAYER_METADATA_TYPE_UNKNOWN": "0: The type is unknown."
      },
      {
        "PLAYER_METADATA_TYPE_SEI": "1: The type is SEI."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_mediaplayerstate",
    "name": "MEDIA_PLAYER_STATE",
    "description": "The playback state.",
    "parameters": [
      {
        "PLAYER_STATE_IDLE": "0: The default state. The media player returns this state code before you open the media resource or after you stop the playback."
      },
      {
        "PLAYER_STATE_OPENING": "1: Opening the media resource."
      },
      {
        "PLAYER_STATE_OPEN_COMPLETED": "2: Opens the media resource successfully."
      },
      {
        "PLAYER_STATE_PLAYING": "3: The media resource is playing."
      },
      {
        "PLAYER_STATE_PAUSED": "4: Pauses the playback."
      },
      {
        "PLAYER_STATE_PLAYBACK_COMPLETED": "5: The playback is complete."
      },
      {
        "PLAYER_STATE_PLAYBACK_ALL_LOOPS_COMPLETED": "6: The loop is complete."
      },
      {
        "PLAYER_STATE_STOPPED": "7: The playback stops."
      },
      {
        "PLAYER_STATE_FAILED": "100: The media player fails to play the media resource."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_mediarecordercontainerformat",
    "name": "MediaRecorderContainerFormat",
    "description": "The format of the recording file.",
    "parameters": [
      {
        "FORMAT_MP4": "1: (Default) MP4."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_mediarecorderstreamtype",
    "name": "MediaRecorderStreamType",
    "description": "The recording content.",
    "parameters": [
      {
        "STREAM_TYPE_AUDIO": "Only audio."
      },
      {
        "STREAM_TYPE_VIDEO": "Only video."
      },
      {
        "STREAM_TYPE_BOTH": "(Default) Audio and video."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_mediasourcetype",
    "name": "MEDIA_SOURCE_TYPE",
    "description": "Media source type.",
    "parameters": [
      {
        "AUDIO_PLAYOUT_SOURCE": "0: Audio playback device."
      },
      {
        "AUDIO_RECORDING_SOURCE": "1: Audio capturing device."
      },
      {
        "PRIMARY_CAMERA_SOURCE": "2: The primary camera."
      },
      {
        "SECONDARY_CAMERA_SOURCE": "3: A secondary camera."
      },
      {
        "CUSTOM_VIDEO_SOURCE": "6: Custom video source."
      },
      {
        "UNKNOWN_MEDIA_SOURCE": "100: Unknown media source."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_mediastreamtype",
    "name": "MEDIA_STREAM_TYPE",
    "description": "The type of the media stream.",
    "parameters": [
      {
        "STREAM_TYPE_UNKNOWN": "0: The type is unknown."
      },
      {
        "STREAM_TYPE_VIDEO": "1: The video stream."
      },
      {
        "STREAM_TYPE_AUDIO": "2: The audio stream."
      },
      {
        "STREAM_TYPE_SUBTITLE": "3: The subtitle stream."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_mediatraceevent",
    "name": "MEDIA_TRACE_EVENT",
    "description": "The rendering state of the media frame.",
    "parameters": [
      {
        "MEDIA_TRACE_EVENT_VIDEO_RENDERED": "0: The video frame has been rendered."
      },
      {
        "MEDIA_TRACE_EVENT_VIDEO_DECODED": "1: The video frame has been decoded."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_metadatatype",
    "name": "METADATA_TYPE",
    "description": "Metadata type of the observer. We only support video metadata for now.",
    "parameters": [
      {
        "UNKNOWN_METADATA": "The type of metadata is unknown."
      },
      {
        "VIDEO_METADATA": "The type of metadata is video."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_musiccachestatustype",
    "name": "MUSIC_CACHE_STATUS_TYPE",
    "description": "The loading statuses of music assets.",
    "parameters": [
      {
        "MUSIC_CACHE_STATUS_TYPE_CACHED": "0: 音乐资源已缓存。"
      },
      {
        "MUSIC_CACHE_STATUS_TYPE_CACHING": "1: 音乐资源正在缓存。"
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_musiccontentcenterstatereason",
    "name": "MusicContentCenterStateReason",
    "description": "The status codes of requests to music content center.",
    "parameters": [
      {
        "kMusicContentCenterReasonOk": "0: The request succeeds."
      },
      {
        "kMusicContentCenterReasonError": "1：一般错误，无明确归因。"
      },
      {
        "kMusicContentCenterReasonGateway": "2：网关异常。 Possible reasons include the following:\n 当前使用的 Token 已过期。 请重新生成 Token。\n The token is invalid. 请确保你使用的是 RTM Token。\n Network error. 请检查你的网络。"
      },
      {
        "kMusicContentCenterReasonPermissionAndResource": "3：权限错误或音乐资源不存在。 请确保你的项目已开通声网音乐内容中心权限，请 。"
      },
      {
        "kMusicContentCenterReasonInternalDataParse": "4：内部数据解析错误。 请 。"
      },
      {
        "kMusicContentCenterReasonMusicLoading": "5：音乐资源加载时出错。 请 。"
      },
      {
        "kMusicContentCenterReasonMusicDecryption": "6：音乐资源解密时出错。 请 。"
      },
      {
        "kMusicContentCenterReasonHttpInternalError": "7：HTTP 内部出现错误。 请稍后重试。"
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_networktype",
    "name": "NETWORK_TYPE",
    "description": "Network type.",
    "parameters": [
      {
        "NETWORK_TYPE_UNKNOWN": "-1: The network type is unknown."
      },
      {
        "NETWORK_TYPE_DISCONNECTED": "0: The SDK disconnects from the network."
      },
      {
        "NETWORK_TYPE_LAN": "1: The network type is LAN."
      },
      {
        "NETWORK_TYPE_WIFI": "2: The network type is Wi-Fi (including hotspots)."
      },
      {
        "NETWORK_TYPE_MOBILE_2G": "3: The network type is mobile 2G."
      },
      {
        "NETWORK_TYPE_MOBILE_3G": "4: The network type is mobile 3G."
      },
      {
        "NETWORK_TYPE_MOBILE_4G": "5: The network type is mobile 4G."
      },
      {
        "NETWORK_TYPE_MOBILE_5G": "6: The network type is mobile 5G."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_observermode",
    "name": "OBSERVER_MODE",
    "description": "The mode for receiving data.",
    "parameters": [
      {
        "RAW_DATA": "Raw data mode, which means the SDK sends you raw data."
      },
      {
        "INTPTR": "Pointer mode, which means the SDK sends you the pointer to the raw data."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_orientationmode",
    "name": "ORIENTATION_MODE",
    "description": "Video output orientation mode.",
    "parameters": [
      {
        "ORIENTATION_MODE_ADAPTIVE": "0: (Default) The output video always follows the orientation of the captured video. The receiver takes the rotational information passed on from the video encoder. This mode applies to scenarios where video orientation can be adjusted on the receiver.\n If the captured video is in landscape mode, the output video is in landscape mode.\n If the captured video is in portrait mode, the output video is in portrait mode."
      },
      {
        "ORIENTATION_MODE_FIXED_LANDSCAPE": "1: In this mode, the SDK always outputs videos in landscape (horizontal) mode. If the captured video is in portrait mode, the video encoder crops it to fit the output. Applies to situations where the receiving end cannot process the rotational information. For example, CDN live streaming."
      },
      {
        "ORIENTATION_MODE_FIXED_PORTRAIT": "2: In this mode, the SDK always outputs video in portrait (portrait) mode. If the captured video is in landscape mode, the video encoder crops it to fit the output. Applies to situations where the receiving end cannot process the rotational information. For example, CDN live streaming."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_permissiontype",
    "name": "PERMISSION_TYPE",
    "description": "The type of the device permission.",
    "parameters": [
      {
        "RECORD_AUDIO": "0: Permission for the audio capture device."
      },
      {
        "CAMERA": "1: Permission for the camera."
      },
      {
        "SCREEN_CAPTURE": "(For Android only) 2: Permission for screen sharing."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_playerpreloadevent",
    "name": "PLAYER_PRELOAD_EVENT",
    "description": "Events that occur when media resources are preloaded.",
    "parameters": [
      {
        "PLAYER_PRELOAD_EVENT_BEGIN": "0: Starts preloading media resources."
      },
      {
        "PLAYER_PRELOAD_EVENT_COMPLETE": "1: Preloading media resources is complete."
      },
      {
        "PLAYER_PRELOAD_EVENT_ERROR": "2: An error occurs when preloading media resources."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_preloadstate",
    "name": "PreloadState",
    "description": "The loading statuses of music assets.",
    "parameters": [
      {
        "kPreloadStateCompleted": "0: The preload of music assets is complete."
      },
      {
        "kPreloadStateFailed": "1: The preload of music assets fails."
      },
      {
        "kPreloadStatePreloading": "2: The music assets are preloading."
      },
      {
        "kPreloadStateRemoved": "3：缓存的音乐资源已被移除。"
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_prioritytype",
    "name": "PRIORITY_TYPE",
    "description": "The priority of the remote user.",
    "parameters": [
      {
        "PRIORITY_HIGH": "The user's priority is high."
      },
      {
        "PRIORITY_NORMAL": "(Default) The user's priority is normal."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_proxytype",
    "name": "PROXY_TYPE",
    "description": "The cloud proxy type.",
    "parameters": [
      {
        "NONE_PROXY_TYPE": "0: Reserved for future use."
      },
      {
        "UDP_PROXY_TYPE": "1: The cloud proxy for the UDP protocol, that is, the Force UDP cloud proxy mode. In this mode, the SDK always transmits data over UDP."
      },
      {
        "TCP_PROXY_TYPE": "2: The cloud proxy for the TCP (encryption) protocol, that is, the Force TCP cloud proxy mode. In this mode, the SDK always transmits data over TCP/TLS 443."
      },
      {
        "LOCAL_PROXY_TYPE": "3: Reserved for future use."
      },
      {
        "TCP_PROXY_AUTO_FALLBACK_TYPE": "4: Automatic mode. In this mode, the SDK attempts a direct connection to SD-RTN™ and automatically switches to TCP/TLS 443 if the attempt fails."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_qualityadaptindication",
    "name": "QUALITY_ADAPT_INDICATION",
    "description": "Quality change of the local video in terms of target frame rate and target bit rate since last count.",
    "parameters": [
      {
        "ADAPT_NONE": "0: The local video quality stays the same."
      },
      {
        "ADAPT_UP_BANDWIDTH": "1: The local video quality improves because the network bandwidth increases."
      },
      {
        "ADAPT_DOWN_BANDWIDTH": "2: The local video quality deteriorates because the network bandwidth decreases."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_qualityreportformattype",
    "name": "QUALITY_REPORT_FORMAT_TYPE",
    "description": "Formats of the quality report.",
    "parameters": [
      {
        "QUALITY_REPORT_JSON": "0: The quality report in JSON format."
      },
      {
        "QUALITY_REPORT_HTML": "1: The quality report in HTML format."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_qualitytype",
    "name": "QUALITY_TYPE",
    "description": "Network quality types.",
    "parameters": [
      {
        "QUALITY_UNKNOWN": "0: The network quality is unknown."
      },
      {
        "QUALITY_EXCELLENT": "1: The network quality is excellent."
      },
      {
        "QUALITY_GOOD": "2: The network quality is quite good, but the bitrate may be slightly lower than excellent."
      },
      {
        "QUALITY_POOR": "3: Users can feel the communication is slightly impaired."
      },
      {
        "QUALITY_BAD": "4: Users cannot communicate smoothly."
      },
      {
        "QUALITY_VBAD": "5: The quality is so bad that users can barely communicate."
      },
      {
        "QUALITY_DOWN": "6: The network is down and users cannot communicate at all."
      },
      {
        "QUALITY_DETECTING": "8: The last-mile network probe test is in progress."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_rawaudioframeopmodetype",
    "name": "RAW_AUDIO_FRAME_OP_MODE_TYPE",
    "description": "The use mode of the audio data.",
    "parameters": [
      {
        "RAW_AUDIO_FRAME_OP_MODE_READ_ONLY": "0: Read-only mode, For example, when users acquire the data with the Agora SDK, then start the media push."
      },
      {
        "RAW_AUDIO_FRAME_OP_MODE_READ_WRITE": "2: Read and write mode, For example, when users have their own audio-effect processing module and perform some voice preprocessing, such as a voice change."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_recorderreasoncode",
    "name": "RecorderReasonCode",
    "description": "The reason for the state change.",
    "parameters": [
      {
        "RECORDER_REASON_NONE": "0: No error."
      },
      {
        "RECORDER_ERROR_WRITE_FAILED": "1: The SDK fails to write the recorded data to a file."
      },
      {
        "RECORDER_ERROR_NO_STREAM": "2: The SDK does not detect any audio and video streams, or audio and video streams are interrupted for more than five seconds during recording."
      },
      {
        "RECORDER_ERROR_OVER_MAX_DURATION": "3: The recording duration exceeds the upper limit."
      },
      {
        "RECORDER_ERROR_CONFIG_CHANGED": "4: The recording configuration changes."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_recorderstate",
    "name": "RecorderState",
    "description": "The current recording state.",
    "parameters": [
      {
        "RECORDER_STATE_ERROR": "-1: An error occurs during the recording. See RecorderReasonCode for the reason."
      },
      {
        "RECORDER_STATE_START": "2: The audio and video recording starts."
      },
      {
        "RECORDER_STATE_STOP": "3: The audio and video recording stops."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_remoteaudiostate",
    "name": "REMOTE_AUDIO_STATE",
    "description": "Remote audio states.",
    "parameters": [
      {
        "REMOTE_AUDIO_STATE_STOPPED": "0: The local audio is in the initial state. The SDK reports this state in the case of REMOTE_AUDIO_REASON_LOCAL_MUTED, REMOTE_AUDIO_REASON_REMOTE_MUTED or REMOTE_AUDIO_REASON_REMOTE_OFFLINE."
      },
      {
        "REMOTE_AUDIO_STATE_STARTING": "1: The first remote audio packet is received."
      },
      {
        "REMOTE_AUDIO_STATE_DECODING": "2: The remote audio stream is decoded and plays normally. The SDK reports this state in the case of REMOTE_AUDIO_REASON_NETWORK_RECOVERY, REMOTE_AUDIO_REASON_LOCAL_UNMUTED or REMOTE_AUDIO_REASON_REMOTE_UNMUTED."
      },
      {
        "REMOTE_AUDIO_STATE_FROZEN": "3: The remote audio is frozen. The SDK reports this state in the case of REMOTE_AUDIO_REASON_NETWORK_CONGESTION."
      },
      {
        "REMOTE_AUDIO_STATE_FAILED": "4: The remote audio fails to start. The SDK reports this state in the case of REMOTE_AUDIO_REASON_INTERNAL."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_remoteaudiostatereason",
    "name": "REMOTE_AUDIO_STATE_REASON",
    "description": "The reason for the remote audio state change.",
    "parameters": [
      {
        "REMOTE_AUDIO_REASON_INTERNAL": "0: The SDK reports this reason when the audio state changes."
      },
      {
        "REMOTE_AUDIO_REASON_NETWORK_CONGESTION": "1: Network congestion."
      },
      {
        "REMOTE_AUDIO_REASON_NETWORK_RECOVERY": "2: Network recovery."
      },
      {
        "REMOTE_AUDIO_REASON_LOCAL_MUTED": "3: The local user stops receiving the remote audio stream or disables the audio module."
      },
      {
        "REMOTE_AUDIO_REASON_LOCAL_UNMUTED": "4: The local user resumes receiving the remote audio stream or enables the audio module."
      },
      {
        "REMOTE_AUDIO_REASON_REMOTE_MUTED": "5: The remote user stops sending the audio stream or disables the audio module."
      },
      {
        "REMOTE_AUDIO_REASON_REMOTE_UNMUTED": "6: The remote user resumes sending the audio stream or enables the audio module."
      },
      {
        "REMOTE_AUDIO_REASON_REMOTE_OFFLINE": "7: The remote user leaves the channel."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_remotevideostate",
    "name": "REMOTE_VIDEO_STATE",
    "description": "The state of the remote video stream.",
    "parameters": [
      {
        "REMOTE_VIDEO_STATE_STOPPED": "0: The remote video is in the initial state. The SDK reports this state in the case of REMOTE_VIDEO_STATE_REASON_LOCAL_MUTED, REMOTE_VIDEO_STATE_REASON_REMOTE_MUTED, or REMOTE_VIDEO_STATE_REASON_REMOTE_OFFLINE."
      },
      {
        "REMOTE_VIDEO_STATE_STARTING": "1: The first remote video packet is received."
      },
      {
        "REMOTE_VIDEO_STATE_DECODING": "2: The remote video stream is decoded and plays normally. The SDK reports this state in the case of REMOTE_VIDEO_STATE_REASON_NETWORK_RECOVERY, REMOTE_VIDEO_STATE_REASON_LOCAL_UNMUTED, REMOTE_VIDEO_STATE_REASON_REMOTE_UNMUTED, or REMOTE_VIDEO_STATE_REASON_AUDIO_FALLBACK_RECOVERY."
      },
      {
        "REMOTE_VIDEO_STATE_FROZEN": "3: The remote video is frozen. The SDK reports this state in the case of REMOTE_VIDEO_STATE_REASON_NETWORK_CONGESTION or REMOTE_VIDEO_STATE_REASON_AUDIO_FALLBACK."
      },
      {
        "REMOTE_VIDEO_STATE_FAILED": "4: The remote video fails to start. The SDK reports this state in the case of REMOTE_VIDEO_STATE_REASON_INTERNAL."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_remotevideostatereason",
    "name": "REMOTE_VIDEO_STATE_REASON",
    "description": "The reason for the remote video state change.",
    "parameters": [
      {
        "REMOTE_VIDEO_STATE_REASON_INTERNAL": "0: The SDK reports this reason when the video state changes."
      },
      {
        "REMOTE_VIDEO_STATE_REASON_NETWORK_CONGESTION": "1: Network congestion."
      },
      {
        "REMOTE_VIDEO_STATE_REASON_NETWORK_RECOVERY": "2: Network is recovered."
      },
      {
        "REMOTE_VIDEO_STATE_REASON_LOCAL_MUTED": "3: The local user stops receiving the remote video stream or disables the video module."
      },
      {
        "REMOTE_VIDEO_STATE_REASON_LOCAL_UNMUTED": "4: The local user resumes receiving the remote video stream or enables the video module."
      },
      {
        "REMOTE_VIDEO_STATE_REASON_REMOTE_MUTED": "5: The remote user stops sending the video stream or disables the video module."
      },
      {
        "REMOTE_VIDEO_STATE_REASON_REMOTE_UNMUTED": "6: The remote user resumes sending the video stream or enables the video module."
      },
      {
        "REMOTE_VIDEO_STATE_REASON_REMOTE_OFFLINE": "7: The remote user leaves the channel."
      },
      {
        "REMOTE_VIDEO_STATE_REASON_AUDIO_FALLBACK": "8: The remote audio-and-video stream falls back to the audio-only stream due to poor network conditions."
      },
      {
        "REMOTE_VIDEO_STATE_REASON_AUDIO_FALLBACK_RECOVERY": "9: The remote audio-only stream switches back to the audio-and-video stream after the network conditions improve."
      },
      {
        "REMOTE_VIDEO_STATE_REASON_SDK_IN_BACKGROUND": "12: (iOS only) The remote user's app has switched to the background."
      },
      {
        "REMOTE_VIDEO_STATE_REASON_CODEC_NOT_SUPPORT": "13: The local video decoder does not support decoding the remote video stream."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_rendermodetype",
    "name": "RENDER_MODE_TYPE",
    "description": "Video display modes.",
    "parameters": [
      {
        "RENDER_MODE_HIDDEN": "1: Hidden mode. The priority is to fill the window. Any excess video that does not match the window size will be cropped."
      },
      {
        "RENDER_MODE_FIT": "2: Fit mode. The priority is to ensure that all video content is displayed. Any areas of the window that are not filled due to the mismatch between video size and window size will be filled with black."
      },
      {
        "RENDER_MODE_ADAPTIVE": "3: Adaptive mode. Deprecated: This enumerator is deprecated and not recommended for use."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_rhythmplayerreason",
    "name": "RHYTHM_PLAYER_REASON",
    "description": "Virtual Metronome error message.",
    "parameters": [
      {
        "RHYTHM_PLAYER_REASON_OK": "(0): The beat files are played normally without errors."
      },
      {
        "RHYTHM_PLAYER_REASON_FAILED": "1: A general error; no specific reason."
      },
      {
        "RHYTHM_PLAYER_REASON_CAN_NOT_OPEN": "801: There is an error when opening the beat files."
      },
      {
        "RHYTHM_PLAYER_REASON_CAN_NOT_PLAY": "802: There is an error when playing the beat files."
      },
      {
        "RHYTHM_PLAYER_REASON_FILE_OVER_DURATION_LIMIT": "(803): The duration of the beat file exceeds the limit. The maximum duration is 1.2 seconds."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_rhythmplayerstatetype",
    "name": "RHYTHM_PLAYER_STATE_TYPE",
    "description": "Virtual metronome state.",
    "parameters": [
      {
        "RHYTHM_PLAYER_STATE_IDLE": "(810): The virtual metronome is not enabled or disabled already."
      },
      {
        "RHYTHM_PLAYER_STATE_OPENING": "811: Opening the beat files."
      },
      {
        "RHYTHM_PLAYER_STATE_DECODING": "812: Decoding the beat files."
      },
      {
        "RHYTHM_PLAYER_STATE_PLAYING": "813: The beat files are playing."
      },
      {
        "RHYTHM_PLAYER_STATE_FAILED": "814: Failed to start virtual metronome. You can use the reported errorCode to troubleshoot the cause of the error, or you can try to start the virtual metronome again."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_rtmpstreamingevent",
    "name": "RTMP_STREAMING_EVENT",
    "description": "Events during the Media Push.",
    "parameters": [
      {
        "RTMP_STREAMING_EVENT_FAILED_LOAD_IMAGE": "1: An error occurs when you add a background image or a watermark image in the Media Push."
      },
      {
        "RTMP_STREAMING_EVENT_URL_ALREADY_IN_USE": "2: The streaming URL is already being used for Media Push. If you want to start new streaming, use a new streaming URL."
      },
      {
        "RTMP_STREAMING_EVENT_ADVANCED_FEATURE_NOT_SUPPORT": "3: The feature is not supported."
      },
      {
        "RTMP_STREAMING_EVENT_REQUEST_TOO_OFTEN": "4: Reserved."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_rtmpstreamlifecycletype",
    "name": "RTMP_STREAM_LIFE_CYCLE_TYPE",
    "description": "Lifecycle of the CDN live video stream.\n\nDeprecated",
    "parameters": [
      {
        "RTMP_STREAM_LIFE_CYCLE_BIND2CHANNEL": "Bind to the channel lifecycle. If all hosts leave the channel, the CDN live streaming stops after 30 seconds."
      },
      {
        "RTMP_STREAM_LIFE_CYCLE_BIND2OWNER": "Bind to the owner of the RTMP stream. If the owner leaves the channel, the CDN live streaming stops immediately."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_rtmpstreampublishreason",
    "name": "RTMP_STREAM_PUBLISH_REASON",
    "description": "Reasons for changes in the status of RTMP or RTMPS streaming.",
    "parameters": [
      {
        "RTMP_STREAM_PUBLISH_REASON_OK": "0: The RTMP or RTMPS streaming has not started or has ended."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_INVALID_ARGUMENT": "1: Invalid argument used. Check the parameter setting."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_ENCRYPTED_STREAM_NOT_ALLOWED": "2: The RTMP or RTMPS streaming is encrypted and cannot be published."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_CONNECTION_TIMEOUT": "3: Timeout for the RTMP or RTMPS streaming."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_INTERNAL_SERVER_ERROR": "4: An error occurs in Agora's streaming server."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_RTMP_SERVER_ERROR": "5: An error occurs in the CDN server."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_TOO_OFTEN": "6: The RTMP or RTMPS streaming publishes too frequently."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_REACH_LIMIT": "7: The host publishes more than 10 URLs. Delete the unnecessary URLs before adding new ones."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_NOT_AUTHORIZED": "8: The host manipulates other hosts' URLs. For example, the host updates or stops other hosts' streams. Check your app logic."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_STREAM_NOT_FOUND": "9: Agora's server fails to find the RTMP or RTMPS streaming."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_FORMAT_NOT_SUPPORTED": "10: The format of the RTMP or RTMPS streaming URL is not supported. Check whether the URL format is correct."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_NOT_BROADCASTER": "11: The user role is not host, so the user cannot use the CDN live streaming function. Check your application code logic."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_TRANSCODING_NO_MIX_STREAM": "13: The UpdateRtmpTranscoding method is called to update the transcoding configuration in a scenario where there is streaming without transcoding. Check your application code logic."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_NET_DOWN": "14: Errors occurred in the host's network."
      },
      {
        "RTMP_STREAM_PUBLISH_REASON_INVALID_PRIVILEGE": "16: Your project does not have permission to use streaming services."
      },
      {
        "RTMP_STREAM_UNPUBLISH_REASON_OK": "100: The streaming has been stopped normally. After you stop the Media Push, the SDK returns this value."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_rtmpstreampublishstate",
    "name": "RTMP_STREAM_PUBLISH_STATE",
    "description": "States of the Media Push.",
    "parameters": [
      {
        "RTMP_STREAM_PUBLISH_STATE_IDLE": "0: The Media Push has not started or has ended."
      },
      {
        "RTMP_STREAM_PUBLISH_STATE_CONNECTING": "1: The streaming server and CDN server are being connected."
      },
      {
        "RTMP_STREAM_PUBLISH_STATE_RUNNING": "2: The RTMP or RTMPS streaming publishes. The SDK successfully publishes the RTMP or RTMPS streaming and returns this state."
      },
      {
        "RTMP_STREAM_PUBLISH_STATE_RECOVERING": "3: The RTMP or RTMPS streaming is recovering. When exceptions occur to the CDN, or the streaming is interrupted, the SDK tries to resume RTMP or RTMPS streaming and returns this state.\n If the SDK successfully resumes the streaming, RTMP_STREAM_PUBLISH_STATE_RUNNING (2) returns.\n If the streaming does not resume within 60 seconds or server errors occur, RTMP_STREAM_PUBLISH_STATE_FAILURE (4) returns. If you feel that 60 seconds is too long, you can also actively try to reconnect."
      },
      {
        "RTMP_STREAM_PUBLISH_STATE_FAILURE": "4: The RTMP or RTMPS streaming fails. After a failure, you can troubleshoot the cause of the error through the returned error code."
      },
      {
        "RTMP_STREAM_PUBLISH_STATE_DISCONNECTING": "5: The SDK is disconnecting from the Agora streaming server and CDN. When you call StopRtmpStream to stop the Media Push normally, the SDK reports the Media Push state as RTMP_STREAM_PUBLISH_STATE_DISCONNECTING and RTMP_STREAM_PUBLISH_STATE_IDLE in sequence."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_saeconnectionchangedreasontype",
    "name": "SAE_CONNECTION_CHANGED_REASON_TYPE",
    "description": "The reason for the change in the connection state between the SDK and the Agora Spatial Audio Server.",
    "parameters": [
      {
        "SAE_CONNECTION_CHANGED_DEFAULT": "0: No error."
      },
      {
        "SAE_CONNECTION_CHANGED_CONNECTING": "1: The SDK is establishing a connection."
      },
      {
        "SAE_CONNECTION_CHANGED_CREATE_ROOM_FAIL": "2: The SDK failed to create the room."
      },
      {
        "SAE_CONNECTION_CHANGED_RTM_DISCONNECT": "3: The connection between the SDK and the RTM system is interrupted."
      },
      {
        "SAE_CONNECTION_CHANGED_RTM_ABORTED": "4: The user is kicked out by the RTM system."
      },
      {
        "SAE_CONNECTION_CHANGED_LOST_SYNC": "5: The SDK has not received a message from the Agora Spatial Audio Server for more than 15 seconds."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_saeconnectionstatetype",
    "name": "SAE_CONNECTION_STATE_TYPE",
    "description": "The connection state between the SDK and the Agora Spatial Audio Server.",
    "parameters": [
      {
        "SAE_CONNECTION_STATE_CONNECTING": "0: The SDK is connecting to the Agora Spatial Audio Server."
      },
      {
        "SAE_CONNECTION_STATE_CONNECTED": "1: Connected. The spatial audio effect settings such as UpdateSelfPosition only take effect in this state."
      },
      {
        "SAE_CONNECTION_STATE_DISCONNECTED": "2: The connection is disconnected."
      },
      {
        "SAE_CONNECTION_STATE_RECONNECTING": "3: The SDK keeps reconnecting to the Agora Spatial Audio Server."
      },
      {
        "SAE_CONNECTION_STATE_RECONNECTED": "4: The connection has been reestablished."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_saedeployregion",
    "name": "SAE_DEPLOY_REGION",
    "description": "The region in which the Agora Spatial Audio Server to be used is located.",
    "parameters": [
      {
        "SAE_DEPLOY_REGION_CN": "(Default) Mainland China."
      },
      {
        "SAE_DEPLOY_REGION_NA": "North America."
      },
      {
        "SAE_DEPLOY_REGION_EU": "Europe."
      },
      {
        "SAE_DEPLOY_REGION_AS": "Asia, excluding Mainland China."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_screencaptureframeratecapability",
    "name": "SCREEN_CAPTURE_FRAMERATE_CAPABILITY",
    "description": "The highest frame rate supported by the screen sharing device.",
    "parameters": [
      {
        "SCREEN_CAPTURE_FRAMERATE_CAPABILITY_15_FPS": "0: The device supports the frame rate of up to 15 fps."
      },
      {
        "SCREEN_CAPTURE_FRAMERATE_CAPABILITY_30_FPS": "1: The device supports the frame rate of up to 30 fps."
      },
      {
        "SCREEN_CAPTURE_FRAMERATE_CAPABILITY_60_FPS": "2: The device supports the frame rate of up to 60 fps."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_screencapturesourcetype",
    "name": "ScreenCaptureSourceType",
    "description": "The type of the shared target. Set in ScreenCaptureSourceInfo.",
    "parameters": [
      {
        "ScreenCaptureSourceType_Unknown": "-1: Unknown type."
      },
      {
        "ScreenCaptureSourceType_Window": "0: The shared target is a window."
      },
      {
        "ScreenCaptureSourceType_Screen": "1: The shared target is a screen of a particular monitor."
      },
      {
        "ScreenCaptureSourceType_Custom": "2: Reserved parameter"
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_screenscenariotype",
    "name": "SCREEN_SCENARIO_TYPE",
    "description": "The screen sharing scenario.",
    "parameters": [
      {
        "SCREEN_SCENARIO_DOCUMENT": "1: (Default) Document. This scenario prioritizes the video quality of screen sharing and reduces the latency of the shared video for the receiver. If you share documents, slides, and tables, you can set this scenario."
      },
      {
        "SCREEN_SCENARIO_GAMING": "2: Game. This scenario prioritizes the smoothness of screen sharing. If you share games, you can set this scenario."
      },
      {
        "SCREEN_SCENARIO_VIDEO": "3: Video. This scenario prioritizes the smoothness of screen sharing. If you share movies or live videos, you can set this scenario."
      },
      {
        "SCREEN_SCENARIO_RDC": "4: Remote control. This scenario prioritizes the video quality of screen sharing and reduces the latency of the shared video for the receiver. If you share the device desktop being remotely controlled, you can set this scenario."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_segmodeltype",
    "name": "SEG_MODEL_TYPE",
    "description": "The type of algorithms to user for background processing.",
    "parameters": [
      {
        "SEG_MODEL_AI": "1: (Default) Use the algorithm suitable for all scenarios."
      },
      {
        "SEG_MODEL_GREEN": "2: Use the algorithm designed specifically for scenarios with a green screen background."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_simulcaststreammode",
    "name": "SIMULCAST_STREAM_MODE",
    "description": "The mode in which the video stream is sent.",
    "parameters": [
      {
        "AUTO_SIMULCAST_STREAM": "-1: By default, do not send the low-quality video stream until a subscription request for the low-quality video stream is received from the receiving end, then automatically start sending low-quality video stream."
      },
      {
        "DISABLE_SIMULCAST_STREAM": "0: Never send low-quality video stream."
      },
      {
        "ENABLE_SIMULCAST_STREAM": "1: Always send low-quality video stream."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_streamfallbackoptions",
    "name": "STREAM_FALLBACK_OPTIONS",
    "description": "Options for handling audio and video stream fallback when network conditions are weak.",
    "parameters": [
      {
        "STREAM_FALLBACK_OPTION_DISABLED": "0: No fallback processing is performed on audio and video streams, the quality of the audio and video streams cannot be guaranteed."
      },
      {
        "STREAM_FALLBACK_OPTION_VIDEO_STREAM_LOW": "1: Only receive low-quality (low resolution, low bitrate) video stream."
      },
      {
        "STREAM_FALLBACK_OPTION_AUDIO_ONLY": "2: When the network conditions are weak, try to receive the low-quality video stream first. If the video cannot be displayed due to extremely weak network environment, then fall back to receiving audio-only stream."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_streampublishstate",
    "name": "STREAM_PUBLISH_STATE",
    "description": "The publishing state.",
    "parameters": [
      {
        "PUB_STATE_IDLE": "0: The initial publishing state after joining the channel."
      },
      {
        "PUB_STATE_NO_PUBLISHED": "1: Fails to publish the local stream. Possible reasons:\n The local user calls MuteLocalAudioStream (true) or MuteLocalVideoStream (true) to stop sending local media streams.\n The local user calls DisableAudio or DisableVideo to disable the local audio or video module.\n The local user calls EnableLocalAudio (false) or EnableLocalVideo (false) to disable the local audio or video capture.\n The role of the local user is audience."
      },
      {
        "PUB_STATE_PUBLISHING": "2: Publishing."
      },
      {
        "PUB_STATE_PUBLISHED": "3: Publishes successfully."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_streamsubscribestate",
    "name": "STREAM_SUBSCRIBE_STATE",
    "description": "The subscribing state.",
    "parameters": [
      {
        "SUB_STATE_IDLE": "0: The initial publishing state after joining the channel."
      },
      {
        "SUB_STATE_NO_SUBSCRIBED": "1: Fails to subscribe to the remote stream. Possible reasons:\n The remote user:\n Calls MuteLocalAudioStream (true) or MuteLocalVideoStream (true) to stop sending local media stream.\n Calls DisableAudio or DisableVideo to disable the local audio or video module.\n Calls EnableLocalAudio (false) or EnableLocalVideo (false) to disable local audio or video capture.\n The role of the remote user is audience.\n The local user calls the following methods to stop receiving remote streams:\n Call MuteRemoteAudioStream (true) or MuteAllRemoteAudioStreams (true) to stop receiving the remote audio stream.\n Call MuteRemoteVideoStream (true) or MuteAllRemoteVideoStreams (true) to stop receiving the remote video stream."
      },
      {
        "SUB_STATE_SUBSCRIBING": "2: Subscribing."
      },
      {
        "SUB_STATE_SUBSCRIBED": "3: The remote stream is received, and the subscription is successful."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_userofflinereasontype",
    "name": "USER_OFFLINE_REASON_TYPE",
    "description": "Reasons for a user being offline.",
    "parameters": [
      {
        "USER_OFFLINE_QUIT": "0: The user quits the call."
      },
      {
        "USER_OFFLINE_DROPPED": "1: The SDK times out and the user drops offline because no data packet is received within a certain period of time. If the user quits the call and the message is not passed to the SDK (due to an unreliable channel), the SDK assumes the user dropped offline."
      },
      {
        "USER_OFFLINE_BECOME_AUDIENCE": "2: The user switches the client role from the host to the audience."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videoapplicationscenariotype",
    "name": "VIDEO_APPLICATION_SCENARIO_TYPE",
    "description": "The video application scenarios.",
    "parameters": [
      {
        "APPLICATION_SCENARIO_GENERAL": "0: (Default) The general scenario."
      },
      {
        "APPLICATION_SCENARIO_MEETING": "APPLICATION_SCENARIO_MEETING (1) is suitable for meeting scenarios. The SDK automatically enables the following strategies:\n In meeting scenarios where low-quality video streams are required to have a high bitrate, the SDK automatically enables multiple technologies used to deal with network congestions, to enhance the performance of the low-quality streams and to ensure the smooth reception by subscribers.\n The SDK monitors the number of subscribers to the high-quality video stream in real time and dynamically adjusts its configuration based on the number of subscribers.\n If nobody subscribers to the high-quality stream, the SDK automatically reduces its bitrate and frame rate to save upstream bandwidth.\n If someone subscribes to the high-quality stream, the SDK resets the high-quality stream to the VideoEncoderConfiguration configuration used in the most recent calling of SetVideoEncoderConfiguration. If no configuration has been set by the user previously, the following values are used:\n Resolution: (Windows and macOS) 1280 × 720; (Android and iOS) 960 × 540\n Frame rate: 15 fps\n Bitrate: (Windows and macOS) 1600 Kbps; (Android and iOS) 1000 Kbps\n The SDK monitors the number of subscribers to the low-quality video stream in real time and dynamically enables or disables it based on the number of subscribers. If the user has called SetDualStreamMode [2/2] to set that never send low-quality video stream (DISABLE_SIMULCAST_STREAM), the dynamic adjustment of the low-quality stream in meeting scenarios will not take effect.\n If nobody subscribes to the low-quality stream, the SDK automatically disables it to save upstream bandwidth.\n If someone subscribes to the low-quality stream, the SDK enables the low-quality stream and resets it to the SimulcastStreamConfig configuration used in the most recent calling of SetDualStreamMode [2/2]. If no configuration has been set by the user previously, the following values are used:\n Resolution: 480 × 272\n Frame rate: 15 fps\n Bitrate: 500 Kbps 1: The meeting scenario."
      },
      {
        "APPLICATION_SCENARIO_1V1": "APPLICATION_SCENARIO_1V1 (2) This is applicable to the scenario. To meet the requirements for low latency and high-quality video in this scenario, the SDK optimizes its strategies, improving performance in terms of video quality, first frame rendering, latency on mid-to-low-end devices, and smoothness under weak network conditions. 2: 1v1 video call scenario."
      },
      {
        "APPLICATION_SCENARIO_LIVESHOW": "APPLICATION_SCENARIO_LIVESHOW (3) This is applicable to the scenario. In this scenario, fast video rendering and high image quality are crucial. The SDK implements several performance optimizations, including automatically enabling accelerated audio and video frame rendering to minimize first-frame latency (no need to call EnableInstantMediaRendering), and B-frame encoding to achieve better image quality and bandwidth efficiency. The SDK also provides enhanced video quality and smooth playback, even in poor network conditions or on lower-end devices. 3. Live show scenario."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videobuffertype",
    "name": "VIDEO_BUFFER_TYPE",
    "description": "The video buffer type.",
    "parameters": [
      {
        "VIDEO_BUFFER_RAW_DATA": "1: The video buffer in the format of raw data."
      },
      {
        "VIDEO_BUFFER_ARRAY": "2: The video buffer in the format of raw data."
      },
      {
        "VIDEO_BUFFER_TEXTURE": "3: The video buffer in the format of Texture."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videocapturetype",
    "name": "VIDEO_CAPTURE_TYPE",
    "description": "The capture type of the custom video source.",
    "parameters": [
      {
        "VIDEO_CAPTURE_UNKNOWN": "Unknown type."
      },
      {
        "VIDEO_CAPTURE_CAMERA": "(Default) Video captured by the camera."
      },
      {
        "VIDEO_CAPTURE_SCREEN": "Video for screen sharing."
      }
    ],
    "returns": "",
    "is_hide": true
  },
  {
    "id": "enum_videocodeccapabilitylevel",
    "name": "VIDEO_CODEC_CAPABILITY_LEVEL",
    "description": "The level of the codec capability.",
    "parameters": [
      {
        "CODEC_CAPABILITY_LEVEL_UNSPECIFIED": "-1: Unsupported video type. Currently, only H.264 and H.265 formats are supported. If the video is in another format, this value will be returned."
      },
      {
        "CODEC_CAPABILITY_LEVEL_BASIC_SUPPORT": "5: Supports encoding and decoding videos up to 1080p and 30 fps."
      },
      {
        "CODEC_CAPABILITY_LEVEL_1080P30FPS": "10: Supports encoding and decoding videos up to1080p and 30 fps."
      },
      {
        "CODEC_CAPABILITY_LEVEL_1080P60FPS": "20: Support encoding and decoding videos up to 1080p and 60 fps."
      },
      {
        "CODEC_CAPABILITY_LEVEL_4K60FPS": "30: Support encoding and decoding videos up to 4K and 30 fps."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videocodecprofiletype",
    "name": "VIDEO_CODEC_PROFILE_TYPE",
    "description": "Video codec profile types.",
    "parameters": [
      {
        "VIDEO_CODEC_PROFILE_BASELINE": "66: Baseline video codec profile; generally used for video calls on mobile phones."
      },
      {
        "VIDEO_CODEC_PROFILE_MAIN": "77: Main video codec profile; generally used in mainstream electronics such as MP4 players, portable video players, PSP, and iPads."
      },
      {
        "VIDEO_CODEC_PROFILE_HIGH": "100: (Default) High video codec profile; generally used in high-resolution live streaming or television."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videocodectype",
    "name": "VIDEO_CODEC_TYPE",
    "description": "Video codec types.",
    "parameters": [
      {
        "VIDEO_CODEC_NONE": "0: (Default) Unspecified codec format. The SDK automatically matches the appropriate codec format based on the current video stream's resolution and device performance."
      },
      {
        "VIDEO_CODEC_VP8": "1: Standard VP8."
      },
      {
        "VIDEO_CODEC_H264": "2: Standard H.264."
      },
      {
        "VIDEO_CODEC_H265": "3: Standard H.265."
      },
      {
        "VIDEO_CODEC_GENERIC": "6: Generic. This type is used for transmitting raw video data, such as encrypted video frames. The SDK returns this type of video frames in callbacks, and you need to decode and render the frames yourself."
      },
      {
        "VIDEO_CODEC_GENERIC_JPEG": "20: Generic JPEG. This type consumes minimum computing resources and applies to IoT devices."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videocodectypeforstream",
    "name": "VIDEO_CODEC_TYPE_FOR_STREAM",
    "description": "The codec type of the output video.",
    "parameters": [
      {
        "VIDEO_CODEC_H264_FOR_STREAM": "1: (Default) H.264."
      },
      {
        "VIDEO_CODEC_H265_FOR_STREAM": "2: H.265."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videocontenthint",
    "name": "VIDEO_CONTENT_HINT",
    "description": "The content hint for screen sharing.",
    "parameters": [
      {
        "CONTENT_HINT_NONE": "(Default) No content hint."
      },
      {
        "CONTENT_HINT_MOTION": "Motion-intensive content. Choose this option if you prefer smoothness or when you are sharing a video clip, movie, or video game."
      },
      {
        "CONTENT_HINT_DETAILS": "Motionless content. Choose this option if you prefer sharpness or when you are sharing a picture, PowerPoint slides, or texts."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videodenoiserlevel",
    "name": "VIDEO_DENOISER_LEVEL",
    "description": "Video noise reduction level.",
    "parameters": [
      {
        "VIDEO_DENOISER_LEVEL_HIGH_QUALITY": "0: (Default) Promotes video quality during video noise reduction. balances performance consumption and video noise reduction quality. The performance consumption is moderate, the video noise reduction speed is moderate, and the overall video quality is optimal."
      },
      {
        "VIDEO_DENOISER_LEVEL_FAST": "1: Promotes reducing performance consumption during video noise reduction. It prioritizes reducing performance consumption over video noise reduction quality. The performance consumption is lower, and the video noise reduction speed is faster. To avoid a noticeable shadowing effect (shadows trailing behind moving objects) in the processed video, Agora recommends that you use this setting when the camera is fixed."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videodenoisermode",
    "name": "VIDEO_DENOISER_MODE",
    "description": "Video noise reduction mode.",
    "parameters": [
      {
        "VIDEO_DENOISER_AUTO": "0: (Default) Automatic mode. The SDK automatically enables or disables the video noise reduction feature according to the ambient light."
      },
      {
        "VIDEO_DENOISER_MANUAL": "1: Manual mode. Users need to enable or disable the video noise reduction feature manually."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videoframetype",
    "name": "VIDEO_FRAME_TYPE",
    "description": "The video frame type.",
    "parameters": [
      {
        "VIDEO_FRAME_TYPE_BLANK_FRAME": "0: A black frame."
      },
      {
        "VIDEO_FRAME_TYPE_KEY_FRAME": "3: Key frame."
      },
      {
        "VIDEO_FRAME_TYPE_DELTA_FRAME": "4: Delta frame."
      },
      {
        "VIDEO_FRAME_TYPE_B_FRAME": "5: The B frame."
      },
      {
        "VIDEO_FRAME_TYPE_DROPPABLE_FRAME": "6: A discarded frame."
      },
      {
        "VIDEO_FRAME_TYPE_UNKNOW": "Unknown frame."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videomirrormodetype",
    "name": "VIDEO_MIRROR_MODE_TYPE",
    "description": "Video mirror mode.",
    "parameters": [
      {
        "VIDEO_MIRROR_MODE_AUTO": "0: The SDK determines the mirror mode.\n For the mirror mode of the local video view: If you use a front camera, the SDK enables the mirror mode by default; if you use a rear camera, the SDK disables the mirror mode by default.\n For the remote user: The mirror mode is disabled by default."
      },
      {
        "VIDEO_MIRROR_MODE_ENABLED": "1: Enable mirror mode."
      },
      {
        "VIDEO_MIRROR_MODE_DISABLED": "2: Disable mirror mode."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videomoduleposition",
    "name": "VIDEO_MODULE_POSITION",
    "description": "The frame position of the video observer.",
    "parameters": [
      {
        "POSITION_POST_CAPTURER": "1: The location of the locally collected video data after preprocessing corresponds to the OnCaptureVideoFrame callback. The observed video here has the effect of video pre-processing, which can be verified by enabling image enhancement, virtual background, or watermark."
      },
      {
        "POSITION_PRE_RENDERER": "2: The pre-renderer position, which corresponds to the video data in the OnRenderVideoFrame callback."
      },
      {
        "POSITION_PRE_ENCODER": "4: The pre-encoder position, which corresponds to the video data in the OnPreEncodeVideoFrame callback. The observed video here has the effects of video pre-processing and encoding pre-processing.\n To verify the pre-processing effects of the video, you can enable image enhancement, virtual background, or watermark.\n To verify the pre-encoding processing effect, you can set a lower frame rate (for example, 5 fps)."
      },
      {
        "POSITION_POST_CAPTURER_ORIGIN": "8: The position after local video capture and before pre-processing. The observed video here does not have pre-processing effects, which can be verified by enabling image enhancement, virtual background, or watermarks."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videoobserverframetype",
    "name": "VIDEO_OBSERVER_FRAME_TYPE",
    "description": "Video frame formats.",
    "parameters": [
      {
        "FRAME_TYPE_RGBA": "2: The format of the video frame is RGBA."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videoorientation",
    "name": "VIDEO_ORIENTATION",
    "description": "The clockwise rotation of the video.",
    "parameters": [
      {
        "VIDEO_ORIENTATION_0": "0: (Default) No rotation."
      },
      {
        "VIDEO_ORIENTATION_90": "90: 90 degrees."
      },
      {
        "VIDEO_ORIENTATION_180": "180: 180 degrees."
      },
      {
        "VIDEO_ORIENTATION_270": "270: 270 degrees."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videopixelformat",
    "name": "VIDEO_PIXEL_FORMAT",
    "description": "The video pixel format.",
    "parameters": [
      {
        "VIDEO_PIXEL_DEFAULT": "0: Raw video pixel format."
      },
      {
        "VIDEO_PIXEL_I420": "1: The format is I420."
      },
      {
        "VIDEO_PIXEL_RGBA": "4: The format is RGBA."
      },
      {
        "VIDEO_PIXEL_I422": "16: The format is I422."
      },
      {
        "VIDEO_TEXTURE_ID3D11TEXTURE2D": "17: The ID3D11TEXTURE2D format. Currently supported types are DXGI_FORMAT_B8G8R8A8_UNORM, DXGI_FORMAT_B8G8R8A8_TYPELESS and DXGI_FORMAT_NV12."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videosourcetype",
    "name": "VIDEO_SOURCE_TYPE",
    "description": "The type of the video source.",
    "parameters": [
      {
        "VIDEO_SOURCE_CAMERA_PRIMARY": "0: (Default) The primary camera."
      },
      {
        "VIDEO_SOURCE_CAMERA": "0: (Default) The primary camera."
      },
      {
        "VIDEO_SOURCE_CAMERA_SECONDARY": "1: The secondary camera."
      },
      {
        "VIDEO_SOURCE_SCREEN_PRIMARY": "2: The primary screen."
      },
      {
        "VIDEO_SOURCE_SCREEN": "2: The primary screen."
      },
      {
        "VIDEO_SOURCE_SCREEN_SECONDARY": "3: The secondary screen."
      },
      {
        "VIDEO_SOURCE_CUSTOM": "4: A custom video source."
      },
      {
        "VIDEO_SOURCE_MEDIA_PLAYER": "5: The media player."
      },
      {
        "VIDEO_SOURCE_RTC_IMAGE_PNG": "6: One PNG image."
      },
      {
        "VIDEO_SOURCE_RTC_IMAGE_JPEG": "7: One JPEG image."
      },
      {
        "VIDEO_SOURCE_RTC_IMAGE_GIF": "8: One GIF image."
      },
      {
        "VIDEO_SOURCE_REMOTE": "9: One remote video acquired by the network."
      },
      {
        "VIDEO_SOURCE_TRANSCODED": "10: One transcoded video source."
      },
      {
        "VIDEO_SOURCE_CAMERA_THIRD": "11: (For Android, Windows, and macOS only) The third camera."
      },
      {
        "VIDEO_SOURCE_CAMERA_FOURTH": "12: (For Android, Windows, and macOS only) The fourth camera."
      },
      {
        "VIDEO_SOURCE_SCREEN_THIRD": "13: (For Windows and macOS only) The third screen."
      },
      {
        "VIDEO_SOURCE_SCREEN_FOURTH": "14: (For Windows and macOS only) The fourth screen."
      },
      {
        "VIDEO_SOURCE_UNKNOWN": "100: An unknown video source."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videostreamtype",
    "name": "VIDEO_STREAM_TYPE",
    "description": "The type of video streams.",
    "parameters": [
      {
        "VIDEO_STREAM_HIGH": "0: High-quality video stream."
      },
      {
        "VIDEO_STREAM_LOW": "1: Low-quality video stream."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videotranscodererror",
    "name": "VIDEO_TRANSCODER_ERROR",
    "description": "The error code of the local video mixing failure.",
    "parameters": [
      {
        "VT_ERR_VIDEO_SOURCE_NOT_READY": "1: The selected video source has not started video capture. You need to create a video track for it and start video capture."
      },
      {
        "VT_ERR_INVALID_VIDEO_SOURCE_TYPE": "2: The video source type is invalid. You need to re-specify the supported video source type."
      },
      {
        "VT_ERR_INVALID_IMAGE_PATH": "3: The image path is invalid. You need to re-specify the correct image path."
      },
      {
        "VT_ERR_UNSUPPORT_IMAGE_FORMAT": "4: The image format is invalid. Make sure the image format is one of PNG, JPEG, or GIF."
      },
      {
        "VT_ERR_INVALID_LAYOUT": "5: The video encoding resolution after video mixing is invalid."
      },
      {
        "VT_ERR_INTERNAL": "20: Unknown internal error."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_videoviewsetupmode",
    "name": "VIDEO_VIEW_SETUP_MODE",
    "description": "Setting mode of the view.",
    "parameters": [
      {
        "VIDEO_VIEW_SETUP_REPLACE": "0: (Default) Clear all added views and replace with a new view."
      },
      {
        "VIDEO_VIEW_SETUP_ADD": "1: Adds a view."
      },
      {
        "VIDEO_VIEW_SETUP_REMOVE": "2: Deletes a view."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_voiceaitunertype",
    "name": "VOICE_AI_TUNER_TYPE",
    "description": "Voice AI tuner sound types.",
    "parameters": [
      {
        "VOICE_AI_TUNER_MATURE_MALE": "0: Mature male voice. A deep and magnetic male voice."
      },
      {
        "VOICE_AI_TUNER_FRESH_MALE": "1: Fresh male voice. A fresh and slightly sweet male voice."
      },
      {
        "VOICE_AI_TUNER_ELEGANT_FEMALE": "2: Elegant female voice. A deep and charming female voice."
      },
      {
        "VOICE_AI_TUNER_SWEET_FEMALE": "3: Sweet female voice. A high-pitched and cute female voice."
      },
      {
        "VOICE_AI_TUNER_WARM_MALE_SINGING": "4: Warm male singing. A warm and melodious male voice."
      },
      {
        "VOICE_AI_TUNER_GENTLE_FEMALE_SINGING": "5: Gentle female singing. A soft and delicate female voice."
      },
      {
        "VOICE_AI_TUNER_HUSKY_MALE_SINGING": "6: Husky male singing. A unique husky male voice."
      },
      {
        "VOICE_AI_TUNER_WARM_ELEGANT_FEMALE_SINGING": "7: Warm elegant female singing. A warm and mature female voice."
      },
      {
        "VOICE_AI_TUNER_POWERFUL_MALE_SINGING": "8: Powerful male singing. A strong and powerful male voice."
      },
      {
        "VOICE_AI_TUNER_DREAMY_FEMALE_SINGING": "9: Dreamy female singing. A dreamy and soft female voice."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_voicebeautifierpreset",
    "name": "VOICE_BEAUTIFIER_PRESET",
    "description": "The options for SDK preset voice beautifier effects.",
    "parameters": [
      {
        "VOICE_BEAUTIFIER_OFF": "Turn off voice beautifier effects and use the original voice."
      },
      {
        "CHAT_BEAUTIFIER_MAGNETIC": "A more magnetic voice. Agora recommends using this enumerator to process a male-sounding voice; otherwise, you may experience vocal distortion."
      },
      {
        "CHAT_BEAUTIFIER_FRESH": "A fresher voice. Agora recommends using this enumerator to process a female-sounding voice; otherwise, you may experience vocal distortion."
      },
      {
        "CHAT_BEAUTIFIER_VITALITY": "A more vital voice. Agora recommends using this enumerator to process a female-sounding voice; otherwise, you may experience vocal distortion."
      },
      {
        "SINGING_BEAUTIFIER": "Singing beautifier effect.\n If you call SetVoiceBeautifierPreset (SINGING_BEAUTIFIER), you can beautify a male-sounding voice and add a reverberation effect that sounds like singing in a small room. Agora recommends using this enumerator to process a male-sounding voice; otherwise, you might experience vocal distortion.\n If you call SetVoiceBeautifierParameters (SINGING_BEAUTIFIER, param1, param2), you can beautify a male or female-sounding voice and add a reverberation effect."
      },
      {
        "TIMBRE_TRANSFORMATION_VIGOROUS": "A more vigorous voice."
      },
      {
        "TIMBRE_TRANSFORMATION_DEEP": "A deep voice."
      },
      {
        "TIMBRE_TRANSFORMATION_MELLOW": "A mellower voice."
      },
      {
        "TIMBRE_TRANSFORMATION_FALSETTO": "Falsetto."
      },
      {
        "TIMBRE_TRANSFORMATION_FULL": "A fuller voice."
      },
      {
        "TIMBRE_TRANSFORMATION_CLEAR": "A clearer voice."
      },
      {
        "TIMBRE_TRANSFORMATION_RESOUNDING": "A more resounding voice."
      },
      {
        "TIMBRE_TRANSFORMATION_RINGING": "A more ringing voice."
      },
      {
        "ULTRA_HIGH_QUALITY_VOICE": "A ultra-high quality voice, which makes the audio clearer and restores more details.\n To achieve better audio effect quality, Agora recommends that you set the profile of SetAudioProfile [2/2] to AUDIO_PROFILE_MUSIC_HIGH_QUALITY (4) or AUDIO_PROFILE_MUSIC_HIGH_QUALITY_STEREO (5) and scenario to AUDIO_SCENARIO_GAME_STREAMING (3) before calling SetVoiceBeautifierPreset.\n If you have an audio capturing device that can already restore audio details to a high degree, Agora recommends that you do not enable ultra-high quality; otherwise, the SDK may over-restore audio details, and you may not hear the anticipated voice effect."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_voicechangerpreset",
    "name": "VOICE_CHANGER_PRESET",
    "description": "Local voice changer options.",
    "parameters": [
      {
        "VOICE_CHANGER_OFF": "The original voice (no local voice change)."
      },
      {
        "VOICE_CHANGER_OLDMAN": "The voice of an old man."
      },
      {
        "VOICE_CHANGER_BABYBOY": "The voice of a little boy."
      },
      {
        "VOICE_CHANGER_BABYGIRL": "The voice of a little girl."
      },
      {
        "VOICE_CHANGER_ZHUBAJIE": "The voice of Zhu Bajie, a character in Journey to the West who has a voice like that of a growling bear."
      },
      {
        "VOICE_CHANGER_ETHEREAL": "The ethereal voice."
      },
      {
        "VOICE_CHANGER_HULK": "The voice of Hulk."
      },
      {
        "VOICE_BEAUTY_VIGOROUS": "A more vigorous voice."
      },
      {
        "VOICE_BEAUTY_DEEP": "A deeper voice."
      },
      {
        "VOICE_BEAUTY_MELLOW": "A mellower voice."
      },
      {
        "VOICE_BEAUTY_FALSETTO": "Falsetto."
      },
      {
        "VOICE_BEAUTY_FULL": "A fuller voice."
      },
      {
        "VOICE_BEAUTY_CLEAR": "A clearer voice."
      },
      {
        "VOICE_BEAUTY_RESOUNDING": "A more resounding voice."
      },
      {
        "VOICE_BEAUTY_RINGING": "A more ringing voice."
      },
      {
        "VOICE_BEAUTY_SPACIAL": "A more spatially resonant voice."
      },
      {
        "GENERAL_BEAUTY_VOICE_MALE": "(For male only) A more magnetic voice. Do not use it when the speaker is a male; otherwise, voice distortion occurs. Do not use it when the speaker is a female; otherwise, voice distortion occurs."
      },
      {
        "GENERAL_BEAUTY_VOICE_FEMALE_FRESH": "(For female only) A fresher voice. Do not use it when the speaker is a male; otherwise, voice distortion occurs. Do not use it when the speaker is a male; otherwise, voice distortion occurs."
      },
      {
        "GENERAL_BEAUTY_VOICE_FEMALE_VITALITY": "(For female only) A more vital voice. Do not use it when the speaker is a male; otherwise, voice distortion occurs. Do not use it when the speaker is a male; otherwise, voice distortion occurs."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_voiceconversionpreset",
    "name": "VOICE_CONVERSION_PRESET",
    "description": "The options for SDK preset voice conversion effects.",
    "parameters": [
      {
        "VOICE_CONVERSION_OFF": "Turn off voice conversion effects and use the original voice."
      },
      {
        "VOICE_CHANGER_NEUTRAL": "A gender-neutral voice. To avoid audio distortion, ensure that you use this enumerator to process a female-sounding voice."
      },
      {
        "VOICE_CHANGER_SWEET": "A sweet voice. To avoid audio distortion, ensure that you use this enumerator to process a female-sounding voice."
      },
      {
        "VOICE_CHANGER_SOLID": "A steady voice. To avoid audio distortion, ensure that you use this enumerator to process a male-sounding voice."
      },
      {
        "VOICE_CHANGER_BASS": "A deep voice. To avoid audio distortion, ensure that you use this enumerator to process a male-sounding voice."
      }
    ],
    "returns": "",
    "is_hide": false
  },
  {
    "id": "enum_warningcode",
    "name": "WarningCode",
    "description": "Warning codes. See https://docs.agora.io/en/Interactive%20Broadcast/error_rtc.",
    "parameters": [],
    "returns": ""
  },
  {
    "id": "enum_watermarkfitmode",
    "name": "WATERMARK_FIT_MODE",
    "description": "The adaptation mode of the watermark.",
    "parameters": [
      {
        "FIT_MODE_COVER_POSITION": "Use the positionInLandscapeMode and positionInPortraitMode values you set in WatermarkOptions. The settings in WatermarkRatio are invalid."
      },
      {
        "FIT_MODE_USE_IMAGE_RATIO": "Use the value you set in WatermarkRatio. The settings in positionInLandscapeMode and positionInPortraitMode in WatermarkOptions are invalid."
      }
    ],
    "returns": "",
    "is_hide": false
  }
]
